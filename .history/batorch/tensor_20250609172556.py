
from pycamia import info_manager

__info__ = info_manager(
    project = "PyCAMIA",
    package = "batorch",
    fileinfo = "The inherited tensor from 'torch' with batch. This is the template to file tensor.py, please modify in this file instead of the auto-generated tensor.py",
    requires = "torch"
)

__all__ = """
    get_cpu_memory_used           get_gpu_memory_used           collect_memory
    turn_on_autodevice            turn_off_autodevice
    set_device     get_device     to_device      default_device auto_device
    new_dim        exist_dim      del_dim        iter_dim       linalg_dim
    inverse inv    diag           diagflat       diagonal       trace tr
    add            sub            mul            div            pow
    fmod           log            ln             log2           log10
    exp            square         sqrt           abs            sign
    sin            cos            tan            cot            sec            csc
    asin           arcsin         acos           arccos         atan           arctan
    matmul         mm             bmm            smm
    floor_divide   true_divide    equal
    addmm          addbmm         saddmm         addcmul
    clamp          floor          ceil           round
    any            all            unique         isin
    isnan          isinf          isposinf       isneginf       isfinite
    unsqueeze      squeeze
    flatten        transpose      t              permute        standard_shape
    duplicate      amplify        repeated       repeat
    gather         flip           detach
    quantile       val_range
    sum            prod           mean           std            norm
    cumsum         cumprod        min            max            median
    cummin         cummax         argmin         argmax
    split          sample         pick
    eig            matpow         matexp         matlog         rank           matnorm
    det            matrix_power   matrix_exp     matrix_log     matrix_rank    matrix_norm

    FuncDim        BatchDim       SequeDim       FeatDim
    SpaceDim       SpecDim        AllDim
    Size           FakeSize       SpecialDimensions             MajorSpecialDimensions
    broadcast      remove_dim     add_dim

    Tensor
    expand         expand_as      expand_to
    complex        tensor         as_tensor      to_bttensor
    empty          full           ones           zeros          tensor_to
    empty_like     full_like      ones_like      zeros_like     tensor_like
    rand           randn          rand_like      randn_like     randperm
    arange         where          reshape
    cat            stack          meshgrid
    eye            eye_like
    batch_arange   feature_arange channel_arange sequence_arange
    batch_tensor   feature_tensor channel_tensor sequence_tensor
    time_tensor    series_tensor
    randint        randint_like
    
    dtype          device
    bfloat16       bool
    cdouble        cfloat         chalf          
    complex128     complex32      complex64
    double         half
    float          float16        float32        float64
    int            int16          int32          int64          int8
    qint32         qint8          quint2x4       quint4x2       quint8
    long           short          uint8
    manual_seed
""".split()

import builtins, re, sys, math
from abc import ABCMeta
from collections import defaultdict
from functools import wraps
from typing import Generator
from .device import GB, AutoDevice, SleepingDevice
from .tensorsize import *

with __info__:
    import torch
    import batorch as bt
    from pyoverload import null, to_torch_dtype, dtype as dtype_, method
    from pycamia import ByteSize, Version
    from pycamia import avouch, touch, alias, void
    from pycamia import execblock, get_num_indent, add_lineno
    from pycamia import tokenize, token_replace, identity_function
    from pycamia import get_alphas, arg_extract, max_argmax
    from pycamia import argmax as argmax_, prod as prod_, item, to_list
    from pycamia import get_reference_line

int_ = builtins.int
min_ = builtins.min
max_ = builtins.max
abs_ = builtins.abs
any_ = builtins.any
all_ = builtins.all
sum_ = builtins.sum
bool_ = builtins.bool
round_ = builtins.round
range_ = builtins.range
float_ = builtins.float
complex_ = builtins.complex
num_ = (int_, float_)

_total_cpu_memory_used = 0
_total_gpu_memory_used = 0
_device = AutoDevice(verbose=True, always_proceed=True)

"""
    TODO:
    sparse-related
    device-related
"""

def get_cpu_memory_used():
    global _total_cpu_memory_used
    return ByteSize(_total_cpu_memory_used)

def get_gpu_memory_used():
    global _total_gpu_memory_used
    return ByteSize(_total_gpu_memory_used)

def collect_memory(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        ret = func(*args, **kwargs)
        if ret.device == torch.device('cpu'):
            global _total_cpu_memory_used
            _total_cpu_memory_used += ret.byte_size()
        else:
            global _total_gpu_memory_used
            _total_gpu_memory_used += ret.byte_size()
        return ret
    return wrapper

def set_device(device):
    global _device
    if isinstance(device, AutoDevice): _device = device
    elif isinstance(device, torch.device): _device = SleepingDevice(device)
    else: raise TypeError("Invalid device type. ")

def get_device():
    global _device
    return _device

def default_device():
    global _device
    return _device.device

def auto_device(x):
    global _device
    return x.to(_device.device)

@collect_memory
def to_device(x):
    global _device
    return x.to(_device.main_device)

def turn_on_autodevice(): _device.turn_on()
def turn_off_autodevice(): _device.turn_off()

def torch_super(self, func_name):
    return method(getattr(torch.Tensor, func_name), self)

class SizeMappingFunctions:
    def __init__(self):
        self.default_shape = identity_function
        self.reduce_funcs = "any all sum prod min max median mean std norm".split()
        self.dimfree_funcs = "cumsum cumprod cummax cummin gather flip".split()
    def __getitem__(self, name):
        if name in self.reduce_funcs:
            return lambda s, d, **k: s if k.get('keepdim', False) else remove_dim(s, d)
        if name in self.dimfree_funcs:
            return lambda s, d: s
        if name == "unsqueeze":
            return lambda s, d: Size(add_dim(tuple(s), d)).update_special_from(d)
        if name == "squeeze":
            return lambda s, d: remove_dim(s, d).update_special_from(d)
        if name == "permute":
            return lambda s, d: s.permute(*d)
        if name == "transpose":
            return lambda s, d1, d2: s.permute(*range_(min_(d1[0], d2[0])), max_(d1[0], d2[0]), *range_(min_(d1[0], d2[0])+1, max_(d1[0], d2[0])), min_(d1[0], d2[0]), *range_(max_(d1[0], d2[0])+1, len(s))) if d1[0] != d2[0] else s
        return self.default_shape
size_mapping = SizeMappingFunctions()

class OperationSizeMappingFunctions:
    def __init__(self):
        self.matmul_funcs = "__matmul__ mm bmm smm".split()
        self.addmul_funcs = "addmm addbmm saddmm".split()
        self.allmatch_funcs = "addcmul where".split()
    def default_shape(self, shape_a, shape_b):
        shape_a, shape_b = shape_a ^ shape_b
        return shape_a, shape_a, shape_b
    def matmul_shape(self, self_shape, other_shape):
        self_shape = Size(self_shape)
        other_shape = Size(other_shape)
        if self_shape.n_dim < 1 or other_shape.n_dim < 1:
            x_shape, y_shape = self_shape ^ other_shape
            return x_shape, x_shape, y_shape
        use_dim = None
        shape_last_2 = self_shape[-2:]
        for dim in [FeatDim, SequeDim, SpaceDim]:
            if shape_last_2.get_n_dim(dim) == 2:
                use_dim = dim
                repr_self_shape = self_shape.with_dim_size(-1, -1).with_dim_size(-2, -1)
                l_size = 2
                break
        else:
            use_dim = self_shape.get_dim_type(-1)
            repr_self_shape = self_shape.with_dim_size(-1, -1)
            l_size = 1
        shape_last_2 = other_shape[-2:]
        for dim in [FeatDim, SequeDim, SpaceDim]:
            if shape_last_2.get_n_dim(dim) == 2:
                if use_dim is not None and dim != use_dim:
                    raise TypeError("Cannot multiply tensors with different dimension type at the last dimensions. ")
                repr_other_shape = other_shape.with_dim_size(-1, -1).with_dim_size(-2, -1)
                r_size = 2
                break
        else:
            repr_other_shape = other_shape.with_dim_size(-1, -1)
            r_size = 1
        x_shape, y_shape = repr_self_shape ^ repr_other_shape
        z_shape = Size(max_(x, y) for x, y in zip(x_shape, y_shape)).special_from(x_shape)
        if l_size == r_size == 1:
            x_shape = x_shape[:-1] + Size(1).special_from(self_shape[-1:]) + self_shape[-1:]
            y_shape = y_shape[:-1] + other_shape[-1:] + Size(1).special_from(other_shape[-1:])
            ref_shape = z_shape[:-1]
        elif l_size == 1:
            x_shape = x_shape[:-2] + Size(1).special_from(self_shape[-1:]) + self_shape[-1:]
            y_shape = y_shape[:-r_size] + other_shape[-r_size:]
            ref_shape = z_shape[:-r_size] + other_shape[-1:]
        elif r_size == 1:
            x_shape = x_shape[:-l_size] + self_shape[-l_size:]
            y_shape = y_shape[:-2] + other_shape[-1:] + Size(1).special_from(other_shape[-1:])
            ref_shape = z_shape[:-l_size] + self_shape[-l_size:-1]
        else:
            x_shape = x_shape[:-l_size] + self_shape[-l_size:]
            y_shape = y_shape[:-r_size] + other_shape[-r_size:]
            ref_shape = z_shape[:-l_size] + self_shape[-l_size:-1] + other_shape[-r_size:-1]
        return ref_shape, x_shape, y_shape
    def addmul_shape(self, shape_a, shape_b, shape_c):
        _, shape_b, shape_c = self.matmul_func(shape_b, shape_c)
        shape_a, shape_b = shape_a ^ shape_b
        _, shape_c = shape_a ^ shape_c
        return shape_a, shape_a, shape_b, shape_c
    def allmatch_shape(self, *shapes):
        broadcasted = broadcast(*shapes, with_size_updates=True)
        return broadcasted, *broadcasted.updated_sizes
    def quantile_shape(self, shape, quantile_shape, first_dim, **kwarg):
        shape_ref = shape if kwarg.get('keepdim', False) else remove_dim(shape, first_dim)
        if quantile_shape.n_dim > 0: shape_ref = Size(FuncDim(1)) + shape_ref
        return shape_ref, None, None
    def __getitem__(self, name):
        if name in self.matmul_funcs: return self.matmul_shape
        if name in self.addmul_funcs: return self.addmul_shape
        if name in self.allmatch_funcs: return self.allmatch_shape
        if name == "quantile": return self.quantile_shape
        return self.default_shape
size_mapping_op = OperationSizeMappingFunctions()

### START BIWAY AUTO GENERATION
@alias("inv")
def inverse(input: 'Tensor', dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    conpute the inverse matrix for dimensions (the first available condition):
    (1) the last 2 feature dimensions (if n_feature_dim >= 2);
    (2) the last 2 space dimensions (if n_space_dim >= 2);
    (3) the last 2 sequence dimensions (if n_sequence_dim >= 2).

    Automatically inheritted method from 'torch.inverse'. The automatically generated codes are as follows:
        [ 1]: @alias("inv")
        [ 2]: def inverse(input: 'Tensor', dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:     conpute the inverse matrix for dimensions (the first available condition):
        [ 5]:     (1) the last 2 feature dimensions (if n_feature_dim >= 2);
        [ 6]:     (2) the last 2 space dimensions (if n_space_dim >= 2);
        [ 7]:     (3) the last 2 sequence dimensions (if n_sequence_dim >= 2).
        [ 8]:     '''
        [ 9]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [10]:
        [11]:     pivot = None
        [12]:     for t in [input]:
        [13]:         if isinstance(t, torch.Tensor): pivot = t; break
        [14]:     subclass = Tensor.get_tensor_subclass(pivot)
        [15]:
        [16]:     if input is None: ...
        [17]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [18]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [19]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [20]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [21]:
        [22]:     dim = linalg_dim[2](input, dim)
        [23]:
        [24]:     # Obtain available sized in arguments (which will be fed into size function).
        [25]:     input_shape=None if input is None else Size(input.shape)
        [26]:     # Use the given inner codes if they are provided.
        [27]:     torch_returned = False
        [28]:     avouch(len(dim) == 2 and dim[0] != dim[1], TypeError("bt.inverse accepts only two dimensions for inversion. "))
        [29]:     if not input.dtype.is_floating_point:  input = input.type(bt.float)
        [30]:     with input.hide_special(), torch._C.DisableTorchFunction():
        [31]:         inv_output = Tensor.inherit_from(torch.linalg.inv(input.move_dim(dim, -1)), input, shape=[])
        [32]:     obj = inv_output.move_dim([-2, -1], dim).special_from(input)
        [33]:
        [34]:     def update_special(obj, updates):
        [35]:         if isinstance(obj, tuple):
        [36]:             for x in obj: update_special(x, updates)
        [37]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [38]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [39]:
        [40]:     if getattr(obj, 'grad_fn', None) is not None:
        [41]:         obj.grad_fn_name = "inverse"
        [42]:     return obj
        [43]:
        The document of the original function is:

        inverse(input, *, out=None) -> Tensor

        Alias for :func:`torch.linalg.inv`

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = linalg_dim[2](input, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    avouch(len(dim) == 2 and dim[0] != dim[1], TypeError("bt.inverse accepts only two dimensions for inversion. "))
    if not input.dtype.is_floating_point:  input = input.type(bt.float)
    with input.hide_special(), torch._C.DisableTorchFunction():
        inv_output = Tensor.inherit_from(torch.linalg.inv(input.move_dim(dim, -1)), input, shape=[])
    obj = inv_output.move_dim([-2, -1], dim).special_from(input)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "inverse"
    return obj

def diag(input: 'Tensor', diagonal=0, dim: linalg_dim[1,2]=None, *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    Compute the diagonal of a 2D matrix or a 2D matrix with diagonal elements from 1D input.
    Regarding the shape of input, the first available condition is performed:
    (1) create 2D feature for 1D feature;
    (2) get 1D diagonal for the last 2 feature dimensions;
    (3) create 2D space for 1D space;
    (4) get 1D diagonal for the last 2 space dimensions;
    (5) create 2D sequence for 1D sequence;
    (6) get 1D diagonal for the last 2 sequence dimensions.

    `diagonal` controls the i-th diagonal, positive for those above the main diagonal, e.g. `diagonal=1` means:
    [0 * 0 0 0 0 0]
    [0 0 * 0 0 0 0]
    [0 0 0 * 0 0 0]
    [0 0 0 0 * 0 0]
    [0 0 0 0 0 * 0]
    [0 0 0 0 0 0 *]
    [0 0 0 0 0 0 0]
    this argument works for both 1->2D and 2->1D.

    Automatically inheritted method from 'torch.diag'. The automatically generated codes are as follows:
        [ 1]: def diag(input: 'Tensor', diagonal=0, dim: linalg_dim[1,2]=None, *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:     Compute the diagonal of a 2D matrix or a 2D matrix with diagonal elements from 1D input.
        [ 4]:     Regarding the shape of input, the first available condition is performed:
        [ 5]:     (1) create 2D feature for 1D feature;
        [ 6]:     (2) get 1D diagonal for the last 2 feature dimensions;
        [ 7]:     (3) create 2D space for 1D space;
        [ 8]:     (4) get 1D diagonal for the last 2 space dimensions;
        [ 9]:     (5) create 2D sequence for 1D sequence;
        [10]:     (6) get 1D diagonal for the last 2 sequence dimensions.
        [11]:
        [12]:     `diagonal` controls the i-th diagonal, positive for those above the main diagonal, e.g. `diagonal=1` means:
        [13]:     [0 * 0 0 0 0 0]
        [14]:     [0 0 * 0 0 0 0]
        [15]:     [0 0 0 * 0 0 0]
        [16]:     [0 0 0 0 * 0 0]
        [17]:     [0 0 0 0 0 * 0]
        [18]:     [0 0 0 0 0 0 *]
        [19]:     [0 0 0 0 0 0 0]
        [20]:     this argument works for both 1->2D and 2->1D.
        [21]:     '''
        [22]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [23]:
        [24]:     pivot = None
        [25]:     for t in [input]:
        [26]:         if isinstance(t, torch.Tensor): pivot = t; break
        [27]:     subclass = Tensor.get_tensor_subclass(pivot)
        [28]:
        [29]:     if input is None: ...
        [30]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [31]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [32]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [33]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [34]:
        [35]:     dim = linalg_dim[1,2](input, dim)
        [36]:
        [37]:     # Obtain available sized in arguments (which will be fed into size function).
        [38]:     input_shape=None if input is None else Size(input.shape)
        [39]:     # Use the given inner codes if they are provided.
        [40]:     torch_returned = False
        [41]:     size = input.shape
        [42]:     if len(dim) == 1:
        [43]:         n = size[dim[0]]
        [44]:         design_mat = cat(zeros(size.with_dim_size(dim[0], 1), device=input.device, dtype=input.dtype), input, dim[0])
        [45]:         index_mat = zeros(n + abs_(diagonal), n + abs_(diagonal), device=input.device).long()
        [46]:         if diagonal >= 0:  index_mat[arange(n, device=input.device), arange(diagonal, diagonal+n, device=input.device)] = arange(n, device=input.device) + 1
        [47]:         else:  index_mat[arange(-diagonal, -diagonal+n, device=input.device), arange(n, device=input.device)] = arange(n, device=input.device) + 1
        [48]:         index_mat.add_special_dim(0, size[dim[0]:dim[0]+1])
        [49]:         index_mat.add_special_dim(1, size[dim[0]:dim[0]+1])
        [50]:         obj = design_mat[(slice(None),) * dim[0] + (index_mat,)]
        [51]:     if len(dim) == 2:
        [52]:         n = min_(size[dim[0]], size[dim[1]]) - abs_(diagonal)
        [53]:         if dim[0] > dim[1]:  dim = dim[::-1]
        [54]:         if diagonal >= 0:  index_x = arange(n, device=input.device).special_from(size[dim[0]:dim[0]+1]); index_y = arange(diagonal, diagonal+n, device=input.device).special_from(size[dim[1]:dim[1]+1])
        [55]:         else:  index_x = arange(-diagonal, -diagonal+n, device=input.device).special_from(size[dim[0]:dim[0]+1]); index_y = arange(n, device=input.device).special_from(size[dim[1]:dim[1]+1])
        [56]:         with input.hide_special(), index_x.hide_special(), index_y.hide_special():
        [57]:             diag_mat = input[(slice(None),) * dim[0] + (index_x,) + (slice(None),) * (dim[1] - dim[0] - 1) + (index_y,)]
        [58]:         if dim[1] - dim[0] - 1 == 0: torch_returned = True; obj = diag_mat.special_from(remove_dim(input.shape, [dim[1]]))
        [59]:         else: torch_returned = True; obj = diag_mat.move_dim(0, dim[0]).special_from(remove_dim(input.shape, [dim[1]]))
        [60]:
        [61]:     def update_special(obj, updates):
        [62]:         if isinstance(obj, tuple):
        [63]:             for x in obj: update_special(x, updates)
        [64]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [65]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [66]:
        [67]:     if getattr(obj, 'grad_fn', None) is not None:
        [68]:         obj.grad_fn_name = "diag"
        [69]:     return obj
        [70]:
        The document of the original function is:

        diag(input, diagonal=0, *, out=None) -> Tensor

        - If :attr:`input` is a vector (1-D tensor), then returns a 2-D square tensor
          with the elements of :attr:`input` as the diagonal.
        - If :attr:`input` is a matrix (2-D tensor), then returns a 1-D tensor with
          the diagonal elements of :attr:`input`.

        The argument :attr:`diagonal` controls which diagonal to consider:

        - If :attr:`diagonal` = 0, it is the main diagonal.
        - If :attr:`diagonal` > 0, it is above the main diagonal.
        - If :attr:`diagonal` < 0, it is below the main diagonal.

        Args:
            input (Tensor): the input tensor.
            diagonal (int, optional): the diagonal to consider

        Keyword args:
            out (Tensor, optional): the output tensor.

        .. seealso::

                :func:`torch.diagonal` always returns the diagonal of its input.

                :func:`torch.diagflat` always constructs a tensor with diagonal elements
                specified by the input.

        Examples:

        Get the square matrix where the input vector is the diagonal::

            >>> a = torch.randn(3)
            >>> a
            tensor([ 0.5950,-0.0872, 2.3298])
            >>> torch.diag(a)
            tensor([[ 0.5950, 0.0000, 0.0000],
                    [ 0.0000,-0.0872, 0.0000],
                    [ 0.0000, 0.0000, 2.3298]])
            >>> torch.diag(a, 1)
            tensor([[ 0.0000, 0.5950, 0.0000, 0.0000],
                    [ 0.0000, 0.0000,-0.0872, 0.0000],
                    [ 0.0000, 0.0000, 0.0000, 2.3298],
                    [ 0.0000, 0.0000, 0.0000, 0.0000]])

        Get the k-th diagonal of a given matrix::

            >>> a = torch.randn(3, 3)
            >>> a
            tensor([[-0.4264, 0.0255,-0.1064],
                    [ 0.8795,-0.2429, 0.1374],
                    [ 0.1029,-0.6482,-1.6300]])
            >>> torch.diag(a, 0)
            tensor([-0.4264,-0.2429,-1.6300])
            >>> torch.diag(a, 1)
            tensor([ 0.0255, 0.1374])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = linalg_dim[1,2](input, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    size = input.shape
    if len(dim) == 1:
        n = size[dim[0]]
        design_mat = cat(zeros(size.with_dim_size(dim[0], 1), device=input.device, dtype=input.dtype), input, dim[0])
        index_mat = zeros(n + abs_(diagonal), n + abs_(diagonal), device=input.device).long()
        if diagonal >= 0:  index_mat[arange(n, device=input.device), arange(diagonal, diagonal+n, device=input.device)] = arange(n, device=input.device) + 1
        else:  index_mat[arange(-diagonal, -diagonal+n, device=input.device), arange(n, device=input.device)] = arange(n, device=input.device) + 1
        index_mat.add_special_dim(0, size[dim[0]:dim[0]+1])
        index_mat.add_special_dim(1, size[dim[0]:dim[0]+1])
        obj = design_mat[(slice(None),) * dim[0] + (index_mat,)]
    if len(dim) == 2:
        n = min_(size[dim[0]], size[dim[1]]) - abs_(diagonal)
        if dim[0] > dim[1]:  dim = dim[::-1]
        if diagonal >= 0:  index_x = arange(n, device=input.device).special_from(size[dim[0]:dim[0]+1]); index_y = arange(diagonal, diagonal+n, device=input.device).special_from(size[dim[1]:dim[1]+1])
        else:  index_x = arange(-diagonal, -diagonal+n, device=input.device).special_from(size[dim[0]:dim[0]+1]); index_y = arange(n, device=input.device).special_from(size[dim[1]:dim[1]+1])
        with input.hide_special(), index_x.hide_special(), index_y.hide_special():
            diag_mat = input[(slice(None),) * dim[0] + (index_x,) + (slice(None),) * (dim[1] - dim[0] - 1) + (index_y,)]
        if dim[1] - dim[0] - 1 == 0: torch_returned = True; obj = diag_mat.special_from(remove_dim(input.shape, [dim[1]]))
        else: torch_returned = True; obj = diag_mat.move_dim(0, dim[0]).special_from(remove_dim(input.shape, [dim[1]]))

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "diag"
    return obj

def diagflat(input: 'Tensor', diagonal=0, dim: exist_dim=[], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.diagflat'. The automatically generated codes are as follows:
        [ 1]: def diagflat(input: 'Tensor', diagonal=0, dim: exist_dim=[], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     dim = exist_dim(input, dim)
        [19]:
        [20]:     # Obtain available sized in arguments (which will be fed into size function).
        [21]:     input_shape=None if input is None else Size(input.shape)
        [22]:     # Use the given inner codes if they are provided.
        [23]:     torch_returned = False
        [24]:     obj = diag(input.flatten(*dim), diagonal=diagonal) # suppress:  special_from
        [25]:
        [26]:     def update_special(obj, updates):
        [27]:         if isinstance(obj, tuple):
        [28]:             for x in obj: update_special(x, updates)
        [29]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [30]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [31]:
        [32]:     if getattr(obj, 'grad_fn', None) is not None:
        [33]:         obj.grad_fn_name = "diagflat"
        [34]:     return obj
        [35]:
        The document of the original function is:

        diagflat(input, offset=0) -> Tensor

        - If :attr:`input` is a vector (1-D tensor), then returns a 2-D square tensor
          with the elements of :attr:`input` as the diagonal.
        - If :attr:`input` is a tensor with more than one dimension, then returns a
          2-D tensor with diagonal elements equal to a flattened :attr:`input`.

        The argument :attr:`offset` controls which diagonal to consider:

        - If :attr:`offset` = 0, it is the main diagonal.
        - If :attr:`offset` > 0, it is above the main diagonal.
        - If :attr:`offset` < 0, it is below the main diagonal.

        Args:
            input (Tensor): the input tensor.
            offset (int, optional): the diagonal to consider. Default: 0 (main
                diagonal).

        Examples::

            >>> a = torch.randn(3)
            >>> a
            tensor([-0.2956, -0.9068,  0.1695])
            >>> torch.diagflat(a)
            tensor([[-0.2956,  0.0000,  0.0000],
                    [ 0.0000, -0.9068,  0.0000],
                    [ 0.0000,  0.0000,  0.1695]])
            >>> torch.diagflat(a, 1)
            tensor([[ 0.0000, -0.2956,  0.0000,  0.0000],
                    [ 0.0000,  0.0000, -0.9068,  0.0000],
                    [ 0.0000,  0.0000,  0.0000,  0.1695],
                    [ 0.0000,  0.0000,  0.0000,  0.0000]])

            >>> a = torch.randn(2, 2)
            >>> a
            tensor([[ 0.2094, -0.3018],
                    [-0.1516,  1.9342]])
            >>> torch.diagflat(a)
            tensor([[ 0.2094,  0.0000,  0.0000,  0.0000],
                    [ 0.0000, -0.3018,  0.0000,  0.0000],
                    [ 0.0000,  0.0000, -0.1516,  0.0000],
                    [ 0.0000,  0.0000,  0.0000,  1.9342]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = exist_dim(input, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    obj = diag(input.flatten(*dim), diagonal=diagonal) # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "diagflat"
    return obj

def diagonal(input: 'Tensor', diagonal=0, dim1=0, dim2=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.diagonal'. The automatically generated codes are as follows:
        [ 1]: def diagonal(input: 'Tensor', diagonal=0, dim1=0, dim2=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     torch_returned = False
        [22]:     obj = diag(input, diagonal=diagonal, dim=(dim1, dim2)) # suppress:  special_from
        [23]:
        [24]:     def update_special(obj, updates):
        [25]:         if isinstance(obj, tuple):
        [26]:             for x in obj: update_special(x, updates)
        [27]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [28]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [29]:
        [30]:     if getattr(obj, 'grad_fn', None) is not None:
        [31]:         obj.grad_fn_name = "diagonal"
        [32]:     return obj
        [33]:
        The document of the original function is:

        diagonal(input, offset=0, dim1=0, dim2=1) -> Tensor

        Returns a partial view of :attr:`input` with the its diagonal elements
        with respect to :attr:`dim1` and :attr:`dim2` appended as a dimension
        at the end of the shape.

        The argument :attr:`offset` controls which diagonal to consider:

        - If :attr:`offset` = 0, it is the main diagonal.
        - If :attr:`offset` > 0, it is above the main diagonal.
        - If :attr:`offset` < 0, it is below the main diagonal.

        Applying :meth:`torch.diag_embed` to the output of this function with
        the same arguments yields a diagonal matrix with the diagonal entries
        of the input. However, :meth:`torch.diag_embed` has different default
        dimensions, so those need to be explicitly specified.

        Args:
            input (Tensor): the input tensor. Must be at least 2-dimensional.
            offset (int, optional): which diagonal to consider. Default: 0
                (main diagonal).
            dim1 (int, optional): first dimension with respect to which to
                take diagonal. Default: 0.
            dim2 (int, optional): second dimension with respect to which to
                take diagonal. Default: 1.

        .. note::  To take a batch diagonal, pass in dim1=-2, dim2=-1.

        Examples::

            >>> a = torch.randn(3, 3)
            >>> a
            tensor([[-1.0854,  1.1431, -0.1752],
                    [ 0.8536, -0.0905,  0.0360],
                    [ 0.6927, -0.3735, -0.4945]])

            >>> torch.diagonal(a, 0)
            tensor([-1.0854, -0.0905, -0.4945])

            >>> torch.diagonal(a, 1)
            tensor([ 1.1431,  0.0360])

            >>> x = torch.randn(2, 5, 4, 2)
            >>> torch.diagonal(x, offset=-1, dim1=1, dim2=2)
            tensor([[[-1.2631,  0.3755, -1.5977, -1.8172],
                     [-1.1065,  1.0401, -0.2235, -0.7938]],

                    [[-1.7325, -0.3081,  0.6166,  0.2335],
                     [ 1.0500,  0.7336, -0.3836, -1.1015]]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    obj = diag(input, diagonal=diagonal, dim=(dim1, dim2)) # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "diagonal"
    return obj

@alias('tr')
def trace(input: 'Tensor', dim:linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.trace'. The automatically generated codes are as follows:
        [ 1]: @alias('tr')
        [ 2]: def trace(input: 'Tensor', dim:linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:         ***
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in [input]:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     if input is None: ...
        [14]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [15]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [17]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [18]:
        [19]:     dim = linalg_dim[2](input, dim)
        [20]:
        [21]:     # Obtain available sized in arguments (which will be fed into size function).
        [22]:     input_shape=None if input is None else Size(input.shape)
        [23]:     # Use the given inner codes if they are provided.
        [24]:     torch_returned = False
        [25]:     obj = diag(input, dim=dim).sum(dim[0]) # suppress:  special_from
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "trace"
        [35]:     return obj
        [36]:
        The document of the original function is:

        trace(input) -> Tensor

        Returns the sum of the elements of the diagonal of the input 2-D matrix.

        Example::

            >>> x = torch.arange(1., 10.).view(3, 3)
            >>> x
            tensor([[ 1.,  2.,  3.],
                    [ 4.,  5.,  6.],
                    [ 7.,  8.,  9.]])
            >>> torch.trace(x)
            tensor(15.)

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = linalg_dim[2](input, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    obj = diag(input, dim=dim).sum(dim[0]) # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "trace"
    return obj

def det(input: 'Tensor', *, out=None, dim:linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.det'. The automatically generated codes are as follows:
        [ 1]: def det(input: 'Tensor', *, out=None, dim:linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     dim = linalg_dim[2](input, dim)
        [19]:
        [20]:     # Obtain available sized in arguments (which will be fed into size function).
        [21]:     input_shape=None if input is None else Size(input.shape)
        [22]:     # Use the given inner codes if they are provided.
        [23]:     torch_returned = False
        [24]:     avouch(len(dim) == 2 and dim[0] != dim[1], TypeError("bt.det accepts only two dimensions for determinant. "))
        [25]:     ref_shape = remove_dim(input.shape, dim)
        [26]:     with input.hide_special(), torch._C.DisableTorchFunction():
        [27]:         obj = Tensor.inherit_from(torch.det(input.move_dim(dim, -1)), input, shape=ref_shape) # suppress:  special_from
        [28]:
        [29]:     def update_special(obj, updates):
        [30]:         if isinstance(obj, tuple):
        [31]:             for x in obj: update_special(x, updates)
        [32]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [33]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [34]:
        [35]:     if getattr(obj, 'grad_fn', None) is not None:
        [36]:         obj.grad_fn_name = "det"
        [37]:     return obj
        [38]:
        The document of the original function is:

        det(input) -> Tensor

        Alias for :func:`torch.linalg.det`

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = linalg_dim[2](input, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    avouch(len(dim) == 2 and dim[0] != dim[1], TypeError("bt.det accepts only two dimensions for determinant. "))
    ref_shape = remove_dim(input.shape, dim)
    with input.hide_special(), torch._C.DisableTorchFunction():
        obj = Tensor.inherit_from(torch.det(input.move_dim(dim, -1)), input, shape=ref_shape) # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "det"
    return obj

# operations
def add(self: 'Tensor', other: 'Tensor', *, alpha=1, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.add'. The automatically generated codes are as follows:
        [ 1]: def add(self: 'Tensor', other: 'Tensor', *, alpha=1, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [self, other]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if self is None: ...
        [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [17]:
        [18]:     if other is None: ...
        [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
        [23]:
        [24]:     # Obtain available sized in arguments (which will be fed into size function).
        [25]:     self_shape=None if self is None else Size(self.shape)
        [26]:     other_shape=None if other is None else Size(other.shape)
        [27]:     # Use the given inner codes if they are provided.
        [28]:     ref_shape, self_shape, other_shape = size_mapping_op['add'](self_shape, other_shape)
        [29]:     self = self.view(self_shape)
        [30]:     other = other.view(other_shape)
        [31]:     with torch._C.DisableTorchFunction():
        [32]:         auto_generated_args = tuple(x for x in [self,other] if x is not None)
        [33]:         obj = torch.add(*auto_generated_args, alpha=alpha,out=out)
        [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [35]:
        [36]:     def update_special(obj, updates):
        [37]:         if isinstance(obj, tuple):
        [38]:             for x in obj: update_special(x, updates)
        [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [41]:
        [42]:     if getattr(obj, 'grad_fn', None) is not None:
        [43]:         obj.grad_fn_name = "add"
        [44]:     return obj
        [45]:
        The document of the original function is:

        add(input, other, *, alpha=1, out=None) -> Tensor

        Adds :attr:`other`, scaled by :attr:`alpha`, to :attr:`input`.

        .. math::
            \text{{out}}_i = \text{{input}}_i + \text{{alpha}} \times \text{{other}}_i

        Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
        :ref:`type promotion <type-promotion-doc>`, and integer, float, and complex inputs.

        Args:
            input (Tensor): the input tensor.
            other (Tensor or Number): the tensor or number to add to :attr:`input`.

        Keyword arguments:
            alpha (Number): the multiplier for :attr:`other`.
            out (Tensor, optional): the output tensor.

        Examples::

            >>> a = torch.randn(4)
            >>> a
            tensor([ 0.0202,  1.0985,  1.3506, -0.6056])
            >>> torch.add(a, 20)
            tensor([ 20.0202,  21.0985,  21.3506,  19.3944])

            >>> b = torch.randn(4)
            >>> b
            tensor([-0.9732, -0.3497,  0.6245,  0.4022])
            >>> c = torch.randn(4, 1)
            >>> c
            tensor([[ 0.3743],
                    [-1.7724],
                    [-0.5811],
                    [-0.8017]])
            >>> torch.add(b, c, alpha=10)
            tensor([[  2.7695,   3.3930,   4.3672,   4.1450],
                    [-18.6971, -18.0736, -17.0994, -17.3216],
                    [ -6.7845,  -6.1610,  -5.1868,  -5.4090],
                    [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self, other]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    if other is None: ...
    elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
    if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if other.device.type == 'cpu': other = other.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    other_shape=None if other is None else Size(other.shape)
    # Use the given inner codes if they are provided.
    ref_shape, self_shape, other_shape = size_mapping_op['add'](self_shape, other_shape)
    self = self.view(self_shape)
    other = other.view(other_shape)
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [self,other] if x is not None)
        obj = torch.add(*auto_generated_args, alpha=alpha,out=out)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "add"
    return obj

def sub(self: 'Tensor', other: 'Tensor', *, alpha=1, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.sub'. The automatically generated codes are as follows:
        [ 1]: def sub(self: 'Tensor', other: 'Tensor', *, alpha=1, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [self, other]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if self is None: ...
        [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [17]:
        [18]:     if other is None: ...
        [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
        [23]:
        [24]:     # Obtain available sized in arguments (which will be fed into size function).
        [25]:     self_shape=None if self is None else Size(self.shape)
        [26]:     other_shape=None if other is None else Size(other.shape)
        [27]:     # Use the given inner codes if they are provided.
        [28]:     ref_shape, self_shape, other_shape = size_mapping_op['sub'](self_shape, other_shape)
        [29]:     self = self.view(self_shape)
        [30]:     other = other.view(other_shape)
        [31]:     with torch._C.DisableTorchFunction():
        [32]:         auto_generated_args = tuple(x for x in [self,other] if x is not None)
        [33]:         obj = torch.sub(*auto_generated_args, alpha=alpha,out=out)
        [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [35]:
        [36]:     def update_special(obj, updates):
        [37]:         if isinstance(obj, tuple):
        [38]:             for x in obj: update_special(x, updates)
        [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [41]:
        [42]:     if getattr(obj, 'grad_fn', None) is not None:
        [43]:         obj.grad_fn_name = "sub"
        [44]:     return obj
        [45]:
        The document of the original function is:

        sub(input, other, *, alpha=1, out=None) -> Tensor

        Subtracts :attr:`other`, scaled by :attr:`alpha`, from :attr:`input`.

        .. math::
            \text{{out}}_i = \text{{input}}_i - \text{{alpha}} \times \text{{other}}_i

        Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
        :ref:`type promotion <type-promotion-doc>`, and integer, float, and complex inputs.

        Args:
            input (Tensor): the input tensor.
            other (Tensor or Number): the tensor or number to subtract from :attr:`input`.

        Keyword args:
            alpha (Number): the multiplier for :attr:`other`.
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.tensor((1, 2))
            >>> b = torch.tensor((0, 1))
            >>> torch.sub(a, b, alpha=2)
            tensor([1, 0])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self, other]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    if other is None: ...
    elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
    if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if other.device.type == 'cpu': other = other.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    other_shape=None if other is None else Size(other.shape)
    # Use the given inner codes if they are provided.
    ref_shape, self_shape, other_shape = size_mapping_op['sub'](self_shape, other_shape)
    self = self.view(self_shape)
    other = other.view(other_shape)
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [self,other] if x is not None)
        obj = torch.sub(*auto_generated_args, alpha=alpha,out=out)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "sub"
    return obj

def mul(self: 'Tensor', value: 'Tensor', out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.mul'. The automatically generated codes are as follows:
        [ 1]: def mul(self: 'Tensor', value: 'Tensor', out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [self, value]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if self is None: ...
        [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [17]:
        [18]:     if value is None: ...
        [19]:     elif not isinstance(value, torch.Tensor): value = torch.tensor(value)
        [20]:     if not isinstance(value, subclass): value = value.as_subclass(subclass).special_from(value.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if value.device.type == 'cpu': value = value.to(pivot.device)
        [23]:
        [24]:     # Obtain available sized in arguments (which will be fed into size function).
        [25]:     self_shape=None if self is None else Size(self.shape)
        [26]:     value_shape=None if value is None else Size(value.shape)
        [27]:     # Use the given inner codes if they are provided.
        [28]:     ref_shape, self_shape, value_shape = size_mapping_op['mul'](self_shape, value_shape)
        [29]:     self = self.view(self_shape)
        [30]:     value = value.view(value_shape)
        [31]:     with torch._C.DisableTorchFunction():
        [32]:         auto_generated_args = tuple(x for x in [self,value] if x is not None)
        [33]:         obj = torch.mul(*auto_generated_args, out=out)
        [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [35]:
        [36]:     def update_special(obj, updates):
        [37]:         if isinstance(obj, tuple):
        [38]:             for x in obj: update_special(x, updates)
        [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [41]:
        [42]:     if getattr(obj, 'grad_fn', None) is not None:
        [43]:         obj.grad_fn_name = "mul"
        [44]:     return obj
        [45]:
        The document of the original function is:

        mul(input, other, *, out=None) -> Tensor

        Multiplies :attr:`input` by :attr:`other`.

        .. math::
            \text{out}_i = \text{input}_i \times \text{other}_i

        Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
        :ref:`type promotion <type-promotion-doc>`, and integer, float, and complex inputs.

        Args:
            input (Tensor): the input tensor.
            other (Tensor or Number) - the tensor or number to multiply input by.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Examples::

            >>> a = torch.randn(3)
            >>> a
            tensor([ 0.2015, -0.4255,  2.6087])
            >>> torch.mul(a, 100)
            tensor([  20.1494,  -42.5491,  260.8663])

            >>> b = torch.randn(4, 1)
            >>> b
            tensor([[ 1.1207],
                    [-0.3137],
                    [ 0.0700],
                    [ 0.8378]])
            >>> c = torch.randn(1, 4)
            >>> c
            tensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])
            >>> torch.mul(b, c)
            tensor([[ 0.5767,  0.1363, -0.5877,  2.5083],
                    [-0.1614, -0.0382,  0.1645, -0.7021],
                    [ 0.0360,  0.0085, -0.0367,  0.1567],
                    [ 0.4312,  0.1019, -0.4394,  1.8753]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self, value]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    if value is None: ...
    elif not isinstance(value, torch.Tensor): value = torch.tensor(value)
    if not isinstance(value, subclass): value = value.as_subclass(subclass).special_from(value.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if value.device.type == 'cpu': value = value.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    value_shape=None if value is None else Size(value.shape)
    # Use the given inner codes if they are provided.
    ref_shape, self_shape, value_shape = size_mapping_op['mul'](self_shape, value_shape)
    self = self.view(self_shape)
    value = value.view(value_shape)
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [self,value] if x is not None)
        obj = torch.mul(*auto_generated_args, out=out)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "mul"
    return obj

def div(self: 'Tensor', value: 'Tensor', *, rounding_mode=None, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.div'. The automatically generated codes are as follows:
        [ 1]: def div(self: 'Tensor', value: 'Tensor', *, rounding_mode=None, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [self, value]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if self is None: ...
        [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [17]:
        [18]:     if value is None: ...
        [19]:     elif not isinstance(value, torch.Tensor): value = torch.tensor(value)
        [20]:     if not isinstance(value, subclass): value = value.as_subclass(subclass).special_from(value.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if value.device.type == 'cpu': value = value.to(pivot.device)
        [23]:
        [24]:     # Obtain available sized in arguments (which will be fed into size function).
        [25]:     self_shape=None if self is None else Size(self.shape)
        [26]:     value_shape=None if value is None else Size(value.shape)
        [27]:     # Use the given inner codes if they are provided.
        [28]:     ref_shape, self_shape, value_shape = size_mapping_op['div'](self_shape, value_shape)
        [29]:     self = self.view(self_shape)
        [30]:     value = value.view(value_shape)
        [31]:     with torch._C.DisableTorchFunction():
        [32]:         auto_generated_args = tuple(x for x in [self,value] if x is not None)
        [33]:         obj = torch.div(*auto_generated_args, rounding_mode=rounding_mode,out=out)
        [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [35]:
        [36]:     def update_special(obj, updates):
        [37]:         if isinstance(obj, tuple):
        [38]:             for x in obj: update_special(x, updates)
        [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [41]:
        [42]:     if getattr(obj, 'grad_fn', None) is not None:
        [43]:         obj.grad_fn_name = "div"
        [44]:     return obj
        [45]:
        The document of the original function is:

        div(input, other, *, rounding_mode=None, out=None) -> Tensor

        Divides each element of the input ``input`` by the corresponding element of
        :attr:`other`.

        .. math::
            \text{out}_i = \frac{\text{input}_i}{\text{other}_i}

        .. note::
            By default, this performs a "true" division like Python 3.
            See the :attr:`rounding_mode` argument for floor division.

        Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
        :ref:`type promotion <type-promotion-doc>`, and integer, float, and complex inputs.
        Always promotes integer types to the default scalar type.

        Args:
            input (Tensor): the dividend
            other (Tensor or Number): the divisor

        Keyword args:
            rounding_mode (str, optional): Type of rounding applied to the result:

                * None - default behavior. Performs no rounding and, if both :attr:`input` and
                  :attr:`other` are integer types, promotes the inputs to the default scalar type.
                  Equivalent to true division in Python (the ``/`` operator) and NumPy's ``np.true_divide``.
                * ``"trunc"`` - rounds the results of the division towards zero.
                  Equivalent to C-style integer division.
                * ``"floor"`` - rounds the results of the division down.
                  Equivalent to floor division in Python (the ``//`` operator) and NumPy's ``np.floor_divide``.

            out (Tensor, optional): the output tensor.

        Examples::

            >>> x = torch.tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])
            >>> torch.div(x, 0.5)
            tensor([ 0.7620,  2.5548, -0.5944, -0.7438,  0.9274])

            >>> a = torch.tensor([[-0.3711, -1.9353, -0.4605, -0.2917],
            ...                   [ 0.1815, -1.0111,  0.9805, -1.5923],
            ...                   [ 0.1062,  1.4581,  0.7759, -1.2344],
            ...                   [-0.1830, -0.0313,  1.1908, -1.4757]])
            >>> b = torch.tensor([ 0.8032,  0.2930, -0.8113, -0.2308])
            >>> torch.div(a, b)
            tensor([[-0.4620, -6.6051,  0.5676,  1.2639],
                    [ 0.2260, -3.4509, -1.2086,  6.8990],
                    [ 0.1322,  4.9764, -0.9564,  5.3484],
                    [-0.2278, -0.1068, -1.4678,  6.3938]])

            >>> torch.div(a, b, rounding_mode='trunc')
            tensor([[-0., -6.,  0.,  1.],
                    [ 0., -3., -1.,  6.],
                    [ 0.,  4., -0.,  5.],
                    [-0., -0., -1.,  6.]])

            >>> torch.div(a, b, rounding_mode='floor')
            tensor([[-1., -7.,  0.,  1.],
                    [ 0., -4., -2.,  6.],
                    [ 0.,  4., -1.,  5.],
                    [-1., -1., -2.,  6.]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self, value]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    if value is None: ...
    elif not isinstance(value, torch.Tensor): value = torch.tensor(value)
    if not isinstance(value, subclass): value = value.as_subclass(subclass).special_from(value.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if value.device.type == 'cpu': value = value.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    value_shape=None if value is None else Size(value.shape)
    # Use the given inner codes if they are provided.
    ref_shape, self_shape, value_shape = size_mapping_op['div'](self_shape, value_shape)
    self = self.view(self_shape)
    value = value.view(value_shape)
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [self,value] if x is not None)
        obj = torch.div(*auto_generated_args, rounding_mode=rounding_mode,out=out)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "div"
    return obj

def pow(input: 'Tensor', exponent, *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.pow'. The automatically generated codes are as follows:
        [ 1]: def pow(input: 'Tensor', exponent, *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input,exponent] if x is not None)
        [23]:         obj = torch.pow(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['pow'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "pow"
        [35]:     return obj
        [36]:
        The document of the original function is:

        pow(input, exponent, *, out=None) -> Tensor

        Takes the power of each element in :attr:`input` with :attr:`exponent` and
        returns a tensor with the result.

        :attr:`exponent` can be either a single ``float`` number or a `Tensor`
        with the same number of elements as :attr:`input`.

        When :attr:`exponent` is a scalar value, the operation applied is:

        .. math::
            \text{out}_i = x_i ^ \text{exponent}

        When :attr:`exponent` is a tensor, the operation applied is:

        .. math::
            \text{out}_i = x_i ^ {\text{exponent}_i}

        When :attr:`exponent` is a tensor, the shapes of :attr:`input`
        and :attr:`exponent` must be :ref:`broadcastable <broadcasting-semantics>`.

        Args:
            input (Tensor): the input tensor.
            exponent (float or tensor): the exponent value

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.randn(4)
            >>> a
            tensor([ 0.4331,  1.2475,  0.6834, -0.2791])
            >>> torch.pow(a, 2)
            tensor([ 0.1875,  1.5561,  0.4670,  0.0779])
            >>> exp = torch.arange(1., 5.)

            >>> a = torch.arange(1., 5.)
            >>> a
            tensor([ 1.,  2.,  3.,  4.])
            >>> exp
            tensor([ 1.,  2.,  3.,  4.])
            >>> torch.pow(a, exp)
            tensor([   1.,    4.,   27.,  256.])

        .. function:: pow(self, exponent, *, out=None) -> Tensor
           :noindex:

        :attr:`self` is a scalar ``float`` value, and :attr:`exponent` is a tensor.
        The returned tensor :attr:`out` is of the same shape as :attr:`exponent`

        The operation applied is:

        .. math::
            \text{out}_i = \text{self} ^ {\text{exponent}_i}

        Args:
            self (float): the scalar base value for the power operation
            exponent (Tensor): the exponent tensor

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> exp = torch.arange(1., 5.)
            >>> base = 2
            >>> torch.pow(base, exp)
            tensor([  2.,   4.,   8.,  16.])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input,exponent] if x is not None)
        obj = torch.pow(*auto_generated_args, out=out)
    ref_shape = size_mapping['pow'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "pow"
    return obj

def fmod(self: 'Tensor', other: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.fmod'. The automatically generated codes are as follows:
        [ 1]: def fmod(self: 'Tensor', other: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [self, other]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if self is None: ...
        [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [17]:
        [18]:     if other is None: ...
        [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
        [23]:
        [24]:     # Obtain available sized in arguments (which will be fed into size function).
        [25]:     self_shape=None if self is None else Size(self.shape)
        [26]:     other_shape=None if other is None else Size(other.shape)
        [27]:     # Use the given inner codes if they are provided.
        [28]:     ref_shape, self_shape, other_shape = size_mapping_op['fmod'](self_shape, other_shape)
        [29]:     self = self.view(self_shape)
        [30]:     other = other.view(other_shape)
        [31]:     with torch._C.DisableTorchFunction():
        [32]:         auto_generated_args = tuple(x for x in [self,other] if x is not None)
        [33]:         obj = torch.fmod(*auto_generated_args, out=out)
        [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [35]:
        [36]:     def update_special(obj, updates):
        [37]:         if isinstance(obj, tuple):
        [38]:             for x in obj: update_special(x, updates)
        [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [41]:
        [42]:     if getattr(obj, 'grad_fn', None) is not None:
        [43]:         obj.grad_fn_name = "fmod"
        [44]:     return obj
        [45]:
        The document of the original function is:

        fmod(input, other, *, out=None) -> Tensor

        Applies C++'s `std::fmod <https://en.cppreference.com/w/cpp/numeric/math/fmod>`_ entrywise.
        The result has the same sign as the dividend :attr:`input` and its absolute value
        is less than that of :attr:`other`.

        This function may be defined in terms of :func:`torch.div` as

        .. code:: python

            torch.fmod(a, b) == a - a.div(b, rounding_mode="trunc") * b

        Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
        :ref:`type promotion <type-promotion-doc>`, and integer and float inputs.

        .. note::

            When the divisor is zero, returns ``NaN`` for floating point dtypes
            on both CPU and GPU; raises ``RuntimeError`` for integer division by
            zero on CPU; Integer division by zero on GPU may return any value.

        .. note::

           Complex inputs are not supported. In some cases, it is not mathematically
           possible to satisfy the definition of a modulo operation with complex numbers.

        .. seealso::

            :func:`torch.remainder` which implements Python's modulus operator.
            This one is defined using division rounding down the result.

        Args:
            input (Tensor): the dividend
            other (Tensor or Scalar): the divisor

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)
            tensor([-1., -0., -1.,  1.,  0.,  1.])
            >>> torch.fmod(torch.tensor([1, 2, 3, 4, 5]), -1.5)
            tensor([1.0000, 0.5000, 0.0000, 1.0000, 0.5000])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self, other]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    if other is None: ...
    elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
    if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if other.device.type == 'cpu': other = other.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    other_shape=None if other is None else Size(other.shape)
    # Use the given inner codes if they are provided.
    ref_shape, self_shape, other_shape = size_mapping_op['fmod'](self_shape, other_shape)
    self = self.view(self_shape)
    other = other.view(other_shape)
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [self,other] if x is not None)
        obj = torch.fmod(*auto_generated_args, out=out)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "fmod"
    return obj

def log(input: 'Tensor', base=torch.e, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.log'. The automatically generated codes are as follows:
        [ 1]: def log(input: 'Tensor', base=torch.e, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     torch_returned = False
        [22]:     with torch._C.DisableTorchFunction():
        [23]:         obj = torch.log(input).as_subclass(torch.Tensor) / torch.log(torch.tensor(base))
        [24]:     ref_shape = size_mapping['log'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "log"
        [35]:     return obj
        [36]:
        The document of the original function is:

        log(input, *, out=None) -> Tensor

        Returns a new tensor with the natural logarithm of the elements
        of :attr:`input`.

        .. math::
            y_{i} = \log_{e} (x_{i})

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.rand(5) * 5
            >>> a
            tensor([4.7767, 4.3234, 1.2156, 0.2411, 4.5739])
            >>> torch.log(a)
            tensor([ 1.5637,  1.4640,  0.1952, -1.4226,  1.5204])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    with torch._C.DisableTorchFunction():
        obj = torch.log(input).as_subclass(torch.Tensor) / torch.log(torch.tensor(base))
    ref_shape = size_mapping['log'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "log"
    return obj

def ln(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.ln'. The automatically generated codes are as follows:
        [ 1]: def ln(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     torch_returned = False
        [22]:     with torch._C.DisableTorchFunction():
        [23]:         obj = torch.log(input).as_subclass(torch.Tensor)
        [24]:     ref_shape = size_mapping['ln'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "ln"
        [35]:     return obj
        [36]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    with torch._C.DisableTorchFunction():
        obj = torch.log(input).as_subclass(torch.Tensor)
    ref_shape = size_mapping['ln'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "ln"
    return obj

def log2(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.log2'. The automatically generated codes are as follows:
        [ 1]: def log2(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.log2(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['log2'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "log2"
        [35]:     return obj
        [36]:
        The document of the original function is:

        log2(input, *, out=None) -> Tensor

        Returns a new tensor with the logarithm to the base 2 of the elements
        of :attr:`input`.

        .. math::
            y_{i} = \log_{2} (x_{i})

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.rand(5)
            >>> a
            tensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])

            >>> torch.log2(a)
            tensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.log2(*auto_generated_args, out=out)
    ref_shape = size_mapping['log2'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "log2"
    return obj

def log10(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.log10'. The automatically generated codes are as follows:
        [ 1]: def log10(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.log10(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['log10'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "log10"
        [35]:     return obj
        [36]:
        The document of the original function is:

        log10(input, *, out=None) -> Tensor

        Returns a new tensor with the logarithm to the base 10 of the elements
        of :attr:`input`.

        .. math::
            y_{i} = \log_{10} (x_{i})

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.rand(5)
            >>> a
            tensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])

            >>> torch.log10(a)
            tensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.log10(*auto_generated_args, out=out)
    ref_shape = size_mapping['log10'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "log10"
    return obj

def exp(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.exp'. The automatically generated codes are as follows:
        [ 1]: def exp(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.exp(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['exp'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "exp"
        [35]:     return obj
        [36]:
        The document of the original function is:

        exp(input, *, out=None) -> Tensor

        Returns a new tensor with the exponential of the elements
        of the input tensor :attr:`input`.

        .. math::
            y_{i} = e^{x_{i}}

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> torch.exp(torch.tensor([0, math.log(2.)]))
            tensor([ 1.,  2.])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.exp(*auto_generated_args, out=out)
    ref_shape = size_mapping['exp'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "exp"
    return obj

def square(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.square'. The automatically generated codes are as follows:
        [ 1]: def square(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.square(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['square'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "square"
        [35]:     return obj
        [36]:
        The document of the original function is:

        square(input, *, out=None) -> Tensor

        Returns a new tensor with the square of the elements of :attr:`input`.

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.randn(4)
            >>> a
            tensor([-2.0755,  1.0226,  0.0831,  0.4806])
            >>> torch.square(a)
            tensor([ 4.3077,  1.0457,  0.0069,  0.2310])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.square(*auto_generated_args, out=out)
    ref_shape = size_mapping['square'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "square"
    return obj

def sqrt(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.sqrt'. The automatically generated codes are as follows:
        [ 1]: def sqrt(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.sqrt(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['sqrt'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "sqrt"
        [35]:     return obj
        [36]:
        The document of the original function is:

        sqrt(input, *, out=None) -> Tensor

        Returns a new tensor with the square-root of the elements of :attr:`input`.

        .. math::
            \text{out}_{i} = \sqrt{\text{input}_{i}}

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.randn(4)
            >>> a
            tensor([-2.0755,  1.0226,  0.0831,  0.4806])
            >>> torch.sqrt(a)
            tensor([    nan,  1.0112,  0.2883,  0.6933])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.sqrt(*auto_generated_args, out=out)
    ref_shape = size_mapping['sqrt'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "sqrt"
    return obj

def abs(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.abs'. The automatically generated codes are as follows:
        [ 1]: def abs(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.abs(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['abs'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "abs"
        [35]:     return obj
        [36]:
        The document of the original function is:

        abs(input, *, out=None) -> Tensor

        Computes the absolute value of each element in :attr:`input`.

        .. math::
            \text{out}_{i} = |\text{input}_{i}|

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> torch.abs(torch.tensor([-1, -2, 3]))
            tensor([ 1,  2,  3])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.abs(*auto_generated_args, out=out)
    ref_shape = size_mapping['abs'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "abs"
    return obj

def sign(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.sign'. The automatically generated codes are as follows:
        [ 1]: def sign(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.sign(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['sign'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "sign"
        [35]:     return obj
        [36]:
        The document of the original function is:

        sign(input, *, out=None) -> Tensor

        Returns a new tensor with the signs of the elements of :attr:`input`.

        .. math::
            \text{out}_{i} = \operatorname{sgn}(\text{input}_{i})

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.tensor([0.7, -1.2, 0., 2.3])
            >>> a
            tensor([ 0.7000, -1.2000,  0.0000,  2.3000])
            >>> torch.sign(a)
            tensor([ 1., -1.,  0.,  1.])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.sign(*auto_generated_args, out=out)
    ref_shape = size_mapping['sign'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "sign"
    return obj

def sin(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.sin'. The automatically generated codes are as follows:
        [ 1]: def sin(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.sin(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['sin'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "sin"
        [35]:     return obj
        [36]:
        The document of the original function is:

        sin(input, *, out=None) -> Tensor

        Returns a new tensor with the sine of the elements of :attr:`input`.

        .. math::
            \text{out}_{i} = \sin(\text{input}_{i})

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.randn(4)
            >>> a
            tensor([-0.5461,  0.1347, -2.7266, -0.2746])
            >>> torch.sin(a)
            tensor([-0.5194,  0.1343, -0.4032, -0.2711])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.sin(*auto_generated_args, out=out)
    ref_shape = size_mapping['sin'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "sin"
    return obj

def cos(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.cos'. The automatically generated codes are as follows:
        [ 1]: def cos(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.cos(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['cos'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "cos"
        [35]:     return obj
        [36]:
        The document of the original function is:

        cos(input, *, out=None) -> Tensor

        Returns a new tensor with the cosine  of the elements of :attr:`input`.

        .. math::
            \text{out}_{i} = \cos(\text{input}_{i})

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.randn(4)
            >>> a
            tensor([ 1.4309,  1.2706, -0.8562,  0.9796])
            >>> torch.cos(a)
            tensor([ 0.1395,  0.2957,  0.6553,  0.5574])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.cos(*auto_generated_args, out=out)
    ref_shape = size_mapping['cos'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "cos"
    return obj

def tan(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.tan'. The automatically generated codes are as follows:
        [ 1]: def tan(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.tan(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['tan'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "tan"
        [35]:     return obj
        [36]:
        The document of the original function is:

        tan(input, *, out=None) -> Tensor

        Returns a new tensor with the tangent of the elements of :attr:`input`.

        .. math::
            \text{out}_{i} = \tan(\text{input}_{i})

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.randn(4)
            >>> a
            tensor([-1.2027, -1.7687,  0.4412, -1.3856])
            >>> torch.tan(a)
            tensor([-2.5930,  4.9859,  0.4722, -5.3366])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.tan(*auto_generated_args, out=out)
    ref_shape = size_mapping['tan'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "tan"
    return obj

def cot(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.cot'. The automatically generated codes are as follows:
        [ 1]: def cot(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     torch_returned = False
        [22]:     with torch._C.DisableTorchFunction():
        [23]:         obj = 1 / torch.tan(input)
        [24]:     ref_shape = size_mapping['cot'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "cot"
        [35]:     return obj
        [36]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    with torch._C.DisableTorchFunction():
        obj = 1 / torch.tan(input)
    ref_shape = size_mapping['cot'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "cot"
    return obj

def sec(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.sec'. The automatically generated codes are as follows:
        [ 1]: def sec(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     torch_returned = False
        [22]:     with torch._C.DisableTorchFunction():
        [23]:         obj = 1 / torch.cos(input)
        [24]:     ref_shape = size_mapping['sec'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "sec"
        [35]:     return obj
        [36]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    with torch._C.DisableTorchFunction():
        obj = 1 / torch.cos(input)
    ref_shape = size_mapping['sec'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "sec"
    return obj

def csc(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.csc'. The automatically generated codes are as follows:
        [ 1]: def csc(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     torch_returned = False
        [22]:     with torch._C.DisableTorchFunction():
        [23]:         obj = 1 / torch.sin(input)
        [24]:     ref_shape = size_mapping['csc'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "csc"
        [35]:     return obj
        [36]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    with torch._C.DisableTorchFunction():
        obj = 1 / torch.sin(input)
    ref_shape = size_mapping['csc'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "csc"
    return obj

def asin(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.asin'. The automatically generated codes are as follows:
        [ 1]: def asin(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.asin(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['asin'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "asin"
        [35]:     return obj
        [36]:
        The document of the original function is:

        asin(input, *, out=None) -> Tensor

        Returns a new tensor with the arcsine of the elements of :attr:`input`.

        .. math::
            \text{out}_{i} = \sin^{-1}(\text{input}_{i})

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.randn(4)
            >>> a
            tensor([-0.5962,  1.4985, -0.4396,  1.4525])
            >>> torch.asin(a)
            tensor([-0.6387,     nan, -0.4552,     nan])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.asin(*auto_generated_args, out=out)
    ref_shape = size_mapping['asin'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "asin"
    return obj

def acos(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.acos'. The automatically generated codes are as follows:
        [ 1]: def acos(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.acos(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['acos'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "acos"
        [35]:     return obj
        [36]:
        The document of the original function is:

        acos(input, *, out=None) -> Tensor

        Computes the inverse cosine of each element in :attr:`input`.

        .. math::
            \text{out}_{i} = \cos^{-1}(\text{input}_{i})

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.randn(4)
            >>> a
            tensor([ 0.3348, -0.5889,  0.2005, -0.1584])
            >>> torch.acos(a)
            tensor([ 1.2294,  2.2004,  1.3690,  1.7298])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.acos(*auto_generated_args, out=out)
    ref_shape = size_mapping['acos'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "acos"
    return obj

def atan(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.atan'. The automatically generated codes are as follows:
        [ 1]: def atan(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.atan(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['atan'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "atan"
        [35]:     return obj
        [36]:
        The document of the original function is:

        atan(input, *, out=None) -> Tensor

        Returns a new tensor with the arctangent of the elements of :attr:`input`.

        .. math::
            \text{out}_{i} = \tan^{-1}(\text{input}_{i})

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.randn(4)
            >>> a
            tensor([ 0.2341,  0.2539, -0.6256, -0.6448])
            >>> torch.atan(a)
            tensor([ 0.2299,  0.2487, -0.5591, -0.5727])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.atan(*auto_generated_args, out=out)
    ref_shape = size_mapping['atan'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "atan"
    return obj

def arcsin(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.arcsin'. The automatically generated codes are as follows:
        [ 1]: def arcsin(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.arcsin(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['arcsin'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "arcsin"
        [35]:     return obj
        [36]:
        The document of the original function is:

        arcsin(input, *, out=None) -> Tensor

        Alias for :func:`torch.asin`.

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.arcsin(*auto_generated_args, out=out)
    ref_shape = size_mapping['arcsin'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "arcsin"
    return obj

def arccos(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.arccos'. The automatically generated codes are as follows:
        [ 1]: def arccos(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.arccos(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['arccos'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "arccos"
        [35]:     return obj
        [36]:
        The document of the original function is:

        arccos(input, *, out=None) -> Tensor

        Alias for :func:`torch.acos`.

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.arccos(*auto_generated_args, out=out)
    ref_shape = size_mapping['arccos'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "arccos"
    return obj

def arctan(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.arctan'. The automatically generated codes are as follows:
        [ 1]: def arctan(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.arctan(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['arctan'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "arctan"
        [35]:     return obj
        [36]:
        The document of the original function is:

        arctan(input, *, out=None) -> Tensor

        Alias for :func:`torch.atan`.

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.arctan(*auto_generated_args, out=out)
    ref_shape = size_mapping['arctan'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "arctan"
    return obj

def mm(input: 'Tensor', other: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.mm'. The automatically generated codes are as follows:
        [ 1]: def mm(input: 'Tensor', other: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input, other]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     if other is None: ...
        [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
        [23]:
        [24]:     # Obtain available sized in arguments (which will be fed into size function).
        [25]:     input_shape=None if input is None else Size(input.shape)
        [26]:     other_shape=None if other is None else Size(other.shape)
        [27]:     # Use the given inner codes if they are provided.
        [28]:     ref_shape, input_shape, other_shape = size_mapping_op['mm'](input_shape, other_shape)
        [29]:     input = input.view(input_shape)
        [30]:     other = other.view(other_shape)
        [31]:     with torch._C.DisableTorchFunction():
        [32]:         auto_generated_args = tuple(x for x in [input,other] if x is not None)
        [33]:         obj = torch.mm(*auto_generated_args, out=out)
        [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [35]:
        [36]:     def update_special(obj, updates):
        [37]:         if isinstance(obj, tuple):
        [38]:             for x in obj: update_special(x, updates)
        [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [41]:
        [42]:     if getattr(obj, 'grad_fn', None) is not None:
        [43]:         obj.grad_fn_name = "mm"
        [44]:     return obj
        [45]:
        The document of the original function is:

        mm(input, mat2, *, out=None) -> Tensor

        Performs a matrix multiplication of the matrices :attr:`input` and :attr:`mat2`.

        If :attr:`input` is a :math:`(n \times m)` tensor, :attr:`mat2` is a
        :math:`(m \times p)` tensor, :attr:`out` will be a :math:`(n \times p)` tensor.

        .. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.
                  For broadcasting matrix products, see :func:`torch.matmul`.

        Supports strided and sparse 2-D tensors as inputs, autograd with
        respect to strided inputs.

        This operation has support for arguments with :ref:`sparse layouts<sparse-docs>`.
        If :attr:`out` is provided it's layout will be used. Otherwise, the result
        layout will be deduced from that of :attr:`input`.

        .. warning::
            Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported,
            or may not have autograd support. If you notice missing functionality please
            open a feature request.

        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

        On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

        Args:
            input (Tensor): the first matrix to be matrix multiplied
            mat2 (Tensor): the second matrix to be matrix multiplied

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> mat1 = torch.randn(2, 3)
            >>> mat2 = torch.randn(3, 3)
            >>> torch.mm(mat1, mat2)
            tensor([[ 0.4851,  0.5037, -0.3633],
                    [-0.0760, -3.6705,  2.4784]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input, other]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    if other is None: ...
    elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
    if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if other.device.type == 'cpu': other = other.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    other_shape=None if other is None else Size(other.shape)
    # Use the given inner codes if they are provided.
    ref_shape, input_shape, other_shape = size_mapping_op['mm'](input_shape, other_shape)
    input = input.view(input_shape)
    other = other.view(other_shape)
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input,other] if x is not None)
        obj = torch.mm(*auto_generated_args, out=out)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "mm"
    return obj

def bmm(input: 'Tensor', other: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.bmm'. The automatically generated codes are as follows:
        [ 1]: def bmm(input: 'Tensor', other: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input, other]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     if other is None: ...
        [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
        [23]:
        [24]:     # Obtain available sized in arguments (which will be fed into size function).
        [25]:     input_shape=None if input is None else Size(input.shape)
        [26]:     other_shape=None if other is None else Size(other.shape)
        [27]:     # Use the given inner codes if they are provided.
        [28]:     ref_shape, input_shape, other_shape = size_mapping_op['bmm'](input_shape, other_shape)
        [29]:     input = input.view(input_shape)
        [30]:     other = other.view(other_shape)
        [31]:     with torch._C.DisableTorchFunction():
        [32]:         auto_generated_args = tuple(x for x in [input,other] if x is not None)
        [33]:         obj = torch.bmm(*auto_generated_args, out=out)
        [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [35]:
        [36]:     def update_special(obj, updates):
        [37]:         if isinstance(obj, tuple):
        [38]:             for x in obj: update_special(x, updates)
        [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [41]:
        [42]:     if getattr(obj, 'grad_fn', None) is not None:
        [43]:         obj.grad_fn_name = "bmm"
        [44]:     return obj
        [45]:
        The document of the original function is:

        bmm(input, mat2, *, out=None) -> Tensor

        Performs a batch matrix-matrix product of matrices stored in :attr:`input`
        and :attr:`mat2`.

        :attr:`input` and :attr:`mat2` must be 3-D tensors each containing
        the same number of matrices.

        If :attr:`input` is a :math:`(b \times n \times m)` tensor, :attr:`mat2` is a
        :math:`(b \times m \times p)` tensor, :attr:`out` will be a
        :math:`(b \times n \times p)` tensor.

        .. math::
            \text{out}_i = \text{input}_i \mathbin{@} \text{mat2}_i

        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

        On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

        .. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.
                  For broadcasting matrix products, see :func:`torch.matmul`.

        Args:
            input (Tensor): the first batch of matrices to be multiplied
            mat2 (Tensor): the second batch of matrices to be multiplied

        Keyword Args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> input = torch.randn(10, 3, 4)
            >>> mat2 = torch.randn(10, 4, 5)
            >>> res = torch.bmm(input, mat2)
            >>> res.size()
            torch.Size([10, 3, 5])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input, other]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    if other is None: ...
    elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
    if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if other.device.type == 'cpu': other = other.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    other_shape=None if other is None else Size(other.shape)
    # Use the given inner codes if they are provided.
    ref_shape, input_shape, other_shape = size_mapping_op['bmm'](input_shape, other_shape)
    input = input.view(input_shape)
    other = other.view(other_shape)
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input,other] if x is not None)
        obj = torch.bmm(*auto_generated_args, out=out)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "bmm"
    return obj

def smm(input: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.smm'. The automatically generated codes are as follows:
        [ 1]: def smm(input: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input, other]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     if other is None: ...
        [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
        [23]:
        [24]:     # Obtain available sized in arguments (which will be fed into size function).
        [25]:     input_shape=None if input is None else Size(input.shape)
        [26]:     other_shape=None if other is None else Size(other.shape)
        [27]:     # Use the given inner codes if they are provided.
        [28]:     ref_shape, input_shape, other_shape = size_mapping_op['smm'](input_shape, other_shape)
        [29]:     input = input.view(input_shape)
        [30]:     other = other.view(other_shape)
        [31]:     with torch._C.DisableTorchFunction():
        [32]:         auto_generated_args = tuple(x for x in [input,other] if x is not None)
        [33]:         obj = torch.smm(*auto_generated_args, )
        [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [35]:
        [36]:     def update_special(obj, updates):
        [37]:         if isinstance(obj, tuple):
        [38]:             for x in obj: update_special(x, updates)
        [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [41]:
        [42]:     if getattr(obj, 'grad_fn', None) is not None:
        [43]:         obj.grad_fn_name = "smm"
        [44]:     return obj
        [45]:
        The document of the original function is:

        smm(input, mat) -> Tensor

        Performs a matrix multiplication of the sparse matrix :attr:`input`
        with the dense matrix :attr:`mat`.

        Args:
            input (Tensor): a sparse matrix to be matrix multiplied
            mat (Tensor): a dense matrix to be matrix multiplied

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input, other]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    if other is None: ...
    elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
    if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if other.device.type == 'cpu': other = other.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    other_shape=None if other is None else Size(other.shape)
    # Use the given inner codes if they are provided.
    ref_shape, input_shape, other_shape = size_mapping_op['smm'](input_shape, other_shape)
    input = input.view(input_shape)
    other = other.view(other_shape)
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input,other] if x is not None)
        obj = torch.smm(*auto_generated_args, )
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "smm"
    return obj

def floor_divide(input: 'Tensor', other: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.floor_divide'. The automatically generated codes are as follows:
        [ 1]: def floor_divide(input: 'Tensor', other: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input, other]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     if other is None: ...
        [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
        [23]:
        [24]:     # Obtain available sized in arguments (which will be fed into size function).
        [25]:     input_shape=None if input is None else Size(input.shape)
        [26]:     other_shape=None if other is None else Size(other.shape)
        [27]:     # Use the given inner codes if they are provided.
        [28]:     ref_shape, input_shape, other_shape = size_mapping_op['floor_divide'](input_shape, other_shape)
        [29]:     input = input.view(input_shape)
        [30]:     other = other.view(other_shape)
        [31]:     with torch._C.DisableTorchFunction():
        [32]:         auto_generated_args = tuple(x for x in [input,other] if x is not None)
        [33]:         obj = torch.floor_divide(*auto_generated_args, out=out)
        [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [35]:
        [36]:     def update_special(obj, updates):
        [37]:         if isinstance(obj, tuple):
        [38]:             for x in obj: update_special(x, updates)
        [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [41]:
        [42]:     if getattr(obj, 'grad_fn', None) is not None:
        [43]:         obj.grad_fn_name = "floor_divide"
        [44]:     return obj
        [45]:
        The document of the original function is:

        floor_divide(input, other, *, out=None) -> Tensor

        .. note::

            Before PyTorch 1.13 :func:`torch.floor_divide` incorrectly performed
            truncation division. To restore the previous behavior use
            :func:`torch.div` with ``rounding_mode='trunc'``.

        Computes :attr:`input` divided by :attr:`other`, elementwise, and floors
        the result.

        .. math::
            \text{{out}}_i = \text{floor} \left( \frac{{\text{{input}}_i}}{{\text{{other}}_i}} \right)

        Supports broadcasting to a common shape, type promotion, and integer and float inputs.

        Args:
            input (Tensor or Number): the dividend
            other (Tensor or Number): the divisor

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.tensor([4.0, 3.0])
            >>> b = torch.tensor([2.0, 2.0])
            >>> torch.floor_divide(a, b)
            tensor([2.0, 1.0])
            >>> torch.floor_divide(a, 1.4)
            tensor([2.0, 2.0])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input, other]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    if other is None: ...
    elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
    if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if other.device.type == 'cpu': other = other.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    other_shape=None if other is None else Size(other.shape)
    # Use the given inner codes if they are provided.
    ref_shape, input_shape, other_shape = size_mapping_op['floor_divide'](input_shape, other_shape)
    input = input.view(input_shape)
    other = other.view(other_shape)
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input,other] if x is not None)
        obj = torch.floor_divide(*auto_generated_args, out=out)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "floor_divide"
    return obj

def true_divide(dividend: 'Tensor', divisor: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.true_divide'. The automatically generated codes are as follows:
        [ 1]: def true_divide(dividend: 'Tensor', divisor: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [dividend, divisor]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if dividend is None: ...
        [13]:     elif not isinstance(dividend, torch.Tensor): dividend = torch.tensor(dividend)
        [14]:     if not isinstance(dividend, subclass): dividend = dividend.as_subclass(subclass).special_from(dividend.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if dividend.device.type == 'cpu': dividend = dividend.to(pivot.device)
        [17]:
        [18]:     if divisor is None: ...
        [19]:     elif not isinstance(divisor, torch.Tensor): divisor = torch.tensor(divisor)
        [20]:     if not isinstance(divisor, subclass): divisor = divisor.as_subclass(subclass).special_from(divisor.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if divisor.device.type == 'cpu': divisor = divisor.to(pivot.device)
        [23]:
        [24]:     # Obtain available sized in arguments (which will be fed into size function).
        [25]:     dividend_shape=None if dividend is None else Size(dividend.shape)
        [26]:     divisor_shape=None if divisor is None else Size(divisor.shape)
        [27]:     # Use the given inner codes if they are provided.
        [28]:     ref_shape, dividend_shape, divisor_shape = size_mapping_op['true_divide'](dividend_shape, divisor_shape)
        [29]:     dividend = dividend.view(dividend_shape)
        [30]:     divisor = divisor.view(divisor_shape)
        [31]:     with torch._C.DisableTorchFunction():
        [32]:         auto_generated_args = tuple(x for x in [dividend,divisor] if x is not None)
        [33]:         obj = torch.true_divide(*auto_generated_args, out=out)
        [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [35]:
        [36]:     def update_special(obj, updates):
        [37]:         if isinstance(obj, tuple):
        [38]:             for x in obj: update_special(x, updates)
        [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [41]:
        [42]:     if getattr(obj, 'grad_fn', None) is not None:
        [43]:         obj.grad_fn_name = "true_divide"
        [44]:     return obj
        [45]:
        The document of the original function is:

        true_divide(dividend, divisor, *, out) -> Tensor

        Alias for :func:`torch.div` with ``rounding_mode=None``.

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [dividend, divisor]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if dividend is None: ...
    elif not isinstance(dividend, torch.Tensor): dividend = torch.tensor(dividend)
    if not isinstance(dividend, subclass): dividend = dividend.as_subclass(subclass).special_from(dividend.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if dividend.device.type == 'cpu': dividend = dividend.to(pivot.device)

    if divisor is None: ...
    elif not isinstance(divisor, torch.Tensor): divisor = torch.tensor(divisor)
    if not isinstance(divisor, subclass): divisor = divisor.as_subclass(subclass).special_from(divisor.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if divisor.device.type == 'cpu': divisor = divisor.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    dividend_shape=None if dividend is None else Size(dividend.shape)
    divisor_shape=None if divisor is None else Size(divisor.shape)
    # Use the given inner codes if they are provided.
    ref_shape, dividend_shape, divisor_shape = size_mapping_op['true_divide'](dividend_shape, divisor_shape)
    dividend = dividend.view(dividend_shape)
    divisor = divisor.view(divisor_shape)
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [dividend,divisor] if x is not None)
        obj = torch.true_divide(*auto_generated_args, out=out)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "true_divide"
    return obj

def equal(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.equal'. The automatically generated codes are as follows:
        [ 1]: def equal(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [self, other]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if self is None: ...
        [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [17]:
        [18]:     if other is None: ...
        [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
        [23]:
        [24]:     # Obtain available sized in arguments (which will be fed into size function).
        [25]:     self_shape=None if self is None else Size(self.shape)
        [26]:     other_shape=None if other is None else Size(other.shape)
        [27]:     # Use the given inner codes if they are provided.
        [28]:     ref_shape, self_shape, other_shape = size_mapping_op['equal'](self_shape, other_shape)
        [29]:     self = self.view(self_shape)
        [30]:     other = other.view(other_shape)
        [31]:     with torch._C.DisableTorchFunction():
        [32]:         auto_generated_args = tuple(x for x in [self,other] if x is not None)
        [33]:         obj = torch.equal(*auto_generated_args, )
        [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [35]:
        [36]:     def update_special(obj, updates):
        [37]:         if isinstance(obj, tuple):
        [38]:             for x in obj: update_special(x, updates)
        [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [41]:
        [42]:     if getattr(obj, 'grad_fn', None) is not None:
        [43]:         obj.grad_fn_name = "equal"
        [44]:     return obj
        [45]:
        The document of the original function is:

        equal(input, other) -> bool

        ``True`` if two tensors have the same size and elements, ``False`` otherwise.

        Example::

            >>> torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2]))
            True

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self, other]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    if other is None: ...
    elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
    if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if other.device.type == 'cpu': other = other.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    other_shape=None if other is None else Size(other.shape)
    # Use the given inner codes if they are provided.
    ref_shape, self_shape, other_shape = size_mapping_op['equal'](self_shape, other_shape)
    self = self.view(self_shape)
    other = other.view(other_shape)
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [self,other] if x is not None)
        obj = torch.equal(*auto_generated_args, )
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "equal"
    return obj

def addmm(input: 'Tensor', mat1: 'Tensor', mat2: 'Tensor', *, beta=1, alpha=1, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.addmm'. The automatically generated codes are as follows:
        [ 1]: def addmm(input: 'Tensor', mat1: 'Tensor', mat2: 'Tensor', *, beta=1, alpha=1, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input, mat1, mat2]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     if mat1 is None: ...
        [19]:     elif not isinstance(mat1, torch.Tensor): mat1 = torch.tensor(mat1)
        [20]:     if not isinstance(mat1, subclass): mat1 = mat1.as_subclass(subclass).special_from(mat1.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if mat1.device.type == 'cpu': mat1 = mat1.to(pivot.device)
        [23]:
        [24]:     if mat2 is None: ...
        [25]:     elif not isinstance(mat2, torch.Tensor): mat2 = torch.tensor(mat2)
        [26]:     if not isinstance(mat2, subclass): mat2 = mat2.as_subclass(subclass).special_from(mat2.shape)
        [27]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [28]:         if mat2.device.type == 'cpu': mat2 = mat2.to(pivot.device)
        [29]:
        [30]:     # Obtain available sized in arguments (which will be fed into size function).
        [31]:     input_shape=None if input is None else Size(input.shape)
        [32]:     mat1_shape=None if mat1 is None else Size(mat1.shape)
        [33]:     mat2_shape=None if mat2 is None else Size(mat2.shape)
        [34]:     # Use the given inner codes if they are provided.
        [35]:     ref_shape, input_shape, mat1_shape, mat2_shape = size_mapping_op['addmm'](input_shape, mat1_shape, mat2_shape)
        [36]:     input = input.view(input_shape)
        [37]:     mat1 = mat1.view(mat1_shape)
        [38]:     mat2 = mat2.view(mat2_shape)
        [39]:     with torch._C.DisableTorchFunction():
        [40]:         auto_generated_args = tuple(x for x in [input,mat1,mat2] if x is not None)
        [41]:         obj = torch.addmm(*auto_generated_args, beta=beta,alpha=alpha,out=out)
        [42]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [43]:
        [44]:     def update_special(obj, updates):
        [45]:         if isinstance(obj, tuple):
        [46]:             for x in obj: update_special(x, updates)
        [47]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [48]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [49]:
        [50]:     if getattr(obj, 'grad_fn', None) is not None:
        [51]:         obj.grad_fn_name = "addmm"
        [52]:     return obj
        [53]:
        The document of the original function is:

        addmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) -> Tensor

        Performs a matrix multiplication of the matrices :attr:`mat1` and :attr:`mat2`.
        The matrix :attr:`input` is added to the final result.

        If :attr:`mat1` is a :math:`(n \times m)` tensor, :attr:`mat2` is a
        :math:`(m \times p)` tensor, then :attr:`input` must be
        :ref:`broadcastable <broadcasting-semantics>` with a :math:`(n \times p)` tensor
        and :attr:`out` will be a :math:`(n \times p)` tensor.

        :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between
        :attr:`mat1` and :attr:`mat2` and the added matrix :attr:`input` respectively.

        .. math::
            \text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i)

        If :attr:`beta` is 0, then :attr:`input` will be ignored, and `nan` and `inf` in
        it will not be propagated.

        For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
        :attr:`alpha` must be real numbers, otherwise they should be integers.

        This operation has support for arguments with :ref:`sparse layouts<sparse-docs>`. If
        :attr:`input` is sparse the result will have the same layout and if :attr:`out`
        is provided it must have the same layout as :attr:`input`.

        .. warning::
            Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported,
            or may not have autograd support. If you notice missing functionality please
            open a feature request.

        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

        On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

        Args:
            input (Tensor): matrix to be added
            mat1 (Tensor): the first matrix to be matrix multiplied
            mat2 (Tensor): the second matrix to be matrix multiplied

        Keyword args:
            beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
            alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`)
            out (Tensor, optional): the output tensor.

        Example::

            >>> M = torch.randn(2, 3)
            >>> mat1 = torch.randn(2, 3)
            >>> mat2 = torch.randn(3, 3)
            >>> torch.addmm(M, mat1, mat2)
            tensor([[-4.8716,  1.4671, -1.3746],
                    [ 0.7573, -3.9555, -2.8681]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input, mat1, mat2]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    if mat1 is None: ...
    elif not isinstance(mat1, torch.Tensor): mat1 = torch.tensor(mat1)
    if not isinstance(mat1, subclass): mat1 = mat1.as_subclass(subclass).special_from(mat1.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if mat1.device.type == 'cpu': mat1 = mat1.to(pivot.device)

    if mat2 is None: ...
    elif not isinstance(mat2, torch.Tensor): mat2 = torch.tensor(mat2)
    if not isinstance(mat2, subclass): mat2 = mat2.as_subclass(subclass).special_from(mat2.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if mat2.device.type == 'cpu': mat2 = mat2.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    mat1_shape=None if mat1 is None else Size(mat1.shape)
    mat2_shape=None if mat2 is None else Size(mat2.shape)
    # Use the given inner codes if they are provided.
    ref_shape, input_shape, mat1_shape, mat2_shape = size_mapping_op['addmm'](input_shape, mat1_shape, mat2_shape)
    input = input.view(input_shape)
    mat1 = mat1.view(mat1_shape)
    mat2 = mat2.view(mat2_shape)
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input,mat1,mat2] if x is not None)
        obj = torch.addmm(*auto_generated_args, beta=beta,alpha=alpha,out=out)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "addmm"
    return obj

def addbmm(input: 'Tensor', batch1: 'Tensor', batch2: 'Tensor', *, beta=1, alpha=1, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.addbmm'. The automatically generated codes are as follows:
        [ 1]: def addbmm(input: 'Tensor', batch1: 'Tensor', batch2: 'Tensor', *, beta=1, alpha=1, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input, batch1, batch2]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     if batch1 is None: ...
        [19]:     elif not isinstance(batch1, torch.Tensor): batch1 = torch.tensor(batch1)
        [20]:     if not isinstance(batch1, subclass): batch1 = batch1.as_subclass(subclass).special_from(batch1.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if batch1.device.type == 'cpu': batch1 = batch1.to(pivot.device)
        [23]:
        [24]:     if batch2 is None: ...
        [25]:     elif not isinstance(batch2, torch.Tensor): batch2 = torch.tensor(batch2)
        [26]:     if not isinstance(batch2, subclass): batch2 = batch2.as_subclass(subclass).special_from(batch2.shape)
        [27]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [28]:         if batch2.device.type == 'cpu': batch2 = batch2.to(pivot.device)
        [29]:
        [30]:     # Obtain available sized in arguments (which will be fed into size function).
        [31]:     input_shape=None if input is None else Size(input.shape)
        [32]:     batch1_shape=None if batch1 is None else Size(batch1.shape)
        [33]:     batch2_shape=None if batch2 is None else Size(batch2.shape)
        [34]:     # Use the given inner codes if they are provided.
        [35]:     torch_returned = False
        [36]:     ref_shape, input_shape, batch1_shape, batch2_shape = size_mapping_op['addbmm'](input_shape, batch1_shape, batch2_shape)
        [37]:     input = input.view(input_shape).squeeze({})
        [38]:     batch1 = batch1.view(batch1_shape)
        [39]:     batch2 = batch2.view(batch2_shape)
        [40]:     with torch._C.DisableTorchFunction():
        [41]:         obj = Tensor.inherit_from(torch.addbmm(input,batch1,batch2, beta=beta,alpha=alpha,out=out), input)
        [42]:
        [43]:     def update_special(obj, updates):
        [44]:         if isinstance(obj, tuple):
        [45]:             for x in obj: update_special(x, updates)
        [46]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [47]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [48]:
        [49]:     if getattr(obj, 'grad_fn', None) is not None:
        [50]:         obj.grad_fn_name = "addbmm"
        [51]:     return obj
        [52]:
        The document of the original function is:

        addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -> Tensor

        Performs a batch matrix-matrix product of matrices stored
        in :attr:`batch1` and :attr:`batch2`,
        with a reduced add step (all matrix multiplications get accumulated
        along the first dimension).
        :attr:`input` is added to the final result.

        :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the
        same number of matrices.

        If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a
        :math:`(b \times m \times p)` tensor, :attr:`input` must be
        :ref:`broadcastable <broadcasting-semantics>` with a :math:`(n \times p)` tensor
        and :attr:`out` will be a :math:`(n \times p)` tensor.

        .. math::
            out = \beta\ \text{input} + \alpha\ (\sum_{i=0}^{b-1} \text{batch1}_i \mathbin{@} \text{batch2}_i)

        If :attr:`beta` is 0, then :attr:`input` will be ignored, and `nan` and `inf` in
        it will not be propagated.

        For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and :attr:`alpha`
        must be real numbers, otherwise they should be integers.

        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

        On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

        Args:
            batch1 (Tensor): the first batch of matrices to be multiplied
            batch2 (Tensor): the second batch of matrices to be multiplied

        Keyword args:
            beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
            input (Tensor): matrix to be added
            alpha (Number, optional): multiplier for `batch1 @ batch2` (:math:`\alpha`)
            out (Tensor, optional): the output tensor.

        Example::

            >>> M = torch.randn(3, 5)
            >>> batch1 = torch.randn(10, 3, 4)
            >>> batch2 = torch.randn(10, 4, 5)
            >>> torch.addbmm(M, batch1, batch2)
            tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],
                    [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],
                    [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input, batch1, batch2]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    if batch1 is None: ...
    elif not isinstance(batch1, torch.Tensor): batch1 = torch.tensor(batch1)
    if not isinstance(batch1, subclass): batch1 = batch1.as_subclass(subclass).special_from(batch1.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if batch1.device.type == 'cpu': batch1 = batch1.to(pivot.device)

    if batch2 is None: ...
    elif not isinstance(batch2, torch.Tensor): batch2 = torch.tensor(batch2)
    if not isinstance(batch2, subclass): batch2 = batch2.as_subclass(subclass).special_from(batch2.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if batch2.device.type == 'cpu': batch2 = batch2.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    batch1_shape=None if batch1 is None else Size(batch1.shape)
    batch2_shape=None if batch2 is None else Size(batch2.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    ref_shape, input_shape, batch1_shape, batch2_shape = size_mapping_op['addbmm'](input_shape, batch1_shape, batch2_shape)
    input = input.view(input_shape).squeeze({})
    batch1 = batch1.view(batch1_shape)
    batch2 = batch2.view(batch2_shape)
    with torch._C.DisableTorchFunction():
        obj = Tensor.inherit_from(torch.addbmm(input,batch1,batch2, beta=beta,alpha=alpha,out=out), input)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "addbmm"
    return obj

def saddmm(input: 'Tensor', mat1: 'Tensor', mat2: 'Tensor', *, beta=1, alpha=1, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.saddmm'. The automatically generated codes are as follows:
        [ 1]: def saddmm(input: 'Tensor', mat1: 'Tensor', mat2: 'Tensor', *, beta=1, alpha=1, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input, mat1, mat2]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     if mat1 is None: ...
        [19]:     elif not isinstance(mat1, torch.Tensor): mat1 = torch.tensor(mat1)
        [20]:     if not isinstance(mat1, subclass): mat1 = mat1.as_subclass(subclass).special_from(mat1.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if mat1.device.type == 'cpu': mat1 = mat1.to(pivot.device)
        [23]:
        [24]:     if mat2 is None: ...
        [25]:     elif not isinstance(mat2, torch.Tensor): mat2 = torch.tensor(mat2)
        [26]:     if not isinstance(mat2, subclass): mat2 = mat2.as_subclass(subclass).special_from(mat2.shape)
        [27]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [28]:         if mat2.device.type == 'cpu': mat2 = mat2.to(pivot.device)
        [29]:
        [30]:     # Obtain available sized in arguments (which will be fed into size function).
        [31]:     input_shape=None if input is None else Size(input.shape)
        [32]:     mat1_shape=None if mat1 is None else Size(mat1.shape)
        [33]:     mat2_shape=None if mat2 is None else Size(mat2.shape)
        [34]:     # Use the given inner codes if they are provided.
        [35]:     ref_shape, input_shape, mat1_shape, mat2_shape = size_mapping_op['saddmm'](input_shape, mat1_shape, mat2_shape)
        [36]:     input = input.view(input_shape)
        [37]:     mat1 = mat1.view(mat1_shape)
        [38]:     mat2 = mat2.view(mat2_shape)
        [39]:     with torch._C.DisableTorchFunction():
        [40]:         auto_generated_args = tuple(x for x in [input,mat1,mat2] if x is not None)
        [41]:         obj = torch.saddmm(*auto_generated_args, beta=beta,alpha=alpha,out=out)
        [42]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [43]:
        [44]:     def update_special(obj, updates):
        [45]:         if isinstance(obj, tuple):
        [46]:             for x in obj: update_special(x, updates)
        [47]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [48]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [49]:
        [50]:     if getattr(obj, 'grad_fn', None) is not None:
        [51]:         obj.grad_fn_name = "saddmm"
        [52]:     return obj
        [53]:
        The document of the original function is:
        None

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input, mat1, mat2]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    if mat1 is None: ...
    elif not isinstance(mat1, torch.Tensor): mat1 = torch.tensor(mat1)
    if not isinstance(mat1, subclass): mat1 = mat1.as_subclass(subclass).special_from(mat1.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if mat1.device.type == 'cpu': mat1 = mat1.to(pivot.device)

    if mat2 is None: ...
    elif not isinstance(mat2, torch.Tensor): mat2 = torch.tensor(mat2)
    if not isinstance(mat2, subclass): mat2 = mat2.as_subclass(subclass).special_from(mat2.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if mat2.device.type == 'cpu': mat2 = mat2.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    mat1_shape=None if mat1 is None else Size(mat1.shape)
    mat2_shape=None if mat2 is None else Size(mat2.shape)
    # Use the given inner codes if they are provided.
    ref_shape, input_shape, mat1_shape, mat2_shape = size_mapping_op['saddmm'](input_shape, mat1_shape, mat2_shape)
    input = input.view(input_shape)
    mat1 = mat1.view(mat1_shape)
    mat2 = mat2.view(mat2_shape)
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input,mat1,mat2] if x is not None)
        obj = torch.saddmm(*auto_generated_args, beta=beta,alpha=alpha,out=out)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "saddmm"
    return obj

def addcmul(input: 'Tensor', tensor1: 'Tensor', tensor2: 'Tensor', *, value=1, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.addcmul'. The automatically generated codes are as follows:
        [ 1]: def addcmul(input: 'Tensor', tensor1: 'Tensor', tensor2: 'Tensor', *, value=1, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input, tensor1, tensor2]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     if tensor1 is None: ...
        [19]:     elif not isinstance(tensor1, torch.Tensor): tensor1 = torch.tensor(tensor1)
        [20]:     if not isinstance(tensor1, subclass): tensor1 = tensor1.as_subclass(subclass).special_from(tensor1.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if tensor1.device.type == 'cpu': tensor1 = tensor1.to(pivot.device)
        [23]:
        [24]:     if tensor2 is None: ...
        [25]:     elif not isinstance(tensor2, torch.Tensor): tensor2 = torch.tensor(tensor2)
        [26]:     if not isinstance(tensor2, subclass): tensor2 = tensor2.as_subclass(subclass).special_from(tensor2.shape)
        [27]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [28]:         if tensor2.device.type == 'cpu': tensor2 = tensor2.to(pivot.device)
        [29]:
        [30]:     # Obtain available sized in arguments (which will be fed into size function).
        [31]:     input_shape=None if input is None else Size(input.shape)
        [32]:     tensor1_shape=None if tensor1 is None else Size(tensor1.shape)
        [33]:     tensor2_shape=None if tensor2 is None else Size(tensor2.shape)
        [34]:     # Use the given inner codes if they are provided.
        [35]:     ref_shape, input_shape, tensor1_shape, tensor2_shape = size_mapping_op['addcmul'](input_shape, tensor1_shape, tensor2_shape)
        [36]:     input = input.view(input_shape)
        [37]:     tensor1 = tensor1.view(tensor1_shape)
        [38]:     tensor2 = tensor2.view(tensor2_shape)
        [39]:     with torch._C.DisableTorchFunction():
        [40]:         auto_generated_args = tuple(x for x in [input,tensor1,tensor2] if x is not None)
        [41]:         obj = torch.addcmul(*auto_generated_args, value=value,out=out)
        [42]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [43]:
        [44]:     def update_special(obj, updates):
        [45]:         if isinstance(obj, tuple):
        [46]:             for x in obj: update_special(x, updates)
        [47]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [48]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [49]:
        [50]:     if getattr(obj, 'grad_fn', None) is not None:
        [51]:         obj.grad_fn_name = "addcmul"
        [52]:     return obj
        [53]:
        The document of the original function is:

        addcmul(input, tensor1, tensor2, *, value=1, out=None) -> Tensor

        Performs the element-wise multiplication of :attr:`tensor1`
        by :attr:`tensor2`, multiplies the result by the scalar :attr:`value`
        and adds it to :attr:`input`.

        .. math::
            \text{out}_i = \text{input}_i + \text{value} \times \text{tensor1}_i \times \text{tensor2}_i

        The shapes of :attr:`tensor`, :attr:`tensor1`, and :attr:`tensor2` must be
        :ref:`broadcastable <broadcasting-semantics>`.

        For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be
        a real number, otherwise an integer.

        Args:
            input (Tensor): the tensor to be added
            tensor1 (Tensor): the tensor to be multiplied
            tensor2 (Tensor): the tensor to be multiplied

        Keyword args:
            value (Number, optional): multiplier for :math:`tensor1 .* tensor2`
            out (Tensor, optional): the output tensor.

        Example::

            >>> t = torch.randn(1, 3)
            >>> t1 = torch.randn(3, 1)
            >>> t2 = torch.randn(1, 3)
            >>> torch.addcmul(t, t1, t2, value=0.1)
            tensor([[-0.8635, -0.6391,  1.6174],
                    [-0.7617, -0.5879,  1.7388],
                    [-0.8353, -0.6249,  1.6511]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input, tensor1, tensor2]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    if tensor1 is None: ...
    elif not isinstance(tensor1, torch.Tensor): tensor1 = torch.tensor(tensor1)
    if not isinstance(tensor1, subclass): tensor1 = tensor1.as_subclass(subclass).special_from(tensor1.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if tensor1.device.type == 'cpu': tensor1 = tensor1.to(pivot.device)

    if tensor2 is None: ...
    elif not isinstance(tensor2, torch.Tensor): tensor2 = torch.tensor(tensor2)
    if not isinstance(tensor2, subclass): tensor2 = tensor2.as_subclass(subclass).special_from(tensor2.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if tensor2.device.type == 'cpu': tensor2 = tensor2.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    tensor1_shape=None if tensor1 is None else Size(tensor1.shape)
    tensor2_shape=None if tensor2 is None else Size(tensor2.shape)
    # Use the given inner codes if they are provided.
    ref_shape, input_shape, tensor1_shape, tensor2_shape = size_mapping_op['addcmul'](input_shape, tensor1_shape, tensor2_shape)
    input = input.view(input_shape)
    tensor1 = tensor1.view(tensor1_shape)
    tensor2 = tensor2.view(tensor2_shape)
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input,tensor1,tensor2] if x is not None)
        obj = torch.addcmul(*auto_generated_args, value=value,out=out)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "addcmul"
    return obj

# value operations
def clamp(input: 'Tensor', min=None, max=None, *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.clamp'. The automatically generated codes are as follows:
        [ 1]: def clamp(input: 'Tensor', min=None, max=None, *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.clamp(*auto_generated_args, min=min,max=max,out=out)
        [24]:     ref_shape = size_mapping['clamp'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "clamp"
        [35]:     return obj
        [36]:
        The document of the original function is:

        clamp(input, min=None, max=None, *, out=None) -> Tensor

        Clamps all elements in :attr:`input` into the range `[` :attr:`min`, :attr:`max` `]`.
        Letting min_value and max_value be :attr:`min` and :attr:`max`, respectively, this returns:

        .. math::
            y_i = \min(\max(x_i, \text{min\_value}_i), \text{max\_value}_i)

        If :attr:`min` is ``None``, there is no lower bound.
        Or, if :attr:`max` is ``None`` there is no upper bound.

        .. note::
            If :attr:`min` is greater than :attr:`max` :func:`torch.clamp(..., min, max) <torch.clamp>`
            sets all elements in :attr:`input` to the value of :attr:`max`.

        Args:
            input (Tensor): the input tensor.
            min (Number or Tensor, optional): lower-bound of the range to be clamped to
            max (Number or Tensor, optional): upper-bound of the range to be clamped to

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.randn(4)
            >>> a
            tensor([-1.7120,  0.1734, -0.0478, -0.0922])
            >>> torch.clamp(a, min=-0.5, max=0.5)
            tensor([-0.5000,  0.1734, -0.0478, -0.0922])

            >>> min = torch.linspace(-1, 1, steps=4)
            >>> torch.clamp(a, min=min)
            tensor([-1.0000,  0.1734,  0.3333,  1.0000])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.clamp(*auto_generated_args, min=min,max=max,out=out)
    ref_shape = size_mapping['clamp'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "clamp"
    return obj

def floor(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.floor'. The automatically generated codes are as follows:
        [ 1]: def floor(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.floor(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['floor'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "floor"
        [35]:     return obj
        [36]:
        The document of the original function is:

        floor(input, *, out=None) -> Tensor

        Returns a new tensor with the floor of the elements of :attr:`input`,
        the largest integer less than or equal to each element.

        For integer inputs, follows the array-api convention of returning a
        copy of the input tensor.

        .. math::
            \text{out}_{i} = \left\lfloor \text{input}_{i} \right\rfloor

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.randn(4)
            >>> a
            tensor([-0.8166,  1.5308, -0.2530, -0.2091])
            >>> torch.floor(a)
            tensor([-1.,  1., -1., -1.])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.floor(*auto_generated_args, out=out)
    ref_shape = size_mapping['floor'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "floor"
    return obj

def ceil(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.ceil'. The automatically generated codes are as follows:
        [ 1]: def ceil(input: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.ceil(*auto_generated_args, out=out)
        [24]:     ref_shape = size_mapping['ceil'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "ceil"
        [35]:     return obj
        [36]:
        The document of the original function is:

        ceil(input, *, out=None) -> Tensor

        Returns a new tensor with the ceil of the elements of :attr:`input`,
        the smallest integer greater than or equal to each element.

        For integer inputs, follows the array-api convention of returning a
        copy of the input tensor.

        .. math::
            \text{out}_{i} = \left\lceil \text{input}_{i} \right\rceil

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.randn(4)
            >>> a
            tensor([-0.6341, -1.4208, -1.0900,  0.5826])
            >>> torch.ceil(a)
            tensor([-0., -1., -1.,  1.])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.ceil(*auto_generated_args, out=out)
    ref_shape = size_mapping['ceil'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "ceil"
    return obj

def round(input: 'Tensor', *, decimals=0, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.round'. The automatically generated codes are as follows:
        [ 1]: def round(input: 'Tensor', *, decimals=0, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.round(*auto_generated_args, decimals=decimals,out=out)
        [24]:     ref_shape = size_mapping['round'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "round"
        [35]:     return obj
        [36]:
        The document of the original function is:

        round(input, *, decimals=0, out=None) -> Tensor

        Rounds elements of :attr:`input` to the nearest integer.

        For integer inputs, follows the array-api convention of returning a
        copy of the input tensor.

        .. note::
            This function implements the "round half to even" to
            break ties when a number is equidistant from two
            integers (e.g. `round(2.5)` is 2).

            When the :attr:\`decimals\` argument is specified the
            algorithm used is similar to NumPy's `around`. This
            algorithm is fast but inexact and it can easily
            overflow for low precision dtypes.
            Eg. `round(tensor([10000], dtype=torch.float16), decimals=3)` is `inf`.

        .. seealso::
            :func:`torch.ceil`, which rounds up.
            :func:`torch.floor`, which rounds down.
            :func:`torch.trunc`, which rounds towards zero.

        Args:
            input (Tensor): the input tensor.
            decimals (int): Number of decimal places to round to (default: 0).
                If decimals is negative, it specifies the number of positions
                to the left of the decimal point.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> torch.round(torch.tensor((4.7, -2.3, 9.1, -7.7)))
            tensor([ 5.,  -2.,  9., -8.])

            >>> # Values equidistant from two integers are rounded towards the
            >>> #   the nearest even value (zero is treated as even)
            >>> torch.round(torch.tensor([-0.5, 0.5, 1.5, 2.5]))
            tensor([-0., 0., 2., 2.])

            >>> # A positive decimals argument rounds to the to that decimal place
            >>> torch.round(torch.tensor([0.1234567]), decimals=3)
            tensor([0.1230])

            >>> # A negative decimals argument rounds to the left of the decimal
            >>> torch.round(torch.tensor([1200.1234567]), decimals=-3)
            tensor([1000.])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.round(*auto_generated_args, decimals=decimals,out=out)
    ref_shape = size_mapping['round'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "round"
    return obj

def any(input: 'Tensor', *dims: del_dim[...], keepdim=False, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.any'. The automatically generated codes are as follows:
        [ 1]: def any(input: 'Tensor', *dims: del_dim[...], keepdim=False, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     dims = del_dim[...](input, *dims)
        [19]:
        [20]:     # Obtain available sized in arguments (which will be fed into size function).
        [21]:     input_shape=None if input is None else Size(input.shape)
        [22]:     # Use the given inner codes if they are provided.
        [23]:     with torch._C.DisableTorchFunction():
        [24]:         for dims_iter, in zip(dims[::-1]):
        [25]:             auto_generated_args = tuple(x for x in [input] if x is not None)
        [26]:             input = torch.any(*auto_generated_args, dims_iter,keepdim=keepdim,out=out)
        [27]:     obj = input
        [28]:     ref_shape = size_mapping['any'](input_shape, dims, **dict(keepdim=keepdim, out=out))
        [29]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [30]:
        [31]:     def update_special(obj, updates):
        [32]:         if isinstance(obj, tuple):
        [33]:             for x in obj: update_special(x, updates)
        [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [36]:
        [37]:     if getattr(obj, 'grad_fn', None) is not None:
        [38]:         obj.grad_fn_name = "any"
        [39]:     return obj
        [40]:
        The document of the original function is:

        any(input) -> Tensor

        Tests if any element in :attr:`input` evaluates to `True`.

        .. note:: This function matches the behaviour of NumPy in returning
                  output of dtype `bool` for all supported dtypes except `uint8`.
                  For `uint8` the dtype of output is `uint8` itself.

        Example::

            >>> a = torch.rand(1, 2).bool()
            >>> a
            tensor([[False, True]], dtype=torch.bool)
            >>> torch.any(a)
            tensor(True, dtype=torch.bool)
            >>> a = torch.arange(0, 3)
            >>> a
            tensor([0, 1, 2])
            >>> torch.any(a)
            tensor(True)

        .. function:: any(input, dim, keepdim=False, *, out=None) -> Tensor
           :noindex:

        For each row of :attr:`input` in the given dimension :attr:`dim`,
        returns `True` if any element in the row evaluate to `True` and `False` otherwise.

        If :attr:`keepdim` is ``True``, the output tensor is of the same size
        as :attr:`input` except in the dimension :attr:`dim` where it is of size 1.
        Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in
        the output tensor having 1 fewer dimension than :attr:`input`.

        Args:
            input (Tensor): the input tensor.
            dim (int): the dimension to reduce.
            keepdim (bool): whether the output tensor has :attr:`dim` retained or not.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.randn(4, 2) < 0
            >>> a
            tensor([[ True,  True],
                    [False,  True],
                    [ True,  True],
                    [False, False]])
            >>> torch.any(a, 1)
            tensor([ True,  True,  True, False])
            >>> torch.any(a, 0)
            tensor([True, True])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dims = del_dim[...](input, *dims)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        for dims_iter, in zip(dims[::-1]):
            auto_generated_args = tuple(x for x in [input] if x is not None)
            input = torch.any(*auto_generated_args, dims_iter,keepdim=keepdim,out=out)
    obj = input
    ref_shape = size_mapping['any'](input_shape, dims, **dict(keepdim=keepdim, out=out))
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "any"
    return obj

def all(input: 'Tensor', *dims: del_dim[...], keepdim=False, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.all'. The automatically generated codes are as follows:
        [ 1]: def all(input: 'Tensor', *dims: del_dim[...], keepdim=False, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     dims = del_dim[...](input, *dims)
        [19]:
        [20]:     # Obtain available sized in arguments (which will be fed into size function).
        [21]:     input_shape=None if input is None else Size(input.shape)
        [22]:     # Use the given inner codes if they are provided.
        [23]:     with torch._C.DisableTorchFunction():
        [24]:         for dims_iter, in zip(dims[::-1]):
        [25]:             auto_generated_args = tuple(x for x in [input] if x is not None)
        [26]:             input = torch.all(*auto_generated_args, dims_iter,keepdim=keepdim,out=out)
        [27]:     obj = input
        [28]:     ref_shape = size_mapping['all'](input_shape, dims, **dict(keepdim=keepdim, out=out))
        [29]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [30]:
        [31]:     def update_special(obj, updates):
        [32]:         if isinstance(obj, tuple):
        [33]:             for x in obj: update_special(x, updates)
        [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [36]:
        [37]:     if getattr(obj, 'grad_fn', None) is not None:
        [38]:         obj.grad_fn_name = "all"
        [39]:     return obj
        [40]:
        The document of the original function is:

        all(input) -> Tensor

        Tests if all elements in :attr:`input` evaluate to `True`.

        .. note:: This function matches the behaviour of NumPy in returning
                  output of dtype `bool` for all supported dtypes except `uint8`.
                  For `uint8` the dtype of output is `uint8` itself.

        Example::

            >>> a = torch.rand(1, 2).bool()
            >>> a
            tensor([[False, True]], dtype=torch.bool)
            >>> torch.all(a)
            tensor(False, dtype=torch.bool)
            >>> a = torch.arange(0, 3)
            >>> a
            tensor([0, 1, 2])
            >>> torch.all(a)
            tensor(False)

        .. function:: all(input, dim, keepdim=False, *, out=None) -> Tensor
           :noindex:

        For each row of :attr:`input` in the given dimension :attr:`dim`,
        returns `True` if all elements in the row evaluate to `True` and `False` otherwise.

        If :attr:`keepdim` is ``True``, the output tensor is of the same size
        as :attr:`input` except in the dimension :attr:`dim` where it is of size 1.
        Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in
        the output tensor having 1 fewer dimension than :attr:`input`.

        Args:
            input (Tensor): the input tensor.
            dim (int): the dimension to reduce.
            keepdim (bool): whether the output tensor has :attr:`dim` retained or not.

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.rand(4, 2).bool()
            >>> a
            tensor([[True, True],
                    [True, False],
                    [True, True],
                    [True, True]], dtype=torch.bool)
            >>> torch.all(a, dim=1)
            tensor([ True, False,  True,  True], dtype=torch.bool)
            >>> torch.all(a, dim=0)
            tensor([ True, False], dtype=torch.bool)

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dims = del_dim[...](input, *dims)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        for dims_iter, in zip(dims[::-1]):
            auto_generated_args = tuple(x for x in [input] if x is not None)
            input = torch.all(*auto_generated_args, dims_iter,keepdim=keepdim,out=out)
    obj = input
    ref_shape = size_mapping['all'](input_shape, dims, **dict(keepdim=keepdim, out=out))
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "all"
    return obj

def unique(input: 'Tensor', sorted=True, return_inverse=False, return_counts=False, dim=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.unique'. The automatically generated codes are as follows:
        [ 1]: def unique(input: 'Tensor', sorted=True, return_inverse=False, return_counts=False, dim=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     torch_returned = False
        [22]:     with torch._C.DisableTorchFunction():
        [23]:         ret = torch.unique(input, sorted=sorted,return_inverse=return_inverse,return_counts=return_counts,dim=dim)
        [24]:
        [25]:     if isinstance(ret, tuple):
        [26]:         if len(ret) >= 1 and ret[0].ndim == 1:  ret[0] = Tensor.inherit_from(ret[0], input, shape=...)
        [27]:         elif len(ret) >= 1:  ret[0] = Tensor.inherit_from(ret[0], input, shape=input_shape)
        [28]:         if len(ret) >= 2 and return_inverse:  ret[1] = Tensor.inherit_from(ret[1], input, shape=input_shape)
        [29]:         elif len(ret) >= 2:  ret[1] = Tensor.inherit_from(ret[1], input, shape=...)
        [30]:         if len(ret) >= 3:  ret[2] = Tensor.inherit_from(ret[2], input, shape=...)
        [31]:         obj = ret
        [32]:     elif ret.ndim == 1:
        [33]:         obj = Tensor.inherit_from(ret, input, shape=[])
        [34]:     else: torch_returned = True; obj = Tensor.inherit_from(ret, input, shape=input_shape)
        [35]:
        [36]:     def update_special(obj, updates):
        [37]:         if isinstance(obj, tuple):
        [38]:             for x in obj: update_special(x, updates)
        [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [41]:
        [42]:     if getattr(obj, 'grad_fn', None) is not None:
        [43]:         obj.grad_fn_name = "unique"
        [44]:     return obj
        [45]:
        The document of the original function is:
        unique(input, sorted=True, return_inverse=False, return_counts=False, dim=None) -> Tuple[Tensor, Tensor, Tensor]

            Returns the unique elements of the input tensor.

            .. note:: This function is different from :func:`torch.unique_consecutive` in the sense that
                this function also eliminates non-consecutive duplicate values.

            .. note:: Currently in the CUDA implementation and the CPU implementation when dim is specified,
                `torch.unique` always sort the tensor at the beginning regardless of the `sort` argument.
                Sorting could be slow, so if your input tensor is already sorted, it is recommended to use
                :func:`torch.unique_consecutive` which avoids the sorting.

            Args:
                input (Tensor): the input tensor
                sorted (bool): Whether to sort the unique elements in ascending order
                    before returning as output.
                return_inverse (bool): Whether to also return the indices for where
                    elements in the original input ended up in the returned unique list.
                return_counts (bool): Whether to also return the counts for each unique
                    element.
                dim (int): the dimension to apply unique. If ``None``, the unique of the
                    flattened input is returned. default: ``None``

            Returns:
                (Tensor, Tensor (optional), Tensor (optional)): A tensor or a tuple of tensors containing

                    - **output** (*Tensor*): the output list of unique scalar elements.
                    - **inverse_indices** (*Tensor*): (optional) if
                      :attr:`return_inverse` is True, there will be an additional
                      returned tensor (same shape as input) representing the indices
                      for where elements in the original input map to in the output;
                      otherwise, this function will only return a single tensor.
                    - **counts** (*Tensor*): (optional) if
                      :attr:`return_counts` is True, there will be an additional
                      returned tensor (same shape as output or output.size(dim),
                      if dim was specified) representing the number of occurrences
                      for each unique value or tensor.

            Example::

                >>> output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))
                >>> output
                tensor([1, 2, 3])

                >>> output, inverse_indices = torch.unique(
                ...     torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)
                >>> output
                tensor([1, 2, 3])
                >>> inverse_indices
                tensor([0, 2, 1, 2])

                >>> output, inverse_indices = torch.unique(
                ...     torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)
                >>> output
                tensor([1, 2, 3])
                >>> inverse_indices
                tensor([[0, 2],
                        [1, 2]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    with torch._C.DisableTorchFunction():
        ret = torch.unique(input, sorted=sorted,return_inverse=return_inverse,return_counts=return_counts,dim=dim)

    if isinstance(ret, tuple):
        if len(ret) >= 1 and ret[0].ndim == 1:  ret[0] = Tensor.inherit_from(ret[0], input, shape=...)
        elif len(ret) >= 1:  ret[0] = Tensor.inherit_from(ret[0], input, shape=input_shape)
        if len(ret) >= 2 and return_inverse:  ret[1] = Tensor.inherit_from(ret[1], input, shape=input_shape)
        elif len(ret) >= 2:  ret[1] = Tensor.inherit_from(ret[1], input, shape=...)
        if len(ret) >= 3:  ret[2] = Tensor.inherit_from(ret[2], input, shape=...)
        obj = ret
    elif ret.ndim == 1:
        obj = Tensor.inherit_from(ret, input, shape=[])
    else: torch_returned = True; obj = Tensor.inherit_from(ret, input, shape=input_shape)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "unique"
    return obj

def isin(elements: 'Tensor', test_elements: 'Tensor', *, assume_unique=False, invert=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.isin'. The automatically generated codes are as follows:
        [ 1]: def isin(elements: 'Tensor', test_elements: 'Tensor', *, assume_unique=False, invert=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [elements, test_elements]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if elements is None: ...
        [13]:     elif not isinstance(elements, torch.Tensor): elements = torch.tensor(elements)
        [14]:     if not isinstance(elements, subclass): elements = elements.as_subclass(subclass).special_from(elements.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if elements.device.type == 'cpu': elements = elements.to(pivot.device)
        [17]:
        [18]:     if test_elements is None: ...
        [19]:     elif not isinstance(test_elements, torch.Tensor): test_elements = torch.tensor(test_elements)
        [20]:     if not isinstance(test_elements, subclass): test_elements = test_elements.as_subclass(subclass).special_from(test_elements.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if test_elements.device.type == 'cpu': test_elements = test_elements.to(pivot.device)
        [23]:
        [24]:     # Obtain available sized in arguments (which will be fed into size function).
        [25]:     elements_shape=None if elements is None else Size(elements.shape)
        [26]:     test_elements_shape=None if test_elements is None else Size(test_elements.shape)
        [27]:     # Use the given inner codes if they are provided.
        [28]:     ref_shape, elements_shape, test_elements_shape = size_mapping_op['isin'](elements_shape, test_elements_shape)
        [29]:     elements = elements.view(elements_shape)
        [30]:     test_elements = test_elements.view(test_elements_shape)
        [31]:     with torch._C.DisableTorchFunction():
        [32]:         auto_generated_args = tuple(x for x in [elements,test_elements] if x is not None)
        [33]:         obj = torch.isin(*auto_generated_args, assume_unique=assume_unique,invert=invert)
        [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [35]:
        [36]:     def update_special(obj, updates):
        [37]:         if isinstance(obj, tuple):
        [38]:             for x in obj: update_special(x, updates)
        [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [41]:
        [42]:     if getattr(obj, 'grad_fn', None) is not None:
        [43]:         obj.grad_fn_name = "isin"
        [44]:     return obj
        [45]:
        The document of the original function is:

        isin(elements, test_elements, *, assume_unique=False, invert=False) -> Tensor

        Tests if each element of :attr:`elements` is in :attr:`test_elements`. Returns
        a boolean tensor of the same shape as :attr:`elements` that is True for elements
        in :attr:`test_elements` and False otherwise.

        .. note::
            One of :attr:`elements` or :attr:`test_elements` can be a scalar, but not both.

        Args:
            elements (Tensor or Scalar): Input elements
            test_elements (Tensor or Scalar): Values against which to test for each input element
            assume_unique (bool, optional): If True, assumes both :attr:`elements` and
                :attr:`test_elements` contain unique elements, which can speed up the
                calculation. Default: False
            invert (bool, optional): If True, inverts the boolean return tensor, resulting in True
                values for elements *not* in :attr:`test_elements`. Default: False

        Returns:
            A boolean tensor of the same shape as :attr:`elements` that is True for elements in
            :attr:`test_elements` and False otherwise

        Example:
            >>> torch.isin(torch.tensor([[1, 2], [3, 4]]), torch.tensor([2, 3]))
            tensor([[False,  True],
                    [ True, False]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [elements, test_elements]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if elements is None: ...
    elif not isinstance(elements, torch.Tensor): elements = torch.tensor(elements)
    if not isinstance(elements, subclass): elements = elements.as_subclass(subclass).special_from(elements.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if elements.device.type == 'cpu': elements = elements.to(pivot.device)

    if test_elements is None: ...
    elif not isinstance(test_elements, torch.Tensor): test_elements = torch.tensor(test_elements)
    if not isinstance(test_elements, subclass): test_elements = test_elements.as_subclass(subclass).special_from(test_elements.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if test_elements.device.type == 'cpu': test_elements = test_elements.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    elements_shape=None if elements is None else Size(elements.shape)
    test_elements_shape=None if test_elements is None else Size(test_elements.shape)
    # Use the given inner codes if they are provided.
    ref_shape, elements_shape, test_elements_shape = size_mapping_op['isin'](elements_shape, test_elements_shape)
    elements = elements.view(elements_shape)
    test_elements = test_elements.view(test_elements_shape)
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [elements,test_elements] if x is not None)
        obj = torch.isin(*auto_generated_args, assume_unique=assume_unique,invert=invert)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "isin"
    return obj

def isnan(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.isnan'. The automatically generated codes are as follows:
        [ 1]: def isnan(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.isnan(*auto_generated_args, )
        [24]:     ref_shape = size_mapping['isnan'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "isnan"
        [35]:     return obj
        [36]:
        The document of the original function is:

        isnan(input) -> Tensor

        Returns a new tensor with boolean elements representing if each element of :attr:`input`
        is NaN or not. Complex values are considered NaN when either their real
        and/or imaginary part is NaN.

        Arguments:
            input (Tensor): the input tensor.

        Returns:
            A boolean tensor that is True where :attr:`input` is NaN and False elsewhere

        Example::

            >>> torch.isnan(torch.tensor([1, float('nan'), 2]))
            tensor([False, True, False])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.isnan(*auto_generated_args, )
    ref_shape = size_mapping['isnan'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "isnan"
    return obj

def isinf(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.isinf'. The automatically generated codes are as follows:
        [ 1]: def isinf(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.isinf(*auto_generated_args, )
        [24]:     ref_shape = size_mapping['isinf'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "isinf"
        [35]:     return obj
        [36]:
        The document of the original function is:

        isinf(input) -> Tensor

        Tests if each element of :attr:`input` is infinite
        (positive or negative infinity) or not.

        .. note::
            Complex values are infinite when their real or imaginary part is
            infinite.

        Args:
            input (Tensor): the input tensor.

        Returns:
            A boolean tensor that is True where :attr:`input` is infinite and False elsewhere

        Example::

            >>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))
            tensor([False,  True,  False,  True,  False])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.isinf(*auto_generated_args, )
    ref_shape = size_mapping['isinf'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "isinf"
    return obj

def isposinf(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.isposinf'. The automatically generated codes are as follows:
        [ 1]: def isposinf(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.isposinf(*auto_generated_args, )
        [24]:     ref_shape = size_mapping['isposinf'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "isposinf"
        [35]:     return obj
        [36]:
        The document of the original function is:

        isposinf(input, *, out=None) -> Tensor
        Tests if each element of :attr:`input` is positive infinity or not.

        Args:
          input (Tensor): the input tensor.

        Keyword args:
          out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.tensor([-float('inf'), float('inf'), 1.2])
            >>> torch.isposinf(a)
            tensor([False,  True, False])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.isposinf(*auto_generated_args, )
    ref_shape = size_mapping['isposinf'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "isposinf"
    return obj

def isneginf(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.isneginf'. The automatically generated codes are as follows:
        [ 1]: def isneginf(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.isneginf(*auto_generated_args, )
        [24]:     ref_shape = size_mapping['isneginf'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "isneginf"
        [35]:     return obj
        [36]:
        The document of the original function is:

        isneginf(input, *, out=None) -> Tensor
        Tests if each element of :attr:`input` is negative infinity or not.

        Args:
          input (Tensor): the input tensor.

        Keyword args:
          out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.tensor([-float('inf'), float('inf'), 1.2])
            >>> torch.isneginf(a)
            tensor([ True, False, False])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.isneginf(*auto_generated_args, )
    ref_shape = size_mapping['isneginf'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "isneginf"
    return obj

def isfinite(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.isfinite'. The automatically generated codes are as follows:
        [ 1]: def isfinite(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [23]:         obj = torch.isfinite(*auto_generated_args, )
        [24]:     ref_shape = size_mapping['isfinite'](input_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "isfinite"
        [35]:     return obj
        [36]:
        The document of the original function is:

        isfinite(input) -> Tensor

        Returns a new tensor with boolean elements representing if each element is `finite` or not.

        Real values are finite when they are not NaN, negative infinity, or infinity.
        Complex values are finite when both their real and imaginary parts are finite.

        Args:
            input (Tensor): the input tensor.

        Returns:
            A boolean tensor that is True where :attr:`input` is finite and False elsewhere

        Example::

            >>> torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))
            tensor([True,  False,  True,  False,  False])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.isfinite(*auto_generated_args, )
    ref_shape = size_mapping['isfinite'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "isfinite"
    return obj

# dimension manipulations
def unsqueeze(self: 'Tensor', *dims: new_dim[...], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.unsqueeze'. The automatically generated codes are as follows:
        [ 1]: def unsqueeze(self: 'Tensor', *dims: new_dim[...], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [self]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if self is None: ...
        [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [17]:
        [18]:     dims = new_dim[...](self, *dims)
        [19]:
        [20]:     # Obtain available sized in arguments (which will be fed into size function).
        [21]:     self_shape=None if self is None else Size(self.shape)
        [22]:     # Use the given inner codes if they are provided.
        [23]:     with torch._C.DisableTorchFunction():
        [24]:         for dims_iter, in zip(dims):
        [25]:             auto_generated_args = tuple(x for x in [self] if x is not None)
        [26]:             self = torch.unsqueeze(*auto_generated_args, dims_iter)
        [27]:     obj = self
        [28]:     ref_shape = size_mapping['unsqueeze'](self_shape, dims)
        [29]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [30]:
        [31]:     def update_special(obj, updates):
        [32]:         if isinstance(obj, tuple):
        [33]:             for x in obj: update_special(x, updates)
        [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [36]:
        [37]:     if getattr(obj, 'grad_fn', None) is not None:
        [38]:         obj.grad_fn_name = "unsqueeze"
        [39]:     return obj
        [40]:
        The document of the original function is:

        unsqueeze(input, dim) -> Tensor

        Returns a new tensor with a dimension of size one inserted at the
        specified position.

        The returned tensor shares the same underlying data with this tensor.

        A :attr:`dim` value within the range ``[-input.dim() - 1, input.dim() + 1)``
        can be used. Negative :attr:`dim` will correspond to :meth:`unsqueeze`
        applied at :attr:`dim` = ``dim + input.dim() + 1``.

        Args:
            input (Tensor): the input tensor.
            dim (int): the index at which to insert the singleton dimension

        Example::

            >>> x = torch.tensor([1, 2, 3, 4])
            >>> torch.unsqueeze(x, 0)
            tensor([[ 1,  2,  3,  4]])
            >>> torch.unsqueeze(x, 1)
            tensor([[ 1],
                    [ 2],
                    [ 3],
                    [ 4]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    dims = new_dim[...](self, *dims)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        for dims_iter, in zip(dims):
            auto_generated_args = tuple(x for x in [self] if x is not None)
            self = torch.unsqueeze(*auto_generated_args, dims_iter)
    obj = self
    ref_shape = size_mapping['unsqueeze'](self_shape, dims)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "unsqueeze"
    return obj

def squeeze(self: 'Tensor', *dims: del_dim[...], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.squeeze'. The automatically generated codes are as follows:
        [ 1]: def squeeze(self: 'Tensor', *dims: del_dim[...], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [self]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if self is None: ...
        [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [17]:
        [18]:     dims = del_dim[...](self, *dims)
        [19]:
        [20]:     # Obtain available sized in arguments (which will be fed into size function).
        [21]:     self_shape=None if self is None else Size(self.shape)
        [22]:     # Use the given inner codes if they are provided.
        [23]:     torch_returned = False
        [24]:     valid_dims = []
        [25]:     with torch._C.DisableTorchFunction():
        [26]:         for d in dims[::-1]:
        [27]:             if self.size(d) == 1:
        [28]:                 valid_dims.append(d)
        [29]:                 self = torch_super(self, 'squeeze')(d)
        [30]:     dims = tuple(valid_dims)
        [31]:     obj = self
        [32]:     ref_shape = size_mapping['squeeze'](self_shape, dims)
        [33]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [34]:
        [35]:     def update_special(obj, updates):
        [36]:         if isinstance(obj, tuple):
        [37]:             for x in obj: update_special(x, updates)
        [38]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [39]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [40]:
        [41]:     if getattr(obj, 'grad_fn', None) is not None:
        [42]:         obj.grad_fn_name = "squeeze"
        [43]:     return obj
        [44]:
        The document of the original function is:

        squeeze(input, dim=None) -> Tensor

        Returns a tensor with all specified dimensions of :attr:`input` of size `1` removed.

        For example, if `input` is of shape:
        :math:`(A \times 1 \times B \times C \times 1 \times D)` then the `input.squeeze()`
        will be of shape: :math:`(A \times B \times C \times D)`.

        When :attr:`dim` is given, a squeeze operation is done only in the given
        dimension(s). If `input` is of shape: :math:`(A \times 1 \times B)`,
        ``squeeze(input, 0)`` leaves the tensor unchanged, but ``squeeze(input, 1)``
        will squeeze the tensor to the shape :math:`(A \times B)`.

        .. note:: The returned tensor shares the storage with the input tensor,
                  so changing the contents of one will change the contents of the other.

        .. warning:: If the tensor has a batch dimension of size 1, then `squeeze(input)`
                  will also remove the batch dimension, which can lead to unexpected
                  errors. Consider specifying only the dims you wish to be squeezed.

        Args:
            input (Tensor): the input tensor.
            dim (int or tuple of ints, optional): if given, the input will be squeezed
                   only in the specified dimensions.

                .. versionchanged:: 2.0
                   :attr:`dim` now accepts tuples of dimensions.

        Example::

            >>> x = torch.zeros(2, 1, 2, 1, 2)
            >>> x.size()
            torch.Size([2, 1, 2, 1, 2])
            >>> y = torch.squeeze(x)
            >>> y.size()
            torch.Size([2, 2, 2])
            >>> y = torch.squeeze(x, 0)
            >>> y.size()
            torch.Size([2, 1, 2, 1, 2])
            >>> y = torch.squeeze(x, 1)
            >>> y.size()
            torch.Size([2, 2, 1, 2])
            >>> y = torch.squeeze(x, (1, 2, 3))
            torch.Size([2, 2, 2])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    dims = del_dim[...](self, *dims)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    valid_dims = []
    with torch._C.DisableTorchFunction():
        for d in dims[::-1]:
            if self.size(d) == 1:
                valid_dims.append(d)
                self = torch_super(self, 'squeeze')(d)
    dims = tuple(valid_dims)
    obj = self
    ref_shape = size_mapping['squeeze'](self_shape, dims)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "squeeze"
    return obj

def flatten(self: 'Tensor', *dims: exist_dim, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.flatten'. The automatically generated codes are as follows:
        [ 1]: def flatten(self: 'Tensor', *dims: exist_dim, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [self]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if self is None: ...
        [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [17]:
        [18]:     dims = exist_dim(self, *dims)
        [19]:
        [20]:     # Obtain available sized in arguments (which will be fed into size function).
        [21]:     self_shape=None if self is None else Size(self.shape)
        [22]:     # Use the given inner codes if they are provided.
        [23]:     torch_returned = False
        [24]:     if len(dims) > 1:  flat_range = min_(dims), max_(dims) + 1
        [25]:     elif len(dims) == 1:  flat_range = dims[0], self.n_dim
        [26]:     else:  flat_range = 0, self.n_dim
        [27]:     ref_shape = self.shape[:flat_range[0] + 1] + self.shape[flat_range[1]:]
        [28]:     if len(ref_shape) == 0:  ref_shape = bt.Size(FuncDim(1))
        [29]:     with torch._C.DisableTorchFunction():
        [30]:         obj = Tensor.inherit_from(torch_super(self, 'flatten')(flat_range[0], flat_range[1]-1), self, shape=ref_shape)
        [31]:
        [32]:     def update_special(obj, updates):
        [33]:         if isinstance(obj, tuple):
        [34]:             for x in obj: update_special(x, updates)
        [35]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [36]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [37]:
        [38]:     if getattr(obj, 'grad_fn', None) is not None:
        [39]:         obj.grad_fn_name = "flatten"
        [40]:     return obj
        [41]:
        The document of the original function is:

        flatten(input, start_dim=0, end_dim=-1) -> Tensor

        Flattens :attr:`input` by reshaping it into a one-dimensional tensor. If :attr:`start_dim` or :attr:`end_dim`
        are passed, only dimensions starting with :attr:`start_dim` and ending with :attr:`end_dim` are flattened.
        The order of elements in :attr:`input` is unchanged.

        Unlike NumPy's flatten, which always copies input's data, this function may return the original object, a view,
        or copy. If no dimensions are flattened, then the original object :attr:`input` is returned. Otherwise, if input can
        be viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the
        flattened shape is input's data copied. See :meth:`torch.Tensor.view` for details on when a view will be returned.

        .. note::
            Flattening a zero-dimensional tensor will return a one-dimensional view.

        Args:
            input (Tensor): the input tensor.
            start_dim (int): the first dim to flatten
            end_dim (int): the last dim to flatten

        Example::

            >>> t = torch.tensor([[[1, 2],
            ...                    [3, 4]],
            ...                   [[5, 6],
            ...                    [7, 8]]])
            >>> torch.flatten(t)
            tensor([1, 2, 3, 4, 5, 6, 7, 8])
            >>> torch.flatten(t, start_dim=1)
            tensor([[1, 2, 3, 4],
                    [5, 6, 7, 8]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    dims = exist_dim(self, *dims)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    if len(dims) > 1:  flat_range = min_(dims), max_(dims) + 1
    elif len(dims) == 1:  flat_range = dims[0], self.n_dim
    else:  flat_range = 0, self.n_dim
    ref_shape = self.shape[:flat_range[0] + 1] + self.shape[flat_range[1]:]
    if len(ref_shape) == 0:  ref_shape = bt.Size(FuncDim(1))
    with torch._C.DisableTorchFunction():
        obj = Tensor.inherit_from(torch_super(self, 'flatten')(flat_range[0], flat_range[1]-1), self, shape=ref_shape)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "flatten"
    return obj

def transpose(self: 'Tensor', dim0: exist_dim[1], dim1: exist_dim[1], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.transpose'. The automatically generated codes are as follows:
        [ 1]: def transpose(self: 'Tensor', dim0: exist_dim[1], dim1: exist_dim[1], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [self]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if self is None: ...
        [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [17]:
        [18]:     dim0 = exist_dim[1](self, dim0)
        [19]:     dim1 = exist_dim[1](self, dim1)
        [20]:
        [21]:     # Obtain available sized in arguments (which will be fed into size function).
        [22]:     self_shape=None if self is None else Size(self.shape)
        [23]:     # Use the given inner codes if they are provided.
        [24]:     with torch._C.DisableTorchFunction():
        [25]:         for dim0_iter, dim1_iter in zip(dim0, dim1):
        [26]:             auto_generated_args = tuple(x for x in [self,dim0_iter,dim1_iter] if x is not None)
        [27]:             self = torch.transpose(*auto_generated_args, )
        [28]:     obj = self
        [29]:     ref_shape = size_mapping['transpose'](self_shape, dim0, dim1)
        [30]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [31]:
        [32]:     def update_special(obj, updates):
        [33]:         if isinstance(obj, tuple):
        [34]:             for x in obj: update_special(x, updates)
        [35]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [36]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [37]:
        [38]:     if getattr(obj, 'grad_fn', None) is not None:
        [39]:         obj.grad_fn_name = "transpose"
        [40]:     return obj
        [41]:
        The document of the original function is:

        transpose(input, dim0, dim1) -> Tensor

        Returns a tensor that is a transposed version of :attr:`input`.
        The given dimensions :attr:`dim0` and :attr:`dim1` are swapped.

        If :attr:`input` is a strided tensor then the resulting :attr:`out`
        tensor shares its underlying storage with the :attr:`input` tensor, so
        changing the content of one would change the content of the other.

        If :attr:`input` is a :ref:`sparse tensor <sparse-docs>` then the
        resulting :attr:`out` tensor *does not* share the underlying storage
        with the :attr:`input` tensor.

        If :attr:`input` is a :ref:`sparse tensor <sparse-docs>` with compressed
        layout (SparseCSR, SparseBSR, SparseCSC or SparseBSC) the arguments
        :attr:`dim0` and :attr:`dim1` must be both batch dimensions, or must
        both be sparse dimensions. The batch dimensions of a sparse tensor are the
        dimensions preceding the sparse dimensions.

        .. note::
            Transpositions which interchange the sparse dimensions of a `SparseCSR`
            or `SparseCSC` layout tensor will result in the layout changing between
            the two options. Transposition of the sparse dimensions of a ` SparseBSR`
            or `SparseBSC` layout tensor will likewise generate a result with the
            opposite layout.

        Args:
            input (Tensor): the input tensor.
            dim0 (int): the first dimension to be transposed
            dim1 (int): the second dimension to be transposed

        Example::

            >>> x = torch.randn(2, 3)
            >>> x
            tensor([[ 1.0028, -0.9893,  0.5809],
                    [-0.1669,  0.7299,  0.4942]])
            >>> torch.transpose(x, 0, 1)
            tensor([[ 1.0028, -0.1669],
                    [-0.9893,  0.7299],
                    [ 0.5809,  0.4942]])

        See also :func:`torch.t`.

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    dim0 = exist_dim[1](self, dim0)
    dim1 = exist_dim[1](self, dim1)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        for dim0_iter, dim1_iter in zip(dim0, dim1):
            auto_generated_args = tuple(x for x in [self,dim0_iter,dim1_iter] if x is not None)
            self = torch.transpose(*auto_generated_args, )
    obj = self
    ref_shape = size_mapping['transpose'](self_shape, dim0, dim1)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "transpose"
    return obj

def t(self, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.t'. The automatically generated codes are as follows:
        [ 1]: def t(self, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in []:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     # Obtain available sized in arguments (which will be fed into size function).
        [13]:
        [14]:     # Use the given inner codes if they are provided.
        [15]:     with torch._C.DisableTorchFunction():
        [16]:         auto_generated_args = tuple(x for x in [self] if x is not None)
        [17]:         obj = torch.t(*auto_generated_args, )
        [18]:     ref_shape = size_mapping['t']()
        [19]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [20]:
        [21]:     def update_special(obj, updates):
        [22]:         if isinstance(obj, tuple):
        [23]:             for x in obj: update_special(x, updates)
        [24]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [25]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [26]:
        [27]:     if getattr(obj, 'grad_fn', None) is not None:
        [28]:         obj.grad_fn_name = "t"
        [29]:     return obj
        [30]:
        The document of the original function is:

        t(input) -> Tensor

        Expects :attr:`input` to be <= 2-D tensor and transposes dimensions 0
        and 1.

        0-D and 1-D tensors are returned as is. When input is a 2-D tensor this
        is equivalent to ``transpose(input, 0, 1)``.

        Args:
            input (Tensor): the input tensor.

        Example::

            >>> x = torch.randn(())
            >>> x
            tensor(0.1995)
            >>> torch.t(x)
            tensor(0.1995)
            >>> x = torch.randn(3)
            >>> x
            tensor([ 2.4320, -0.4608,  0.7702])
            >>> torch.t(x)
            tensor([ 2.4320, -0.4608,  0.7702])
            >>> x = torch.randn(2, 3)
            >>> x
            tensor([[ 0.4875,  0.9158, -0.5872],
                    [ 0.3938, -0.6929,  0.6932]])
            >>> torch.t(x)
            tensor([[ 0.4875,  0.3938],
                    [ 0.9158, -0.6929],
                    [-0.5872,  0.6932]])

        See also :func:`torch.transpose`.

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    # Obtain available sized in arguments (which will be fed into size function).

    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [self] if x is not None)
        obj = torch.t(*auto_generated_args, )
    ref_shape = size_mapping['t']()
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "t"
    return obj

def permute(self: 'Tensor', *dims, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.permute'. The automatically generated codes are as follows:
        [ 1]: def permute(self: 'Tensor', *dims, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [self]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if self is None: ...
        [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     self_shape=None if self is None else Size(self.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     torch_returned = False
        [22]:     ref_shape = Size(*dims)
        [23]:     dims = exist_dim(self, *dims)
        [24]:     with torch._C.DisableTorchFunction():
        [25]:         obj = torch.permute(self, dims)
        [26]:     obj = Tensor.inherit_from(obj, self, shape=size_mapping['permute'](self_shape, dims))
        [27]:     if ref_shape.has_special:  obj.special_from(ref_shape)
        [28]:     obj = obj
        [29]:
        [30]:     def update_special(obj, updates):
        [31]:         if isinstance(obj, tuple):
        [32]:             for x in obj: update_special(x, updates)
        [33]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [34]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [35]:
        [36]:     if getattr(obj, 'grad_fn', None) is not None:
        [37]:         obj.grad_fn_name = "permute"
        [38]:     return obj
        [39]:
        The document of the original function is:

        permute(input, dims) -> Tensor

        Returns a view of the original tensor :attr:`input` with its dimensions permuted.

        Args:
            input (Tensor): the input tensor.
            dims (tuple of int): The desired ordering of dimensions

        Example:
            >>> x = torch.randn(2, 3, 5)
            >>> x.size()
            torch.Size([2, 3, 5])
            >>> torch.permute(x, (2, 0, 1)).size()
            torch.Size([5, 2, 3])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    ref_shape = Size(*dims)
    dims = exist_dim(self, *dims)
    with torch._C.DisableTorchFunction():
        obj = torch.permute(self, dims)
    obj = Tensor.inherit_from(obj, self, shape=size_mapping['permute'](self_shape, dims))
    if ref_shape.has_special:  obj.special_from(ref_shape)
    obj = obj

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "permute"
    return obj

def standard_shape(self, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.standard_shape'. The automatically generated codes are as follows:
        [ 1]: def standard_shape(self, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in []:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     # Obtain available sized in arguments (which will be fed into size function).
        [13]:
        [14]:     # Use the given inner codes if they are provided.
        [15]:     torch_returned = False
        [16]:     left_dims = []
        [17]:     right_dims = []
        [18]:     for dim in MajorSpecialDimensions + [SpaceDim]:
        [19]:         start, stop = getattr(self, f"{dim.name}_range")
        [20]:         if start is not None:
        [21]:             if dim.last:  right_dims = list(range(start, stop)) + right_dims
        [22]:             else:  left_dims.extend(list(range(start, stop)))
        [23]:     permutation = left_dims + right_dims
        [24]:     obj = permute(self, *permutation) # suppress:  special_from
        [25]:
        [26]:     def update_special(obj, updates):
        [27]:         if isinstance(obj, tuple):
        [28]:             for x in obj: update_special(x, updates)
        [29]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [30]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [31]:
        [32]:     if getattr(obj, 'grad_fn', None) is not None:
        [33]:         obj.grad_fn_name = "standard_shape"
        [34]:     return obj
        [35]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    # Obtain available sized in arguments (which will be fed into size function).

    # Use the given inner codes if they are provided.
    torch_returned = False
    left_dims = []
    right_dims = []
    for dim in MajorSpecialDimensions + [SpaceDim]:
        start, stop = getattr(self, f"{dim.name}_range")
        if start is not None:
            if dim.last:  right_dims = list(range(start, stop)) + right_dims
            else:  left_dims.extend(list(range(start, stop)))
    permutation = left_dims + right_dims
    obj = permute(self, *permutation) # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "standard_shape"
    return obj

def duplicate(self: 'Tensor', num=2, dim: new_dim[1]={}, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    data.duplicate(num, 0): data(n_1, n_2) => (num, n_1, n_2)
    Duplicate a tensor by `num` times and stack them as a new tensor.

    Args:
        num (int, optional): The number of duplications. Defaults to 2.
        dim (int/new_dim, optional): The dimension to stack along. Defaults to batch.

    Automatically inheritted method from 'torch.duplicate'. The automatically generated codes are as follows:
        [ 1]: def duplicate(self: 'Tensor', num=2, dim: new_dim[1]={}, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:     data.duplicate(num, 0): data(n_1, n_2) => (num, n_1, n_2)
        [ 4]:     Duplicate a tensor by `num` times and stack them as a new tensor.
        [ 5]:
        [ 6]:     Args:
        [ 7]:         num (int, optional): The number of duplications. Defaults to 2.
        [ 8]:         dim (int/new_dim, optional): The dimension to stack along. Defaults to batch.
        [ 9]:     '''
        [10]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [11]:
        [12]:     pivot = None
        [13]:     for t in [self]:
        [14]:         if isinstance(t, torch.Tensor): pivot = t; break
        [15]:     subclass = Tensor.get_tensor_subclass(pivot)
        [16]:
        [17]:     if self is None: ...
        [18]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [19]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [20]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [21]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [22]:
        [23]:     dim = new_dim[1](self, dim)
        [24]:
        [25]:     # Obtain available sized in arguments (which will be fed into size function).
        [26]:     self_shape=None if self is None else Size(self.shape)
        [27]:     # Use the given inner codes if they are provided.
        [28]:     torch_returned = False
        [29]:     obj = self.unsqueeze(dim).repeat((1,) * dim[0] + (num,) + (1,) * (self.ndim - dim[0])).special_from(dim)
        [30]:
        [31]:     def update_special(obj, updates):
        [32]:         if isinstance(obj, tuple):
        [33]:             for x in obj: update_special(x, updates)
        [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [36]:
        [37]:     if getattr(obj, 'grad_fn', None) is not None:
        [38]:         obj.grad_fn_name = "duplicate"
        [39]:     return obj
        [40]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    dim = new_dim[1](self, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    obj = self.unsqueeze(dim).repeat((1,) * dim[0] + (num,) + (1,) * (self.ndim - dim[0])).special_from(dim)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "duplicate"
    return obj

def amplify(self: 'Tensor', num=2, dim: exist_dim[1]={}, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    data.amplify(num, 0): data(n_1, n_2) => (n_1 * num, n_2)
    Amplify a dimension of a tensor by enlarging with duplications: amplifying [a, b, c] with number 2 results in [a, a, b, b, c, c].
    Note that one should use 'repeated' (for one dimension) or 'repeat' (for all dimensions) to duplicate the whole tensor and
        concatenate the duplications together ([a, b, c] => [a, b, c, a, b, c]).

    Args:
        num (int, optional): The number of duplications. Defaults to 2.
        dim (int/new_dim, optional): The dimension to stack along. Defaults to batch.

    Automatically inheritted method from 'torch.amplify'. The automatically generated codes are as follows:
        [ 1]: def amplify(self: 'Tensor', num=2, dim: exist_dim[1]={}, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:     data.amplify(num, 0): data(n_1, n_2) => (n_1 * num, n_2)
        [ 4]:     Amplify a dimension of a tensor by enlarging with duplications: amplifying [a, b, c] with number 2 results in [a, a, b, b, c, c].
        [ 5]:     Note that one should use 'repeated' (for one dimension) or 'repeat' (for all dimensions) to duplicate the whole tensor and
        [ 6]:         concatenate the duplications together ([a, b, c] => [a, b, c, a, b, c]).
        [ 7]:
        [ 8]:     Args:
        [ 9]:         num (int, optional): The number of duplications. Defaults to 2.
        [10]:         dim (int/new_dim, optional): The dimension to stack along. Defaults to batch.
        [11]:     '''
        [12]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [13]:
        [14]:     pivot = None
        [15]:     for t in [self]:
        [16]:         if isinstance(t, torch.Tensor): pivot = t; break
        [17]:     subclass = Tensor.get_tensor_subclass(pivot)
        [18]:
        [19]:     if self is None: ...
        [20]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [21]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [22]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [23]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [24]:
        [25]:     dim = exist_dim[1](self, dim)
        [26]:
        [27]:     # Obtain available sized in arguments (which will be fed into size function).
        [28]:     self_shape=None if self is None else Size(self.shape)
        [29]:     # Use the given inner codes if they are provided.
        [30]:     torch_returned = False
        [31]:     dim = dim[0]
        [32]:     with self.hide_special():
        [33]:         output = self.duplicate(num, dim+1).flatten(dim, dim + 1)
        [34]:     obj = output.special_from(self)
        [35]:
        [36]:     def update_special(obj, updates):
        [37]:         if isinstance(obj, tuple):
        [38]:             for x in obj: update_special(x, updates)
        [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [41]:
        [42]:     if getattr(obj, 'grad_fn', None) is not None:
        [43]:         obj.grad_fn_name = "amplify"
        [44]:     return obj
        [45]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    dim = exist_dim[1](self, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    dim = dim[0]
    with self.hide_special():
        output = self.duplicate(num, dim+1).flatten(dim, dim + 1)
    obj = output.special_from(self)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "amplify"
    return obj

def repeated(self: 'Tensor', num=2, dim: exist_dim[1]={}, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    data.repeated(num, 0): data(n_1, n_2) => (num * n_1, n_2)
    Repeat a tensor by `num` times along one dimension `dim` (use 'repeat' for multiple dimensions) and concatenate them as a new tensor.
    Note that repeating [a, b, c] with number 2 results in [a, b, c, a, b, c].
        One should use 'amplify' to amplify to [a, a, b, b, c, c].

    Args:
        num (int, optional): The number of duplications. Defaults to 2.
        dim (int/new_dim, optional): The dimension to stack along. Defaults to batch.

    Automatically inheritted method from 'torch.repeated'. The automatically generated codes are as follows:
        [ 1]: def repeated(self: 'Tensor', num=2, dim: exist_dim[1]={}, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:     data.repeated(num, 0): data(n_1, n_2) => (num * n_1, n_2)
        [ 4]:     Repeat a tensor by `num` times along one dimension `dim` (use 'repeat' for multiple dimensions) and concatenate them as a new tensor.
        [ 5]:     Note that repeating [a, b, c] with number 2 results in [a, b, c, a, b, c].
        [ 6]:         One should use 'amplify' to amplify to [a, a, b, b, c, c].
        [ 7]:
        [ 8]:     Args:
        [ 9]:         num (int, optional): The number of duplications. Defaults to 2.
        [10]:         dim (int/new_dim, optional): The dimension to stack along. Defaults to batch.
        [11]:     '''
        [12]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [13]:
        [14]:     pivot = None
        [15]:     for t in [self]:
        [16]:         if isinstance(t, torch.Tensor): pivot = t; break
        [17]:     subclass = Tensor.get_tensor_subclass(pivot)
        [18]:
        [19]:     if self is None: ...
        [20]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [21]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [22]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [23]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [24]:
        [25]:     dim = exist_dim[1](self, dim)
        [26]:
        [27]:     # Obtain available sized in arguments (which will be fed into size function).
        [28]:     self_shape=None if self is None else Size(self.shape)
        [29]:     # Use the given inner codes if they are provided.
        [30]:     torch_returned = False
        [31]:     dim = dim[0]
        [32]:     with self.hide_special():
        [33]:         output = self.duplicate(num, dim).flatten(dim, dim + 1)
        [34]:     obj = output.special_from(self)
        [35]:
        [36]:     def update_special(obj, updates):
        [37]:         if isinstance(obj, tuple):
        [38]:             for x in obj: update_special(x, updates)
        [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [41]:
        [42]:     if getattr(obj, 'grad_fn', None) is not None:
        [43]:         obj.grad_fn_name = "repeated"
        [44]:     return obj
        [45]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    dim = exist_dim[1](self, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    dim = dim[0]
    with self.hide_special():
        output = self.duplicate(num, dim).flatten(dim, dim + 1)
    obj = output.special_from(self)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "repeated"
    return obj

def repeat(self: 'Tensor', *size: 'Size', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.repeat'. The automatically generated codes are as follows:
        [ 1]: def repeat(self: 'Tensor', *size: 'Size', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [self]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if self is None: ...
        [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     self_shape=None if self is None else Size(self.shape)
        [20]:     size=Size(*size)
        [21]:     # Use the given inner codes if they are provided.
        [22]:     torch_returned = False
        [23]:     with torch._C.DisableTorchFunction():
        [24]:         obj = Tensor.inherit_from(torch_super(self, 'repeat')(*size), self).update_special_from(size)
        [25]:
        [26]:     def update_special(obj, updates):
        [27]:         if isinstance(obj, tuple):
        [28]:             for x in obj: update_special(x, updates)
        [29]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [30]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [31]:
        [32]:     if getattr(obj, 'grad_fn', None) is not None:
        [33]:         obj.grad_fn_name = "repeat"
        [34]:     return obj
        [35]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    size=Size(*size)
    # Use the given inner codes if they are provided.
    torch_returned = False
    with torch._C.DisableTorchFunction():
        obj = Tensor.inherit_from(torch_super(self, 'repeat')(*size), self).update_special_from(size)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "repeat"
    return obj

# resamplers
def gather(self: 'Tensor', dim: exist_dim[1], index, *, sparse_grad=False, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.gather'. The automatically generated codes are as follows:
        [ 1]: def gather(self: 'Tensor', dim: exist_dim[1], index, *, sparse_grad=False, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [self]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if self is None: ...
        [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [17]:
        [18]:     dim = exist_dim[1](self, dim)
        [19]:
        [20]:     # Obtain available sized in arguments (which will be fed into size function).
        [21]:     self_shape=None if self is None else Size(self.shape)
        [22]:     # Use the given inner codes if they are provided.
        [23]:     with torch._C.DisableTorchFunction():
        [24]:         for dim_iter, in zip(dim):
        [25]:             auto_generated_args = tuple(x for x in [self,dim_iter,index] if x is not None)
        [26]:             self = torch.gather(*auto_generated_args, sparse_grad=sparse_grad,out=out)
        [27]:     obj = self
        [28]:     ref_shape = size_mapping['gather'](self_shape, dim)
        [29]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [30]:
        [31]:     def update_special(obj, updates):
        [32]:         if isinstance(obj, tuple):
        [33]:             for x in obj: update_special(x, updates)
        [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [36]:
        [37]:     if getattr(obj, 'grad_fn', None) is not None:
        [38]:         obj.grad_fn_name = "gather"
        [39]:     return obj
        [40]:
        The document of the original function is:

        gather(input, dim, index, *, sparse_grad=False, out=None) -> Tensor

        Gathers values along an axis specified by `dim`.

        For a 3-D tensor the output is specified by::

            out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
            out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
            out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2

        :attr:`input` and :attr:`index` must have the same number of dimensions.
        It is also required that ``index.size(d) <= input.size(d)`` for all
        dimensions ``d != dim``.  :attr:`out` will have the same shape as :attr:`index`.
        Note that ``input`` and ``index`` do not broadcast against each other.

        Args:
            input (Tensor): the source tensor
            dim (int): the axis along which to index
            index (LongTensor): the indices of elements to gather

        Keyword arguments:
            sparse_grad (bool, optional): If ``True``, gradient w.r.t. :attr:`input` will be a sparse tensor.
            out (Tensor, optional): the destination tensor

        Example::

            >>> t = torch.tensor([[1, 2], [3, 4]])
            >>> torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))
            tensor([[ 1,  1],
                    [ 4,  3]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    dim = exist_dim[1](self, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        for dim_iter, in zip(dim):
            auto_generated_args = tuple(x for x in [self,dim_iter,index] if x is not None)
            self = torch.gather(*auto_generated_args, sparse_grad=sparse_grad,out=out)
    obj = self
    ref_shape = size_mapping['gather'](self_shape, dim)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "gather"
    return obj

def flip(self: 'Tensor', *dims: exist_dim, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.flip'. The automatically generated codes are as follows:
        [ 1]: def flip(self: 'Tensor', *dims: exist_dim, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [self]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if self is None: ...
        [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [17]:
        [18]:     dims = exist_dim(self, *dims)
        [19]:
        [20]:     # Obtain available sized in arguments (which will be fed into size function).
        [21]:     self_shape=None if self is None else Size(self.shape)
        [22]:     # Use the given inner codes if they are provided.
        [23]:     with torch._C.DisableTorchFunction():
        [24]:         auto_generated_args = tuple(x for x in [self] if x is not None)
        [25]:         obj = torch.flip(*auto_generated_args, *dims)
        [26]:     ref_shape = size_mapping['flip'](self_shape, dims)
        [27]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [28]:
        [29]:     def update_special(obj, updates):
        [30]:         if isinstance(obj, tuple):
        [31]:             for x in obj: update_special(x, updates)
        [32]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [33]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [34]:
        [35]:     if getattr(obj, 'grad_fn', None) is not None:
        [36]:         obj.grad_fn_name = "flip"
        [37]:     return obj
        [38]:
        The document of the original function is:

        flip(input, dims) -> Tensor

        Reverse the order of an n-D tensor along given axis in dims.

        .. note::
            `torch.flip` makes a copy of :attr:`input`'s data. This is different from NumPy's `np.flip`,
            which returns a view in constant time. Since copying a tensor's data is more work than viewing that data,
            `torch.flip` is expected to be slower than `np.flip`.

        Args:
            input (Tensor): the input tensor.
            dims (a list or tuple): axis to flip on

        Example::

            >>> x = torch.arange(8).view(2, 2, 2)
            >>> x
            tensor([[[ 0,  1],
                     [ 2,  3]],

                    [[ 4,  5],
                     [ 6,  7]]])
            >>> torch.flip(x, [0, 1])
            tensor([[[ 6,  7],
                     [ 4,  5]],

                    [[ 2,  3],
                     [ 0,  1]]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    dims = exist_dim(self, *dims)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [self] if x is not None)
        obj = torch.flip(*auto_generated_args, *dims)
    ref_shape = size_mapping['flip'](self_shape, dims)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "flip"
    return obj

# properties
def detach(self: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.detach'. The automatically generated codes are as follows:
        [ 1]: def detach(self: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [self]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if self is None: ...
        [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     self_shape=None if self is None else Size(self.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [self] if x is not None)
        [23]:         obj = torch.detach(*auto_generated_args, )
        [24]:     ref_shape = size_mapping['detach'](self_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "detach"
        [35]:     return obj
        [36]:
        The document of the original function is:
        None

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [self] if x is not None)
        obj = torch.detach(*auto_generated_args, )
    ref_shape = size_mapping['detach'](self_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "detach"
    return obj

def quantile(self: 'Tensor', q: 'Tensor', dim=None, keepdim=False, *, interpolation='linear', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.quantile'. The automatically generated codes are as follows:
        [ 1]: def quantile(self: 'Tensor', q: 'Tensor', dim=None, keepdim=False, *, interpolation='linear', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [self, q]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if self is None: ...
        [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [17]:
        [18]:     if q is None: ...
        [19]:     elif not isinstance(q, torch.Tensor): q = torch.tensor(q)
        [20]:     if not isinstance(q, subclass): q = q.as_subclass(subclass).special_from(q.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if q.device.type == 'cpu': q = q.to(pivot.device)
        [23]:
        [24]:     # Obtain available sized in arguments (which will be fed into size function).
        [25]:     self_shape=None if self is None else Size(self.shape)
        [26]:     q_shape=None if q is None else Size(q.shape)
        [27]:     # Use the given inner codes if they are provided.
        [28]:     torch_returned = False
        [29]:     n_dim_count = None
        [30]:     if dim is not None:
        [31]:         dim = exist_dim(self, dim)
        [32]:         if len(dim) > 1:  self = self.flatten(dim); n_dim_count = len(dim)
        [33]:         ref_shape, _, _ = size_mapping_op['quantile'](self.shape, q_shape, dim[:1], keepdim=keepdim)
        [34]:     with torch._C.DisableTorchFunction():
        [35]:         if dim is None:  res = Tensor.inherit_from(torch.quantile(self, q, keepdim=keepdim,interpolation=interpolation), self, shape=q_shape); dim = [int_(q.n_dim > 0)]
        [36]:         else:  res = Tensor.inherit_from(torch.quantile(self, q, dim[0], keepdim=keepdim,interpolation=interpolation), self, shape=ref_shape)
        [37]:     if keepdim:
        [38]:         d = dim[0] + int_(q.n_dim > 0)
        [39]:         obj = res.split_dim(d, res.shape[d:d+1] * n_dim_count)
        [40]:     else: torch_returned = True; obj = res
        [41]:
        [42]:     def update_special(obj, updates):
        [43]:         if isinstance(obj, tuple):
        [44]:             for x in obj: update_special(x, updates)
        [45]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [46]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [47]:
        [48]:     if getattr(obj, 'grad_fn', None) is not None:
        [49]:         obj.grad_fn_name = "quantile"
        [50]:     return obj
        [51]:
        The document of the original function is:

        quantile(input, q, dim=None, keepdim=False, *, interpolation='linear', out=None) -> Tensor

        Computes the q-th quantiles of each row of the :attr:`input` tensor along the dimension :attr:`dim`.

        To compute the quantile, we map q in [0, 1] to the range of indices [0, n] to find the location
        of the quantile in the sorted input. If the quantile lies between two data points ``a < b`` with
        indices ``i`` and ``j`` in the sorted order, result is computed according to the given
        :attr:`interpolation` method as follows:

        - ``linear``: ``a + (b - a) * fraction``, where ``fraction`` is the fractional part of the computed quantile index.
        - ``lower``: ``a``.
        - ``higher``: ``b``.
        - ``nearest``: ``a`` or ``b``, whichever's index is closer to the computed quantile index (rounding down for .5 fractions).
        - ``midpoint``: ``(a + b) / 2``.

        If :attr:`q` is a 1D tensor, the first dimension of the output represents the quantiles and has size
        equal to the size of :attr:`q`, the remaining dimensions are what remains from the reduction.

        .. note::
            By default :attr:`dim` is ``None`` resulting in the :attr:`input` tensor being flattened before computation.

        Args:
            input (Tensor): the input tensor.
            q (float or Tensor): a scalar or 1D tensor of values in the range [0, 1].
            dim (int): the dimension to reduce.
            keepdim (bool): whether the output tensor has :attr:`dim` retained or not.

        Keyword arguments:
            interpolation (str): interpolation method to use when the desired quantile lies between two data points.
                                    Can be ``linear``, ``lower``, ``higher``, ``midpoint`` and ``nearest``.
                                    Default is ``linear``.
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.randn(2, 3)
            >>> a
            tensor([[ 0.0795, -1.2117,  0.9765],
                    [ 1.1707,  0.6706,  0.4884]])
            >>> q = torch.tensor([0.25, 0.5, 0.75])
            >>> torch.quantile(a, q, dim=1, keepdim=True)
            tensor([[[-0.5661],
                    [ 0.5795]],

                    [[ 0.0795],
                    [ 0.6706]],

                    [[ 0.5280],
                    [ 0.9206]]])
            >>> torch.quantile(a, q, dim=1, keepdim=True).shape
            torch.Size([3, 2, 1])
            >>> a = torch.arange(4.)
            >>> a
            tensor([0., 1., 2., 3.])
            >>> torch.quantile(a, 0.6, interpolation='linear')
            tensor(1.8000)
            >>> torch.quantile(a, 0.6, interpolation='lower')
            tensor(1.)
            >>> torch.quantile(a, 0.6, interpolation='higher')
            tensor(2.)
            >>> torch.quantile(a, 0.6, interpolation='midpoint')
            tensor(1.5000)
            >>> torch.quantile(a, 0.6, interpolation='nearest')
            tensor(2.)
            >>> torch.quantile(a, 0.4, interpolation='nearest')
            tensor(1.)

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self, q]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    if q is None: ...
    elif not isinstance(q, torch.Tensor): q = torch.tensor(q)
    if not isinstance(q, subclass): q = q.as_subclass(subclass).special_from(q.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if q.device.type == 'cpu': q = q.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    q_shape=None if q is None else Size(q.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    n_dim_count = None
    if dim is not None:
        dim = exist_dim(self, dim)
        if len(dim) > 1:  self = self.flatten(dim); n_dim_count = len(dim)
        ref_shape, _, _ = size_mapping_op['quantile'](self.shape, q_shape, dim[:1], keepdim=keepdim)
    with torch._C.DisableTorchFunction():
        if dim is None:  res = Tensor.inherit_from(torch.quantile(self, q, keepdim=keepdim,interpolation=interpolation), self, shape=q_shape); dim = [int_(q.n_dim > 0)]
        else:  res = Tensor.inherit_from(torch.quantile(self, q, dim[0], keepdim=keepdim,interpolation=interpolation), self, shape=ref_shape)
    if keepdim:
        d = dim[0] + int_(q.n_dim > 0)
        obj = res.split_dim(d, res.shape[d:d+1] * n_dim_count)
    else: torch_returned = True; obj = res

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "quantile"
    return obj

def val_range(self, dim: exist_dim=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    Compute the range in dimensions `dim`, resulting in a squeeze of these dimensions
        to a sole functional dimension of 2, i.e. the minimal and maximal values.

    Args: dim (int/exist_dim, optional): The dimensions to find range. Defaults to None.
    Output: ((2), ...[size without the specified dimnsions])

    Automatically inheritted method from 'torch.val_range'. The automatically generated codes are as follows:
        [ 1]: def val_range(self, dim: exist_dim=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:     Compute the range in dimensions `dim`, resulting in a squeeze of these dimensions
        [ 4]:         to a sole functional dimension of 2, i.e. the minimal and maximal values.
        [ 5]:
        [ 6]:     Args: dim (int/exist_dim, optional): The dimensions to find range. Defaults to None.
        [ 7]:     Output: ((2), ...[size without the specified dimnsions])
        [ 8]:     '''
        [ 9]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [10]:
        [11]:     pivot = None
        [12]:     for t in []:
        [13]:         if isinstance(t, torch.Tensor): pivot = t; break
        [14]:     subclass = Tensor.get_tensor_subclass(pivot)
        [15]:
        [16]:     dim = exist_dim(self, dim)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:
        [20]:     # Use the given inner codes if they are provided.
        [21]:     torch_returned = False
        [22]:     obj = stack(self.min(dim), self.max(dim), FuncDim) # suppress:  special_from
        [23]:
        [24]:     def update_special(obj, updates):
        [25]:         if isinstance(obj, tuple):
        [26]:             for x in obj: update_special(x, updates)
        [27]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [28]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [29]:
        [30]:     if getattr(obj, 'grad_fn', None) is not None:
        [31]:         obj.grad_fn_name = "val_range"
        [32]:     return obj
        [33]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    dim = exist_dim(self, dim)

    # Obtain available sized in arguments (which will be fed into size function).

    # Use the given inner codes if they are provided.
    torch_returned = False
    obj = stack(self.min(dim), self.max(dim), FuncDim) # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "val_range"
    return obj

# reductions
def sum(input: 'Tensor', *dim: del_dim[:], keepdim=False, dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.sum'. The automatically generated codes are as follows:
        [ 1]: def sum(input: 'Tensor', *dim: del_dim[:], keepdim=False, dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     dim = del_dim[:](input, *dim)
        [19]:
        [20]:     # Obtain available sized in arguments (which will be fed into size function).
        [21]:     input_shape=None if input is None else Size(input.shape)
        [22]:     # Use the given inner codes if they are provided.
        [23]:     with torch._C.DisableTorchFunction():
        [24]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [25]:         obj = torch.sum(*auto_generated_args, dim,keepdim=keepdim,dtype=dtype)
        [26]:     ref_shape = size_mapping['sum'](input_shape, dim, **dict(keepdim=keepdim, dtype=dtype))
        [27]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [28]:
        [29]:     def update_special(obj, updates):
        [30]:         if isinstance(obj, tuple):
        [31]:             for x in obj: update_special(x, updates)
        [32]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [33]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [34]:
        [35]:     if getattr(obj, 'grad_fn', None) is not None:
        [36]:         obj.grad_fn_name = "sum"
        [37]:     return obj
        [38]:
        The document of the original function is:

        sum(input, *, dtype=None) -> Tensor

        Returns the sum of all elements in the :attr:`input` tensor.

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                If specified, the input tensor is casted to :attr:`dtype` before the operation
                is performed. This is useful for preventing data type overflows. Default: None.

        Example::

            >>> a = torch.randn(1, 3)
            >>> a
            tensor([[ 0.1133, -0.9567,  0.2958]])
            >>> torch.sum(a)
            tensor(-0.5475)

        .. function:: sum(input, dim, keepdim=False, *, dtype=None) -> Tensor
           :noindex:

        Returns the sum of each row of the :attr:`input` tensor in the given
        dimension :attr:`dim`. If :attr:`dim` is a list of dimensions,
        reduce over all of them.

        If :attr:`keepdim` is ``True``, the output tensor is of the same size
        as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
        Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
        output tensor having 1 (or ``len(dim)``) fewer dimension(s).

        Args:
            input (Tensor): the input tensor.

            dim (int or tuple of ints, optional): the dimension or dimensions to reduce.
                If ``None``, all dimensions are reduced.

            keepdim (bool): whether the output tensor has :attr:`dim` retained or not.

        Keyword args:
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                If specified, the input tensor is casted to :attr:`dtype` before the operation
                is performed. This is useful for preventing data type overflows. Default: None.

        Example::

            >>> a = torch.randn(4, 4)
            >>> a
            tensor([[ 0.0569, -0.2475,  0.0737, -0.3429],
                    [-0.2993,  0.9138,  0.9337, -1.6864],
                    [ 0.1132,  0.7892, -0.1003,  0.5688],
                    [ 0.3637, -0.9906, -0.4752, -1.5197]])
            >>> torch.sum(a, 1)
            tensor([-0.4598, -0.1381,  1.3708, -2.6217])
            >>> b = torch.arange(4 * 5 * 6).view(4, 5, 6)
            >>> torch.sum(b, (2, 1))
            tensor([  435.,  1335.,  2235.,  3135.])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = del_dim[:](input, *dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.sum(*auto_generated_args, dim,keepdim=keepdim,dtype=dtype)
    ref_shape = size_mapping['sum'](input_shape, dim, **dict(keepdim=keepdim, dtype=dtype))
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "sum"
    return obj

def prod(input: 'Tensor', dim: del_dim[...]=None, keepdim=False, dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.prod'. The automatically generated codes are as follows:
        [ 1]: def prod(input: 'Tensor', dim: del_dim[...]=None, keepdim=False, dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     dim = del_dim[...](input, dim)
        [19]:
        [20]:     # Obtain available sized in arguments (which will be fed into size function).
        [21]:     input_shape=None if input is None else Size(input.shape)
        [22]:     # Use the given inner codes if they are provided.
        [23]:     with torch._C.DisableTorchFunction():
        [24]:         for dim_iter, in zip(dim[::-1]):
        [25]:             auto_generated_args = tuple(x for x in [input] if x is not None)
        [26]:             input = torch.prod(*auto_generated_args, dim=dim_iter,keepdim=keepdim,dtype=dtype)
        [27]:     obj = input
        [28]:     ref_shape = size_mapping['prod'](input_shape, dim, **dict(dim=dim, keepdim=keepdim, dtype=dtype))
        [29]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [30]:
        [31]:     def update_special(obj, updates):
        [32]:         if isinstance(obj, tuple):
        [33]:             for x in obj: update_special(x, updates)
        [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [36]:
        [37]:     if getattr(obj, 'grad_fn', None) is not None:
        [38]:         obj.grad_fn_name = "prod"
        [39]:     return obj
        [40]:
        The document of the original function is:

        prod(input, *, dtype=None) -> Tensor

        Returns the product of all elements in the :attr:`input` tensor.

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                If specified, the input tensor is casted to :attr:`dtype` before the operation
                is performed. This is useful for preventing data type overflows. Default: None.

        Example::

            >>> a = torch.randn(1, 3)
            >>> a
            tensor([[-0.8020,  0.5428, -1.5854]])
            >>> torch.prod(a)
            tensor(0.6902)

        .. function:: prod(input, dim, keepdim=False, *, dtype=None) -> Tensor
           :noindex:

        Returns the product of each row of the :attr:`input` tensor in the given
        dimension :attr:`dim`.

        If :attr:`keepdim` is ``True``, the output tensor is of the same size
        as :attr:`input` except in the dimension :attr:`dim` where it is of size 1.
        Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in
        the output tensor having 1 fewer dimension than :attr:`input`.

        Args:
            input (Tensor): the input tensor.
            dim (int): the dimension to reduce.
            keepdim (bool): whether the output tensor has :attr:`dim` retained or not.

        Keyword args:
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                If specified, the input tensor is casted to :attr:`dtype` before the operation
                is performed. This is useful for preventing data type overflows. Default: None.

        Example::

            >>> a = torch.randn(4, 2)
            >>> a
            tensor([[ 0.5261, -0.3837],
                    [ 1.1857, -0.2498],
                    [-1.1646,  0.0705],
                    [ 1.1131, -1.0629]])
            >>> torch.prod(a, 1)
            tensor([-0.2018, -0.2962, -0.0821, -1.1831])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = del_dim[...](input, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        for dim_iter, in zip(dim[::-1]):
            auto_generated_args = tuple(x for x in [input] if x is not None)
            input = torch.prod(*auto_generated_args, dim=dim_iter,keepdim=keepdim,dtype=dtype)
    obj = input
    ref_shape = size_mapping['prod'](input_shape, dim, **dict(dim=dim, keepdim=keepdim, dtype=dtype))
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "prod"
    return obj

def mean(input: 'Tensor', *dim: del_dim[:], keepdim=False, dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.mean'. The automatically generated codes are as follows:
        [ 1]: def mean(input: 'Tensor', *dim: del_dim[:], keepdim=False, dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     dim = del_dim[:](input, *dim)
        [19]:
        [20]:     # Obtain available sized in arguments (which will be fed into size function).
        [21]:     input_shape=None if input is None else Size(input.shape)
        [22]:     # Use the given inner codes if they are provided.
        [23]:     with torch._C.DisableTorchFunction():
        [24]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [25]:         obj = torch.mean(*auto_generated_args, dim,keepdim=keepdim,dtype=dtype)
        [26]:     ref_shape = size_mapping['mean'](input_shape, dim, **dict(keepdim=keepdim, dtype=dtype))
        [27]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [28]:
        [29]:     def update_special(obj, updates):
        [30]:         if isinstance(obj, tuple):
        [31]:             for x in obj: update_special(x, updates)
        [32]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [33]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [34]:
        [35]:     if getattr(obj, 'grad_fn', None) is not None:
        [36]:         obj.grad_fn_name = "mean"
        [37]:     return obj
        [38]:
        The document of the original function is:

        mean(input, *, dtype=None) -> Tensor

        Returns the mean value of all elements in the :attr:`input` tensor.

        Args:
            input (Tensor): the input tensor.

        Keyword args:
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                If specified, the input tensor is casted to :attr:`dtype` before the operation
                is performed. This is useful for preventing data type overflows. Default: None.

        Example::

            >>> a = torch.randn(1, 3)
            >>> a
            tensor([[ 0.2294, -0.5481,  1.3288]])
            >>> torch.mean(a)
            tensor(0.3367)

        .. function:: mean(input, dim, keepdim=False, *, dtype=None, out=None) -> Tensor
           :noindex:

        Returns the mean value of each row of the :attr:`input` tensor in the given
        dimension :attr:`dim`. If :attr:`dim` is a list of dimensions,
        reduce over all of them.

        If :attr:`keepdim` is ``True``, the output tensor is of the same size
        as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
        Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
        output tensor having 1 (or ``len(dim)``) fewer dimension(s).

        Args:
            input (Tensor): the input tensor.
            dim (int or tuple of ints): the dimension or dimensions to reduce.
            keepdim (bool): whether the output tensor has :attr:`dim` retained or not.

        Keyword args:
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                If specified, the input tensor is casted to :attr:`dtype` before the operation
                is performed. This is useful for preventing data type overflows. Default: None.
            out (Tensor, optional): the output tensor.

        .. seealso::

            :func:`torch.nanmean` computes the mean value of `non-NaN` elements.

        Example::

            >>> a = torch.randn(4, 4)
            >>> a
            tensor([[-0.3841,  0.6320,  0.4254, -0.7384],
                    [-0.9644,  1.0131, -0.6549, -1.4279],
                    [-0.2951, -1.3350, -0.7694,  0.5600],
                    [ 1.0842, -0.9580,  0.3623,  0.2343]])
            >>> torch.mean(a, 1)
            tensor([-0.0163, -0.5085, -0.4599,  0.1807])
            >>> torch.mean(a, 1, True)
            tensor([[-0.0163],
                    [-0.5085],
                    [-0.4599],
                    [ 0.1807]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = del_dim[:](input, *dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.mean(*auto_generated_args, dim,keepdim=keepdim,dtype=dtype)
    ref_shape = size_mapping['mean'](input_shape, dim, **dict(keepdim=keepdim, dtype=dtype))
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "mean"
    return obj

def std(input: 'Tensor', *dim: del_dim[:], correction=1, keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.std'. The automatically generated codes are as follows:
        [ 1]: def std(input: 'Tensor', *dim: del_dim[:], correction=1, keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     dim = del_dim[:](input, *dim)
        [19]:
        [20]:     # Obtain available sized in arguments (which will be fed into size function).
        [21]:     input_shape=None if input is None else Size(input.shape)
        [22]:     # Use the given inner codes if they are provided.
        [23]:     with torch._C.DisableTorchFunction():
        [24]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [25]:         obj = torch.std(*auto_generated_args, dim,correction=correction,keepdim=keepdim)
        [26]:     ref_shape = size_mapping['std'](input_shape, dim, **dict(correction=correction, keepdim=keepdim))
        [27]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [28]:
        [29]:     def update_special(obj, updates):
        [30]:         if isinstance(obj, tuple):
        [31]:             for x in obj: update_special(x, updates)
        [32]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [33]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [34]:
        [35]:     if getattr(obj, 'grad_fn', None) is not None:
        [36]:         obj.grad_fn_name = "std"
        [37]:     return obj
        [38]:
        The document of the original function is:

        std(input, dim=None, *, correction=1, keepdim=False, out=None) -> Tensor

        Calculates the standard deviation over the dimensions specified by :attr:`dim`.
        :attr:`dim` can be a single dimension, list of dimensions, or ``None`` to
        reduce over all dimensions.

        The standard deviation (:math:`\sigma`) is calculated as

        .. math:: \sigma = \sqrt{\frac{1}{N - \delta N}\sum_{i=0}^{N-1}(x_i-\bar{x})^2}

        where :math:`x` is the sample set of elements, :math:`\bar{x}` is the
        sample mean, :math:`N` is the number of samples and :math:`\delta N` is
        the :attr:`correction`.

        If :attr:`keepdim` is ``True``, the output tensor is of the same size
        as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
        Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
        output tensor having 1 (or ``len(dim)``) fewer dimension(s).

        Args:
            input (Tensor): the input tensor.
            dim (int or tuple of ints): the dimension or dimensions to reduce.

        Keyword args:
            correction (int): difference between the sample size and sample degrees of freedom.
                Defaults to `Bessel's correction`_, ``correction=1``.

                .. versionchanged:: 2.0
                    Previously this argument was called ``unbiased`` and was a boolean
                    with ``True`` corresponding to ``correction=1`` and ``False`` being
                    ``correction=0``.
            keepdim (bool): whether the output tensor has :attr:`dim` retained or not.
            out (Tensor, optional): the output tensor.

        Example:

            >>> a = torch.tensor(
            ...     [[ 0.2035,  1.2959,  1.8101, -0.4644],
            ...      [ 1.5027, -0.3270,  0.5905,  0.6538],
            ...      [-1.5745,  1.3330, -0.5596, -0.6548],
            ...      [ 0.1264, -0.5080,  1.6420,  0.1992]])
            >>> torch.std(a, dim=1, keepdim=True)
            tensor([[1.0311],
                    [0.7477],
                    [1.2204],
                    [0.9087]])

        .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = del_dim[:](input, *dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.std(*auto_generated_args, dim,correction=correction,keepdim=keepdim)
    ref_shape = size_mapping['std'](input_shape, dim, **dict(correction=correction, keepdim=keepdim))
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "std"
    return obj

def norm(input: 'Tensor', p='fro', dim: del_dim[...]=None, keepdim=False, dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.norm'. The automatically generated codes are as follows:
        [ 1]: def norm(input: 'Tensor', p='fro', dim: del_dim[...]=None, keepdim=False, dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     dim = del_dim[...](input, dim)
        [19]:
        [20]:     # Obtain available sized in arguments (which will be fed into size function).
        [21]:     input_shape=None if input is None else Size(input.shape)
        [22]:     # Use the given inner codes if they are provided.
        [23]:     with torch._C.DisableTorchFunction():
        [24]:         for dim_iter, in zip(dim[::-1]):
        [25]:             auto_generated_args = tuple(x for x in [input] if x is not None)
        [26]:             input = torch.norm(*auto_generated_args, p=p,dim=dim_iter,keepdim=keepdim,dtype=dtype)
        [27]:     obj = input
        [28]:     ref_shape = size_mapping['norm'](input_shape, dim, **dict(p=p, dim=dim, keepdim=keepdim, dtype=dtype))
        [29]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [30]:
        [31]:     def update_special(obj, updates):
        [32]:         if isinstance(obj, tuple):
        [33]:             for x in obj: update_special(x, updates)
        [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [36]:
        [37]:     if getattr(obj, 'grad_fn', None) is not None:
        [38]:         obj.grad_fn_name = "norm"
        [39]:     return obj
        [40]:
        The document of the original function is:
        Returns the matrix norm or vector norm of a given tensor.

            .. warning::

                torch.norm is deprecated and may be removed in a future PyTorch release.
                Its documentation and behavior may be incorrect, and it is no longer
                actively maintained.

                Use :func:`torch.linalg.vector_norm` when computing vector norms and
                :func:`torch.linalg.matrix_norm` when computing matrix norms.
                For a function with a similar behavior as this one see :func:`torch.linalg.norm`.
                Note, however, the signature for these functions is slightly different than the
                signature for ``torch.norm``.

            Args:
                input (Tensor): The input tensor. Its data type must be either a floating
                    point or complex type. For complex inputs, the norm is calculated using the
                    absolute value of each element. If the input is complex and neither
                    :attr:`dtype` nor :attr:`out` is specified, the result's data type will
                    be the corresponding floating point type (e.g. float if :attr:`input` is
                    complexfloat).

                p (int, float, inf, -inf, 'fro', 'nuc', optional): the order of norm. Default: ``'fro'``
                    The following norms can be calculated:

                    ======  ==============  ==========================
                    ord     matrix norm     vector norm
                    ======  ==============  ==========================
                    'fro'   Frobenius norm  --
                    'nuc'   nuclear norm    --
                    Number  --              sum(abs(x)**ord)**(1./ord)
                    ======  ==============  ==========================

                    The vector norm can be calculated across any number of dimensions.
                    The corresponding dimensions of :attr:`input` are flattened into
                    one dimension, and the norm is calculated on the flattened
                    dimension.

                    Frobenius norm produces the same result as ``p=2`` in all cases
                    except when :attr:`dim` is a list of three or more dims, in which
                    case Frobenius norm throws an error.

                    Nuclear norm can only be calculated across exactly two dimensions.

                dim (int, tuple of ints, list of ints, optional):
                    Specifies which dimension or dimensions of :attr:`input` to
                    calculate the norm across. If :attr:`dim` is ``None``, the norm will
                    be calculated across all dimensions of :attr:`input`. If the norm
                    type indicated by :attr:`p` does not support the specified number of
                    dimensions, an error will occur.
                keepdim (bool, optional): whether the output tensors have :attr:`dim`
                    retained or not. Ignored if :attr:`dim` = ``None`` and
                    :attr:`out` = ``None``. Default: ``False``
                out (Tensor, optional): the output tensor. Ignored if
                    :attr:`dim` = ``None`` and :attr:`out` = ``None``.
                dtype (:class:`torch.dtype`, optional): the desired data type of
                    returned tensor. If specified, the input tensor is casted to
                    :attr:`dtype` while performing the operation. Default: None.

            .. note::
                Even though ``p='fro'`` supports any number of dimensions, the true
                mathematical definition of Frobenius norm only applies to tensors with
                exactly two dimensions. :func:`torch.linalg.matrix_norm` with ``ord='fro'``
                aligns with the mathematical definition, since it can only be applied across
                exactly two dimensions.

            Example::

                >>> import torch
                >>> a = torch.arange(9, dtype= torch.float) - 4
                >>> b = a.reshape((3, 3))
                >>> torch.norm(a)
                tensor(7.7460)
                >>> torch.norm(b)
                tensor(7.7460)
                >>> torch.norm(a, float('inf'))
                tensor(4.)
                >>> torch.norm(b, float('inf'))
                tensor(4.)
                >>> c = torch.tensor([[ 1, 2, 3], [-1, 1, 4]] , dtype=torch.float)
                >>> torch.norm(c, dim=0)
                tensor([1.4142, 2.2361, 5.0000])
                >>> torch.norm(c, dim=1)
                tensor([3.7417, 4.2426])
                >>> torch.norm(c, p=1, dim=1)
                tensor([6., 6.])
                >>> d = torch.arange(8, dtype=torch.float).reshape(2, 2, 2)
                >>> torch.norm(d, dim=(1, 2))
                tensor([ 3.7417, 11.2250])
                >>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])
                (tensor(3.7417), tensor(11.2250))

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = del_dim[...](input, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        for dim_iter, in zip(dim[::-1]):
            auto_generated_args = tuple(x for x in [input] if x is not None)
            input = torch.norm(*auto_generated_args, p=p,dim=dim_iter,keepdim=keepdim,dtype=dtype)
    obj = input
    ref_shape = size_mapping['norm'](input_shape, dim, **dict(p=p, dim=dim, keepdim=keepdim, dtype=dtype))
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "norm"
    return obj

def cumsum(input: 'Tensor', dim: del_dim[1], dtype=None, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.cumsum'. The automatically generated codes are as follows:
        [ 1]: def cumsum(input: 'Tensor', dim: del_dim[1], dtype=None, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     dim = del_dim[1](input, dim)
        [19]:
        [20]:     # Obtain available sized in arguments (which will be fed into size function).
        [21]:     input_shape=None if input is None else Size(input.shape)
        [22]:     # Use the given inner codes if they are provided.
        [23]:     with torch._C.DisableTorchFunction():
        [24]:         for dim_iter, in zip(dim[::-1]):
        [25]:             auto_generated_args = tuple(x for x in [input,dim_iter] if x is not None)
        [26]:             input = torch.cumsum(*auto_generated_args, dtype=dtype,out=out)
        [27]:     obj = input
        [28]:     ref_shape = size_mapping['cumsum'](input_shape, dim)
        [29]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [30]:
        [31]:     def update_special(obj, updates):
        [32]:         if isinstance(obj, tuple):
        [33]:             for x in obj: update_special(x, updates)
        [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [36]:
        [37]:     if getattr(obj, 'grad_fn', None) is not None:
        [38]:         obj.grad_fn_name = "cumsum"
        [39]:     return obj
        [40]:
        The document of the original function is:

        cumsum(input, dim, *, dtype=None, out=None) -> Tensor

        Returns the cumulative sum of elements of :attr:`input` in the dimension
        :attr:`dim`.

        For example, if :attr:`input` is a vector of size N, the result will also be
        a vector of size N, with elements.

        .. math::
            y_i = x_1 + x_2 + x_3 + \dots + x_i

        Args:
            input (Tensor): the input tensor.
            dim  (int): the dimension to do the operation over

        Keyword args:
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                If specified, the input tensor is casted to :attr:`dtype` before the operation
                is performed. This is useful for preventing data type overflows. Default: None.
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.randn(10)
            >>> a
            tensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,
                     0.1850, -1.1571, -0.4243])
            >>> torch.cumsum(a, dim=0)
            tensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,
                    -1.8209, -2.9780, -3.4022])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = del_dim[1](input, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        for dim_iter, in zip(dim[::-1]):
            auto_generated_args = tuple(x for x in [input,dim_iter] if x is not None)
            input = torch.cumsum(*auto_generated_args, dtype=dtype,out=out)
    obj = input
    ref_shape = size_mapping['cumsum'](input_shape, dim)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "cumsum"
    return obj

def cumprod(input: 'Tensor', dim: del_dim[1], dtype=None, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.cumprod'. The automatically generated codes are as follows:
        [ 1]: def cumprod(input: 'Tensor', dim: del_dim[1], dtype=None, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     dim = del_dim[1](input, dim)
        [19]:
        [20]:     # Obtain available sized in arguments (which will be fed into size function).
        [21]:     input_shape=None if input is None else Size(input.shape)
        [22]:     # Use the given inner codes if they are provided.
        [23]:     with torch._C.DisableTorchFunction():
        [24]:         for dim_iter, in zip(dim[::-1]):
        [25]:             auto_generated_args = tuple(x for x in [input,dim_iter] if x is not None)
        [26]:             input = torch.cumprod(*auto_generated_args, dtype=dtype,out=out)
        [27]:     obj = input
        [28]:     ref_shape = size_mapping['cumprod'](input_shape, dim)
        [29]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [30]:
        [31]:     def update_special(obj, updates):
        [32]:         if isinstance(obj, tuple):
        [33]:             for x in obj: update_special(x, updates)
        [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [36]:
        [37]:     if getattr(obj, 'grad_fn', None) is not None:
        [38]:         obj.grad_fn_name = "cumprod"
        [39]:     return obj
        [40]:
        The document of the original function is:

        cumprod(input, dim, *, dtype=None, out=None) -> Tensor

        Returns the cumulative product of elements of :attr:`input` in the dimension
        :attr:`dim`.

        For example, if :attr:`input` is a vector of size N, the result will also be
        a vector of size N, with elements.

        .. math::
            y_i = x_1 \times x_2\times x_3\times \dots \times x_i

        Args:
            input (Tensor): the input tensor.
            dim  (int): the dimension to do the operation over

        Keyword args:
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                If specified, the input tensor is casted to :attr:`dtype` before the operation
                is performed. This is useful for preventing data type overflows. Default: None.
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.randn(10)
            >>> a
            tensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,
                    -0.2129, -0.4206,  0.1968])
            >>> torch.cumprod(a, dim=0)
            tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,
                     0.0014, -0.0006, -0.0001])

            >>> a[5] = 0.0
            >>> torch.cumprod(a, dim=0)
            tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,
                     0.0000, -0.0000, -0.0000])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = del_dim[1](input, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        for dim_iter, in zip(dim[::-1]):
            auto_generated_args = tuple(x for x in [input,dim_iter] if x is not None)
            input = torch.cumprod(*auto_generated_args, dtype=dtype,out=out)
    obj = input
    ref_shape = size_mapping['cumprod'](input_shape, dim)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "cumprod"
    return obj

def __indexed_reduce__(func_name, self: 'Tensor', *dim, keepdim=None, out=None, ret_index_only=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.__indexed_reduce__'. The automatically generated codes are as follows:
        [ 1]: def __indexed_reduce__(func_name, self: 'Tensor', *dim, keepdim=None, out=None, ret_index_only=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [self]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if self is None: ...
        [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     self_shape=None if self is None else Size(self.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     torch_returned = False
        [22]:     if len(dim) == 1 and isinstance(dim[0], torch.Tensor):
        [23]:         other = dim[0]
        [24]:         subclass = Tensor.get_tensor_subclass(self)
        [25]:         other = other.as_subclass(subclass).special_from(other.shape) if not isinstance(other, subclass) else other
        [26]:         self_shape = Size(self.shape); other_shape = Size(other.shape)
        [27]:         ref_shape, self_shape, other_shape = size_mapping_op[func_name](self_shape, other_shape)
        [28]:         with torch._C.DisableTorchFunction():
        [29]:             obj = Tensor.inherit_from(torch_super(self, func_name)(other, **(dict(out=out) if locals().get('out', None) is not None else {})), self, shape=ref_shape)
        [30]:     else:
        [31]:         dim = del_dim(self, *dim)
        [32]:         indices = None
        [33]:         num_dim = 0
        [34]:         init_shape = self.shape
        [35]:         with torch._C.DisableTorchFunction():
        [36]:             for d in dim[::-1]:
        [37]:                 result = torch_super(self, func_name)(d, **dict(keepdim=keepdim) if keepdim is not None else {})
        [38]:                 self = result.values
        [39]:                 res_indices = Tensor.inherit_from(result.indices, self, shape=[])
        [40]:                 if indices is None:  indices = res_indices.unsqueeze(FuncDim(0))
        [41]:                 elif keepdim or keepdim is None:  indices = cat(res_indices.unsqueeze(FuncDim(0)), indices.gather(d+1, res_indices.duplicate(num_dim, FuncDim(0))), 0)
        [42]:                 else:  indices = cat(res_indices.unsqueeze(FuncDim(0)), indices.gather(d+1, res_indices.unsqueeze(d).duplicate(num_dim, FuncDim(0))).squeeze_(d+1), 0)
        [43]:                 num_dim += 1
        [44]:         if keepdim is False:  cur_shape = remove_dim(init_shape, dim)
        [45]:         else:  cur_shape = init_shape
        [46]:         if ret_index_only:
        [47]:             if indices is None:  return
        [48]:             init_shape_tensor = tensor_to(init_shape, self)
        [49]:             dim_tensor = tensor(dim)
        [50]:             dim_size = cat(init_shape_tensor[dim_tensor][1:].flip().cumprod(0).flip(), tensor_like(ones(1), init_shape_tensor)).with_func_dim(True)
        [51]:             flatten_indices = (dim_size * indices).sum(FuncDim)
        [52]:             indices = indices.special_from(Size(FuncDim(1)) + cur_shape)
        [53]:             flatten_indices = flatten_indices.special_from(cur_shape)
        [54]:             if len(dim) == 1:  indices.squeeze_(0)
        [55]:             flatten_indices.indices = indices
        [56]:             flatten_indices.values = Tensor.inherit_from(self, self, shape=cur_shape)
        [57]:             if out is not None:
        [58]:                 out.zero_().add_(flatten_indices)
        [59]:             obj = flatten_indices
        [60]:         else:
        [61]:             self = Tensor.inherit_from(self, self, shape=cur_shape)
        [62]:             self.indices = indices.special_from(Size(FuncDim(1)) + cur_shape) if indices is not None else None
        [63]:             if len(dim) == 1:  self.indices.squeeze_(0)
        [64]:             # indices_tuple = indices.special_from(cur_shape + (1,)).split(dim=-1, squeezedim=True)
        [65]:             # self.indices = indices_tuple if len(dim) > 1 else indices_tuple[0]
        [66]:             self.values = self
        [67]:             if out is not None:
        [68]:                 if isinstance(out, tuple):
        [69]:                     out[0].zero_().add_(self.values)
        [70]:                     if len(out) > 1:  out[1].zero_().add_(self.indices)
        [71]:                 else:  out.zero_().add_(self.values)
        [72]:             obj = self
        [73]:
        [74]:     def update_special(obj, updates):
        [75]:         if isinstance(obj, tuple):
        [76]:             for x in obj: update_special(x, updates)
        [77]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [78]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [79]:
        [80]:     if getattr(obj, 'grad_fn', None) is not None:
        [81]:         obj.grad_fn_name = "__indexed_reduce__"
        [82]:     return obj
        [83]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [self]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if self is None: ...
    elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
    if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if self.device.type == 'cpu': self = self.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    self_shape=None if self is None else Size(self.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    if len(dim) == 1 and isinstance(dim[0], torch.Tensor):
        other = dim[0]
        subclass = Tensor.get_tensor_subclass(self)
        other = other.as_subclass(subclass).special_from(other.shape) if not isinstance(other, subclass) else other
        self_shape = Size(self.shape); other_shape = Size(other.shape)
        ref_shape, self_shape, other_shape = size_mapping_op[func_name](self_shape, other_shape)
        with torch._C.DisableTorchFunction():
            obj = Tensor.inherit_from(torch_super(self, func_name)(other, **(dict(out=out) if locals().get('out', None) is not None else {})), self, shape=ref_shape)
    else:
        dim = del_dim(self, *dim)
        indices = None
        num_dim = 0
        init_shape = self.shape
        with torch._C.DisableTorchFunction():
            for d in dim[::-1]:
                result = torch_super(self, func_name)(d, **dict(keepdim=keepdim) if keepdim is not None else {})
                self = result.values
                res_indices = Tensor.inherit_from(result.indices, self, shape=[])
                if indices is None:  indices = res_indices.unsqueeze(FuncDim(0))
                elif keepdim or keepdim is None:  indices = cat(res_indices.unsqueeze(FuncDim(0)), indices.gather(d+1, res_indices.duplicate(num_dim, FuncDim(0))), 0)
                else:  indices = cat(res_indices.unsqueeze(FuncDim(0)), indices.gather(d+1, res_indices.unsqueeze(d).duplicate(num_dim, FuncDim(0))).squeeze_(d+1), 0)
                num_dim += 1
        if keepdim is False:  cur_shape = remove_dim(init_shape, dim)
        else:  cur_shape = init_shape
        if ret_index_only:
            if indices is None:  return
            init_shape_tensor = tensor_to(init_shape, self)
            dim_tensor = tensor(dim)
            dim_size = cat(init_shape_tensor[dim_tensor][1:].flip().cumprod(0).flip(), tensor_like(ones(1), init_shape_tensor)).with_func_dim(True)
            flatten_indices = (dim_size * indices).sum(FuncDim)
            indices = indices.special_from(Size(FuncDim(1)) + cur_shape)
            flatten_indices = flatten_indices.special_from(cur_shape)
            if len(dim) == 1:  indices.squeeze_(0)
            flatten_indices.indices = indices
            flatten_indices.values = Tensor.inherit_from(self, self, shape=cur_shape)
            if out is not None:
                out.zero_().add_(flatten_indices)
            obj = flatten_indices
        else:
            self = Tensor.inherit_from(self, self, shape=cur_shape)
            self.indices = indices.special_from(Size(FuncDim(1)) + cur_shape) if indices is not None else None
            if len(dim) == 1:  self.indices.squeeze_(0)
            # indices_tuple = indices.special_from(cur_shape + (1,)).split(dim=-1, squeezedim=True)
            # self.indices = indices_tuple if len(dim) > 1 else indices_tuple[0]
            self.values = self
            if out is not None:
                if isinstance(out, tuple):
                    out[0].zero_().add_(self.values)
                    if len(out) > 1:  out[1].zero_().add_(self.indices)
                else:  out.zero_().add_(self.values)
            obj = self

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "__indexed_reduce__"
    return obj

def min(input: 'Tensor', *dim, keepdim=False, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.min'. The automatically generated codes are as follows:
        [ 1]: def min(input: 'Tensor', *dim, keepdim=False, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     torch_returned = False
        [22]:     obj = __indexed_reduce__('min', input, *dim, keepdim=keepdim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from
        [23]:
        [24]:     def update_special(obj, updates):
        [25]:         if isinstance(obj, tuple):
        [26]:             for x in obj: update_special(x, updates)
        [27]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [28]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [29]:
        [30]:     if getattr(obj, 'grad_fn', None) is not None:
        [31]:         obj.grad_fn_name = "min"
        [32]:     return obj
        [33]:
        The document of the original function is:

        min(input) -> Tensor

        Returns the minimum value of all elements in the :attr:`input` tensor.

        .. warning::
            This function produces deterministic (sub)gradients unlike ``min(dim=0)``

        Args:
            input (Tensor): the input tensor.

        Example::

            >>> a = torch.randn(1, 3)
            >>> a
            tensor([[ 0.6750,  1.0857,  1.7197]])
            >>> torch.min(a)
            tensor(0.6750)

        .. function:: min(input, dim, keepdim=False, *, out=None) -> (Tensor, LongTensor)
           :noindex:

        Returns a namedtuple ``(values, indices)`` where ``values`` is the minimum
        value of each row of the :attr:`input` tensor in the given dimension
        :attr:`dim`. And ``indices`` is the index location of each minimum value found
        (argmin).

        If :attr:`keepdim` is ``True``, the output tensors are of the same size as
        :attr:`input` except in the dimension :attr:`dim` where they are of size 1.
        Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in
        the output tensors having 1 fewer dimension than :attr:`input`.

        .. note:: If there are multiple minimal values in a reduced row then
                  the indices of the first minimal value are returned.

        Args:
            input (Tensor): the input tensor.
            dim (int): the dimension to reduce.
            keepdim (bool): whether the output tensor has :attr:`dim` retained or not.

        Keyword args:
            out (tuple, optional): the tuple of two output tensors (min, min_indices)

        Example::

            >>> a = torch.randn(4, 4)
            >>> a
            tensor([[-0.6248,  1.1334, -1.1899, -0.2803],
                    [-1.4644, -0.2635, -0.3651,  0.6134],
                    [ 0.2457,  0.0384,  1.0128,  0.7015],
                    [-0.1153,  2.9849,  2.1458,  0.5788]])
            >>> torch.min(a, 1)
            torch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))

        .. function:: min(input, other, *, out=None) -> Tensor
           :noindex:

        See :func:`torch.minimum`.

        which is:

        minimum(input, other, *, out=None) -> Tensor

        Computes the element-wise minimum of :attr:`input` and :attr:`other`.

        .. note::
            If one of the elements being compared is a NaN, then that element is returned.
            :func:`minimum` is not supported for tensors with complex dtypes.

        Args:
            input (Tensor): the input tensor.
            other (Tensor): the second input tensor

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.tensor((1, 2, -1))
            >>> b = torch.tensor((3, 0, 4))
            >>> torch.minimum(a, b)
            tensor([1, 0, -1])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    obj = __indexed_reduce__('min', input, *dim, keepdim=keepdim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "min"
    return obj

def max(input: 'Tensor', *dim, keepdim=False, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.max'. The automatically generated codes are as follows:
        [ 1]: def max(input: 'Tensor', *dim, keepdim=False, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     torch_returned = False
        [22]:     obj = __indexed_reduce__('max', input, *dim, keepdim=keepdim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from
        [23]:
        [24]:     def update_special(obj, updates):
        [25]:         if isinstance(obj, tuple):
        [26]:             for x in obj: update_special(x, updates)
        [27]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [28]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [29]:
        [30]:     if getattr(obj, 'grad_fn', None) is not None:
        [31]:         obj.grad_fn_name = "max"
        [32]:     return obj
        [33]:
        The document of the original function is:

        max(input) -> Tensor

        Returns the maximum value of all elements in the ``input`` tensor.

        .. warning::
            This function produces deterministic (sub)gradients unlike ``max(dim=0)``

        Args:
            input (Tensor): the input tensor.

        Example::

            >>> a = torch.randn(1, 3)
            >>> a
            tensor([[ 0.6763,  0.7445, -2.2369]])
            >>> torch.max(a)
            tensor(0.7445)

        .. function:: max(input, dim, keepdim=False, *, out=None) -> (Tensor, LongTensor)
           :noindex:

        Returns a namedtuple ``(values, indices)`` where ``values`` is the maximum
        value of each row of the :attr:`input` tensor in the given dimension
        :attr:`dim`. And ``indices`` is the index location of each maximum value found
        (argmax).

        If ``keepdim`` is ``True``, the output tensors are of the same size
        as ``input`` except in the dimension ``dim`` where they are of size 1.
        Otherwise, ``dim`` is squeezed (see :func:`torch.squeeze`), resulting
        in the output tensors having 1 fewer dimension than ``input``.

        .. note:: If there are multiple maximal values in a reduced row then
                  the indices of the first maximal value are returned.

        Args:
            input (Tensor): the input tensor.
            dim (int): the dimension to reduce.
            keepdim (bool): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.

        Keyword args:
            out (tuple, optional): the result tuple of two output tensors (max, max_indices)

        Example::

            >>> a = torch.randn(4, 4)
            >>> a
            tensor([[-1.2360, -0.2942, -0.1222,  0.8475],
                    [ 1.1949, -1.1127, -2.2379, -0.6702],
                    [ 1.5717, -0.9207,  0.1297, -1.8768],
                    [-0.6172,  1.0036, -0.6060, -0.2432]])
            >>> torch.max(a, 1)
            torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))

        .. function:: max(input, other, *, out=None) -> Tensor
           :noindex:

        See :func:`torch.maximum`.

        which is:

        maximum(input, other, *, out=None) -> Tensor

        Computes the element-wise maximum of :attr:`input` and :attr:`other`.

        .. note::
            If one of the elements being compared is a NaN, then that element is returned.
            :func:`maximum` is not supported for tensors with complex dtypes.

        Args:
            input (Tensor): the input tensor.
            other (Tensor): the second input tensor

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> a = torch.tensor((1, 2, -1))
            >>> b = torch.tensor((3, 0, 4))
            >>> torch.maximum(a, b)
            tensor([3, 2, 4])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    obj = __indexed_reduce__('max', input, *dim, keepdim=keepdim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "max"
    return obj

def median(input: 'Tensor', *dim, keepdim=False, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.median'. The automatically generated codes are as follows:
        [ 1]: def median(input: 'Tensor', *dim, keepdim=False, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     torch_returned = False
        [22]:     obj = __indexed_reduce__('median', input, *dim, keepdim=keepdim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from
        [23]:
        [24]:     def update_special(obj, updates):
        [25]:         if isinstance(obj, tuple):
        [26]:             for x in obj: update_special(x, updates)
        [27]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [28]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [29]:
        [30]:     if getattr(obj, 'grad_fn', None) is not None:
        [31]:         obj.grad_fn_name = "median"
        [32]:     return obj
        [33]:
        The document of the original function is:

        median(input) -> Tensor

        Returns the median of the values in :attr:`input`.

        .. note::
            The median is not unique for :attr:`input` tensors with an even number
            of elements. In this case the lower of the two medians is returned. To
            compute the mean of both medians, use :func:`torch.quantile` with ``q=0.5`` instead.

        .. warning::
            This function produces deterministic (sub)gradients unlike ``median(dim=0)``

        Args:
            input (Tensor): the input tensor.

        Example::

            >>> a = torch.randn(1, 3)
            >>> a
            tensor([[ 1.5219, -1.5212,  0.2202]])
            >>> torch.median(a)
            tensor(0.2202)

        .. function:: median(input, dim=-1, keepdim=False, *, out=None) -> (Tensor, LongTensor)
           :noindex:

        Returns a namedtuple ``(values, indices)`` where ``values`` contains the median of each row of :attr:`input`
        in the dimension :attr:`dim`, and ``indices`` contains the index of the median values found in the dimension :attr:`dim`.

        By default, :attr:`dim` is the last dimension of the :attr:`input` tensor.

        If :attr:`keepdim` is ``True``, the output tensors are of the same size
        as :attr:`input` except in the dimension :attr:`dim` where they are of size 1.
        Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in
        the outputs tensor having 1 fewer dimension than :attr:`input`.

        .. note::
            The median is not unique for :attr:`input` tensors with an even number
            of elements in the dimension :attr:`dim`. In this case the lower of the
            two medians is returned. To compute the mean of both medians in
            :attr:`input`, use :func:`torch.quantile` with ``q=0.5`` instead.

        .. warning::
            ``indices`` does not necessarily contain the first occurrence of each
            median value found, unless it is unique.
            The exact implementation details are device-specific.
            Do not expect the same result when run on CPU and GPU in general.
            For the same reason do not expect the gradients to be deterministic.

        Args:
            input (Tensor): the input tensor.
            dim (int): the dimension to reduce.
            keepdim (bool): whether the output tensor has :attr:`dim` retained or not.

        Keyword args:
            out ((Tensor, Tensor), optional): The first tensor will be populated with the median values and the second
                                              tensor, which must have dtype long, with their indices in the dimension
                                              :attr:`dim` of :attr:`input`.

        Example::

            >>> a = torch.randn(4, 5)
            >>> a
            tensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],
                    [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],
                    [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],
                    [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])
            >>> torch.median(a, 1)
            torch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    obj = __indexed_reduce__('median', input, *dim, keepdim=keepdim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "median"
    return obj

def cummin(input: 'Tensor', dim: exist_dim[1]=None, *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.cummin'. The automatically generated codes are as follows:
        [ 1]: def cummin(input: 'Tensor', dim: exist_dim[1]=None, *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     dim = exist_dim[1](input, dim)
        [19]:
        [20]:     # Obtain available sized in arguments (which will be fed into size function).
        [21]:     input_shape=None if input is None else Size(input.shape)
        [22]:     # Use the given inner codes if they are provided.
        [23]:     torch_returned = False
        [24]:     obj = __indexed_reduce__('cummin', input, *dim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from
        [25]:
        [26]:     def update_special(obj, updates):
        [27]:         if isinstance(obj, tuple):
        [28]:             for x in obj: update_special(x, updates)
        [29]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [30]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [31]:
        [32]:     if getattr(obj, 'grad_fn', None) is not None:
        [33]:         obj.grad_fn_name = "cummin"
        [34]:     return obj
        [35]:
        The document of the original function is:

        cummin(input, dim, *, out=None) -> (Tensor, LongTensor)
        Returns a namedtuple ``(values, indices)`` where ``values`` is the cumulative minimum of
        elements of :attr:`input` in the dimension :attr:`dim`. And ``indices`` is the index
        location of each maximum value found in the dimension :attr:`dim`.

        .. math::
            y_i = min(x_1, x_2, x_3, \dots, x_i)

        Args:
            input (Tensor): the input tensor.
            dim  (int): the dimension to do the operation over

        Keyword args:
            out (tuple, optional): the result tuple of two output tensors (values, indices)

        Example::

            >>> a = torch.randn(10)
            >>> a
            tensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220, -0.3885,  1.1762,
                 0.9165,  1.6684])
            >>> torch.cummin(a, dim=0)
            torch.return_types.cummin(
                values=tensor([-0.2284, -0.6628, -0.6628, -0.6628, -1.3298, -1.3298, -1.3298, -1.3298,
                -1.3298, -1.3298]),
                indices=tensor([0, 1, 1, 1, 4, 4, 4, 4, 4, 4]))

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = exist_dim[1](input, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    obj = __indexed_reduce__('cummin', input, *dim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "cummin"
    return obj

def cummax(input: 'Tensor', dim: exist_dim[1]=None, *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.cummax'. The automatically generated codes are as follows:
        [ 1]: def cummax(input: 'Tensor', dim: exist_dim[1]=None, *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     dim = exist_dim[1](input, dim)
        [19]:
        [20]:     # Obtain available sized in arguments (which will be fed into size function).
        [21]:     input_shape=None if input is None else Size(input.shape)
        [22]:     # Use the given inner codes if they are provided.
        [23]:     torch_returned = False
        [24]:     obj = __indexed_reduce__('cummax', input, *dim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from
        [25]:
        [26]:     def update_special(obj, updates):
        [27]:         if isinstance(obj, tuple):
        [28]:             for x in obj: update_special(x, updates)
        [29]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [30]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [31]:
        [32]:     if getattr(obj, 'grad_fn', None) is not None:
        [33]:         obj.grad_fn_name = "cummax"
        [34]:     return obj
        [35]:
        The document of the original function is:

        cummax(input, dim, *, out=None) -> (Tensor, LongTensor)
        Returns a namedtuple ``(values, indices)`` where ``values`` is the cumulative maximum of
        elements of :attr:`input` in the dimension :attr:`dim`. And ``indices`` is the index
        location of each maximum value found in the dimension :attr:`dim`.

        .. math::
            y_i = max(x_1, x_2, x_3, \dots, x_i)

        Args:
            input (Tensor): the input tensor.
            dim  (int): the dimension to do the operation over

        Keyword args:
            out (tuple, optional): the result tuple of two output tensors (values, indices)

        Example::

            >>> a = torch.randn(10)
            >>> a
            tensor([-0.3449, -1.5447,  0.0685, -1.5104, -1.1706,  0.2259,  1.4696, -1.3284,
                 1.9946, -0.8209])
            >>> torch.cummax(a, dim=0)
            torch.return_types.cummax(
                values=tensor([-0.3449, -0.3449,  0.0685,  0.0685,  0.0685,  0.2259,  1.4696,  1.4696,
                 1.9946,  1.9946]),
                indices=tensor([0, 0, 2, 2, 2, 5, 6, 6, 8, 8]))

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = exist_dim[1](input, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    obj = __indexed_reduce__('cummax', input, *dim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "cummax"
    return obj

def argmin(input: 'Tensor', *dim, keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.argmin'. The automatically generated codes are as follows:
        [ 1]: def argmin(input: 'Tensor', *dim, keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     torch_returned = False
        [22]:     obj = __indexed_reduce__('min', input, *dim, keepdim=keepdim, ret_index_only=True) # suppress:  special_from
        [23]:
        [24]:     def update_special(obj, updates):
        [25]:         if isinstance(obj, tuple):
        [26]:             for x in obj: update_special(x, updates)
        [27]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [28]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [29]:
        [30]:     if getattr(obj, 'grad_fn', None) is not None:
        [31]:         obj.grad_fn_name = "argmin"
        [32]:     return obj
        [33]:
        The document of the original function is:

        argmin(input, dim=None, keepdim=False) -> LongTensor

        Returns the indices of the minimum value(s) of the flattened tensor or along a dimension

        This is the second value returned by :meth:`torch.min`. See its
        documentation for the exact semantics of this method.

        .. note:: If there are multiple minimal values then the indices of the first minimal value are returned.

        Args:
            input (Tensor): the input tensor.
            dim (int): the dimension to reduce. If ``None``, the argmin of the flattened input is returned.
            keepdim (bool): whether the output tensor has :attr:`dim` retained or not..

        Example::

            >>> a = torch.randn(4, 4)
            >>> a
            tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],
                    [ 1.0100, -1.1975, -0.0102, -0.4732],
                    [-0.9240,  0.1207, -0.7506, -1.0213],
                    [ 1.7809, -1.2960,  0.9384,  0.1438]])
            >>> torch.argmin(a)
            tensor(13)
            >>> torch.argmin(a, dim=1)
            tensor([ 2,  1,  3,  1])
            >>> torch.argmin(a, dim=1, keepdim=True)
            tensor([[2],
                    [1],
                    [3],
                    [1]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    obj = __indexed_reduce__('min', input, *dim, keepdim=keepdim, ret_index_only=True) # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "argmin"
    return obj

def argmax(input: 'Tensor', *dim, keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.argmax'. The automatically generated codes are as follows:
        [ 1]: def argmax(input: 'Tensor', *dim, keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     input_shape=None if input is None else Size(input.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     torch_returned = False
        [22]:     obj = __indexed_reduce__('max', input, *dim, keepdim=keepdim, ret_index_only=True) # suppress:  special_from
        [23]:
        [24]:     def update_special(obj, updates):
        [25]:         if isinstance(obj, tuple):
        [26]:             for x in obj: update_special(x, updates)
        [27]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [28]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [29]:
        [30]:     if getattr(obj, 'grad_fn', None) is not None:
        [31]:         obj.grad_fn_name = "argmax"
        [32]:     return obj
        [33]:
        The document of the original function is:

        argmax(input) -> LongTensor

        Returns the indices of the maximum value of all elements in the :attr:`input` tensor.

        This is the second value returned by :meth:`torch.max`. See its
        documentation for the exact semantics of this method.

        .. note:: If there are multiple maximal values then the indices of the first maximal value are returned.

        Args:
            input (Tensor): the input tensor.

        Example::

            >>> a = torch.randn(4, 4)
            >>> a
            tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
                    [-0.7401, -0.8805, -0.3402, -1.1936],
                    [ 0.4907, -1.3948, -1.0691, -0.3132],
                    [-1.6092,  0.5419, -0.2993,  0.3195]])
            >>> torch.argmax(a)
            tensor(0)

        .. function:: argmax(input, dim, keepdim=False) -> LongTensor
           :noindex:

        Returns the indices of the maximum values of a tensor across a dimension.

        This is the second value returned by :meth:`torch.max`. See its
        documentation for the exact semantics of this method.

        Args:
            input (Tensor): the input tensor.
            dim (int): the dimension to reduce. If ``None``, the argmax of the flattened input is returned.
            keepdim (bool): whether the output tensor has :attr:`dim` retained or not. Ignored if ``dim=None``.

        Example::

            >>> a = torch.randn(4, 4)
            >>> a
            tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
                    [-0.7401, -0.8805, -0.3402, -1.1936],
                    [ 0.4907, -1.3948, -1.0691, -0.3132],
                    [-1.6092,  0.5419, -0.2993,  0.3195]])
            >>> torch.argmax(a, dim=1)
            tensor([ 0,  2,  0,  1])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    obj = __indexed_reduce__('max', input, *dim, keepdim=keepdim, ret_index_only=True) # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "argmax"
    return obj

# slicing functions
def split(self, split_size: int=1, dim: exist_dim[1] = {}, squeezedim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    split(self, split_size: int=1, dim: exist_dim = {}) -> Tensor
    Split a tensor into a tuple of tensors, along `dim`, with split_size for each tensor.

    Args:
        split_size (int or list, optional): The split size for each tensor, using a list of integers adding up to size to split the dimension accordingly. Defaults to 1.
        dim (int/exist_dim, optional): The dimension to split along. Defaults to batch.

    Automatically inheritted method from 'torch.split'. The automatically generated codes are as follows:
        [ 1]: def split(self, split_size: int=1, dim: exist_dim[1] = {}, squeezedim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:     split(self, split_size: int=1, dim: exist_dim = {}) -> Tensor
        [ 4]:     Split a tensor into a tuple of tensors, along `dim`, with split_size for each tensor.
        [ 5]:
        [ 6]:     Args:
        [ 7]:         split_size (int or list, optional): The split size for each tensor, using a list of integers adding up to size to split the dimension accordingly. Defaults to 1.
        [ 8]:         dim (int/exist_dim, optional): The dimension to split along. Defaults to batch.
        [ 9]:     '''
        [10]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [11]:
        [12]:     pivot = None
        [13]:     for t in []:
        [14]:         if isinstance(t, torch.Tensor): pivot = t; break
        [15]:     subclass = Tensor.get_tensor_subclass(pivot)
        [16]:
        [17]:     dim = exist_dim[1](self, dim)
        [18]:
        [19]:     # Obtain available sized in arguments (which will be fed into size function).
        [20]:
        [21]:     # Use the given inner codes if they are provided.
        [22]:     torch_returned = False
        [23]:     dim = dim[0]
        [24]:     with torch._C.DisableTorchFunction():
        [25]:         if squeezedim:
        [26]:             avouch(split_size == 1 or all_(s == 1 for s in split_size), TypeError("Keyword argument 'squeezedim' is only active for 'split_size=1' in bt.Tensor.split. "))
        [27]:             obj = tuple(Tensor.inherit_from(x, self).squeeze_(dim) for x in torch_super(self, 'split')(split_size, dim))
        [28]:         else: torch_returned = True; obj = tuple(Tensor.inherit_from(x, self) for x in torch_super(self, 'split')(split_size, dim))
        [29]:
        [30]:     def update_special(obj, updates):
        [31]:         if isinstance(obj, tuple):
        [32]:             for x in obj: update_special(x, updates)
        [33]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [34]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [35]:
        [36]:     if getattr(obj, 'grad_fn', None) is not None:
        [37]:         obj.grad_fn_name = "split"
        [38]:     return obj
        [39]:
        The document of the original function is:
        Splits the tensor into chunks. Each chunk is a view of the original tensor.

            If :attr:`split_size_or_sections` is an integer type, then :attr:`tensor` will
            be split into equally sized chunks (if possible). Last chunk will be smaller if
            the tensor size along the given dimension :attr:`dim` is not divisible by
            :attr:`split_size`.

            If :attr:`split_size_or_sections` is a list, then :attr:`tensor` will be split
            into ``len(split_size_or_sections)`` chunks with sizes in :attr:`dim` according
            to :attr:`split_size_or_sections`.

            Args:
                tensor (Tensor): tensor to split.
                split_size_or_sections (int) or (list(int)): size of a single chunk or
                    list of sizes for each chunk
                dim (int): dimension along which to split the tensor.

            Example::

                >>> a = torch.arange(10).reshape(5, 2)
                >>> a
                tensor([[0, 1],
                        [2, 3],
                        [4, 5],
                        [6, 7],
                        [8, 9]])
                >>> torch.split(a, 2)
                (tensor([[0, 1],
                         [2, 3]]),
                 tensor([[4, 5],
                         [6, 7]]),
                 tensor([[8, 9]]))
                >>> torch.split(a, [1, 4])
                (tensor([[0, 1]]),
                 tensor([[2, 3],
                         [4, 5],
                         [6, 7],
                         [8, 9]]))

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    dim = exist_dim[1](self, dim)

    # Obtain available sized in arguments (which will be fed into size function).

    # Use the given inner codes if they are provided.
    torch_returned = False
    dim = dim[0]
    with torch._C.DisableTorchFunction():
        if squeezedim:
            avouch(split_size == 1 or all_(s == 1 for s in split_size), TypeError("Keyword argument 'squeezedim' is only active for 'split_size=1' in bt.Tensor.split. "))
            obj = tuple(Tensor.inherit_from(x, self).squeeze_(dim) for x in torch_super(self, 'split')(split_size, dim))
        else: torch_returned = True; obj = tuple(Tensor.inherit_from(x, self) for x in torch_super(self, 'split')(split_size, dim))

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "split"
    return obj

def sample(self, number: int = 1, dim: exist_dim = {}, random: bool = True, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    sample(self, numbder: int = 1, dim: int = self.batch_dimension, random: bool = True) -> Tensor

    Sample `number` of slices from a given dimension.

    Args:
        number (int, optional): the number of slices. Defaults to `1`.
        dim (int/exist_dim, keyword argument): the dimension to slice or select. Defaults to batch dimension.
        random (bool, optional): whether to randomly pick the slices or not. Defaults to True.

    Note:
        Using `sample(num, dim)` for data of size (n_1, n_2, ..., n_r) would result in
            a tensor of size (n_1, n_2, ..., n_{dim-1}, num, n_{dim+1}, ..., n_r)

    Examples::
        >>> data.shape
        batorch.Size([4, 3], 5, 6)
        >>> data.sample(1, 2, random=False).shape
        batorch.Size([4, 3], 1, 6)
        >>> # The above is equivalant to data[:, :, :1, ...].shape.
        >>> data.sample(7, [], random=False).shape
        batorch.Size(7, 5, 6)
        >>> # The above is equivalant to data.flatten(0, 1)[:7].shape.

    Automatically inheritted method from 'torch.sample'. The automatically generated codes are as follows:
        [ 1]: def sample(self, number: int = 1, dim: exist_dim = {}, random: bool = True, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:     sample(self, numbder: int = 1, dim: int = self.batch_dimension, random: bool = True) -> Tensor
        [ 4]:
        [ 5]:     Sample `number` of slices from a given dimension.
        [ 6]:
        [ 7]:     Args:
        [ 8]:         number (int, optional): the number of slices. Defaults to `1`.
        [ 9]:         dim (int/exist_dim, keyword argument): the dimension to slice or select. Defaults to batch dimension.
        [10]:         random (bool, optional): whether to randomly pick the slices or not. Defaults to True.
        [11]:
        [12]:     Note:
        [13]:         Using `sample(num, dim)` for data of size (n_1, n_2, ..., n_r) would result in
        [14]:             a tensor of size (n_1, n_2, ..., n_{dim-1}, num, n_{dim+1}, ..., n_r)
        [15]:
        [16]:     Examples::
        [17]:         >>> data.shape
        [18]:         batorch.Size([4, 3], 5, 6)
        [19]:         >>> data.sample(1, 2, random=False).shape
        [20]:         batorch.Size([4, 3], 1, 6)
        [21]:         >>> # The above is equivalant to data[:, :, :1, ...].shape.
        [22]:         >>> data.sample(7, [], random=False).shape
        [23]:         batorch.Size(7, 5, 6)
        [24]:         >>> # The above is equivalant to data.flatten(0, 1)[:7].shape.
        [25]:     '''
        [26]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [27]:
        [28]:     pivot = None
        [29]:     for t in []:
        [30]:         if isinstance(t, torch.Tensor): pivot = t; break
        [31]:     subclass = Tensor.get_tensor_subclass(pivot)
        [32]:
        [33]:     dim = exist_dim(self, dim)
        [34]:
        [35]:     # Obtain available sized in arguments (which will be fed into size function).
        [36]:
        [37]:     # Use the given inner codes if they are provided.
        [38]:     torch_returned = False
        [39]:     if len(dim) > 1:  self = self.merge_dims(*dim, target=dim[0])
        [40]:     sample_indices = [slice(None)] * self.n_dim
        [41]:     dim = dim[0]
        [42]:     if random:
        [43]:         import random
        [44]:         n_total = self.size(dim)
        [45]:         n_round = number // n_total
        [46]:         n_remain = number % n_total
        [47]:         samples = cat(randperm({n_round}, n_total, device=self.device).flatten().view(-1), tensor(random.sample(range_(n_total), k = n_remain), device=self.device), 0)
        [48]:     else:
        [49]:         avouch(number <= self.size(dim), TypeError(f"Too many elements needed to be sampled from dimension {dim}"))
        [50]:         samples = tensor(range_(number))
        [51]:     sample_indices[dim] = samples.special_from(self.shape[dim:dim+1])
        [52]:     obj = self[tuple(sample_indices)] # suppress:  special_from
        [53]:
        [54]:     def update_special(obj, updates):
        [55]:         if isinstance(obj, tuple):
        [56]:             for x in obj: update_special(x, updates)
        [57]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [58]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [59]:
        [60]:     if getattr(obj, 'grad_fn', None) is not None:
        [61]:         obj.grad_fn_name = "sample"
        [62]:     return obj
        [63]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    dim = exist_dim(self, dim)

    # Obtain available sized in arguments (which will be fed into size function).

    # Use the given inner codes if they are provided.
    torch_returned = False
    if len(dim) > 1:  self = self.merge_dims(*dim, target=dim[0])
    sample_indices = [slice(None)] * self.n_dim
    dim = dim[0]
    if random:
        import random
        n_total = self.size(dim)
        n_round = number // n_total
        n_remain = number % n_total
        samples = cat(randperm({n_round}, n_total, device=self.device).flatten().view(-1), tensor(random.sample(range_(n_total), k = n_remain), device=self.device), 0)
    else:
        avouch(number <= self.size(dim), TypeError(f"Too many elements needed to be sampled from dimension {dim}"))
        samples = tensor(range_(number))
    sample_indices[dim] = samples.special_from(self.shape[dim:dim+1])
    obj = self[tuple(sample_indices)] # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "sample"
    return obj

def pick(self, index: int = None, dim: exist_dim = {}, random: bool = False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    pick(self, index: int = 0, dim: int = self.batch_dimension, random: bool = False) -> Tensor

    Pick one slice on dimension `dim` for big tensors.

    Args:
        index (int, optional): the slice index to pick. Defaults to `None`.
        dim (int/exist_dim, keyword argument): the dimension to slice or select. Defaults to batch dimension.
        random (bool, optional): whether to randomly pick the slice or not. Defaults to False.

    Note:
        Using `pick(index, dim)` for data of size (n_1, n_2, ..., n_r) would result in
            a tensor of size (n_1, n_2, ..., n_{dim-1}, n_{dim+1}, ..., n_r)

    Examples::
        >>> data.shape
        batorch.Size(4, 3, 5, 6)
        >>> data.pick(-1, 2, random=False).shape
        batorch.Size(4, 3, 6)
        >>> # The above is equivalant to data[:, :, 4, ...].shape.

    Automatically inheritted method from 'torch.pick'. The automatically generated codes are as follows:
        [ 1]: def pick(self, index: int = None, dim: exist_dim = {}, random: bool = False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:     pick(self, index: int = 0, dim: int = self.batch_dimension, random: bool = False) -> Tensor
        [ 4]:
        [ 5]:     Pick one slice on dimension `dim` for big tensors.
        [ 6]:
        [ 7]:     Args:
        [ 8]:         index (int, optional): the slice index to pick. Defaults to `None`.
        [ 9]:         dim (int/exist_dim, keyword argument): the dimension to slice or select. Defaults to batch dimension.
        [10]:         random (bool, optional): whether to randomly pick the slice or not. Defaults to False.
        [11]:
        [12]:     Note:
        [13]:         Using `pick(index, dim)` for data of size (n_1, n_2, ..., n_r) would result in
        [14]:             a tensor of size (n_1, n_2, ..., n_{dim-1}, n_{dim+1}, ..., n_r)
        [15]:
        [16]:     Examples::
        [17]:         >>> data.shape
        [18]:         batorch.Size(4, 3, 5, 6)
        [19]:         >>> data.pick(-1, 2, random=False).shape
        [20]:         batorch.Size(4, 3, 6)
        [21]:         >>> # The above is equivalant to data[:, :, 4, ...].shape.
        [22]:     '''
        [23]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [24]:
        [25]:     pivot = None
        [26]:     for t in []:
        [27]:         if isinstance(t, torch.Tensor): pivot = t; break
        [28]:     subclass = Tensor.get_tensor_subclass(pivot)
        [29]:
        [30]:     dim = exist_dim(self, dim)
        [31]:
        [32]:     # Obtain available sized in arguments (which will be fed into size function).
        [33]:
        [34]:     # Use the given inner codes if they are provided.
        [35]:     torch_returned = False
        [36]:     if len(dim) > 1:  self = self.merge_dims(*dim, target=dim[0])
        [37]:     dim = dim[0]
        [38]:     if random:
        [39]:         avouch(index is None, "'index' should be None if random pick is enabled. Use keyword argument 'dim=xx' to identify the dimension.")
        [40]:         import random
        [41]:         index = random.randint(0, self.size(dim)-1)
        [42]:     else:
        [43]:         avouch(isinstance(index, int_) and -self.size(dim) <= index < self.size(dim) or isinstance(index, (slice, Tensor)),
        [44]:                TypeError(f"Invalid index for picking from dimension {dim} of size {self.size(dim)}: {index} ({index.__class__}). "))
        [45]:     obj = self[(slice(None),) * dim + (index,)] # suppress:  special_from
        [46]:
        [47]:     def update_special(obj, updates):
        [48]:         if isinstance(obj, tuple):
        [49]:             for x in obj: update_special(x, updates)
        [50]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [51]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [52]:
        [53]:     if getattr(obj, 'grad_fn', None) is not None:
        [54]:         obj.grad_fn_name = "pick"
        [55]:     return obj
        [56]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    dim = exist_dim(self, dim)

    # Obtain available sized in arguments (which will be fed into size function).

    # Use the given inner codes if they are provided.
    torch_returned = False
    if len(dim) > 1:  self = self.merge_dims(*dim, target=dim[0])
    dim = dim[0]
    if random:
        avouch(index is None, "'index' should be None if random pick is enabled. Use keyword argument 'dim=xx' to identify the dimension.")
        import random
        index = random.randint(0, self.size(dim)-1)
    else:
        avouch(isinstance(index, int_) and -self.size(dim) <= index < self.size(dim) or isinstance(index, (slice, Tensor)),
               TypeError(f"Invalid index for picking from dimension {dim} of size {self.size(dim)}: {index} ({index.__class__}). "))
    obj = self[(slice(None),) * dim + (index,)] # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "pick"
    return obj

def eig(input: 'Tensor', dim: linalg_dim[2]=None, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    Find eigen values and vectors for matrix `input`: (for the first available condition):
    (1) in feature dimensions if more than 2D is available;
    (2) in space dimensions if more than 2D is available;
    (3) in sequence dimensions if more than 2D is available.

    Automatically inheritted method from 'torch.eig'. The automatically generated codes are as follows:
        [ 1]: def eig(input: 'Tensor', dim: linalg_dim[2]=None, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:     Find eigen values and vectors for matrix `input`: (for the first available condition):
        [ 4]:     (1) in feature dimensions if more than 2D is available;
        [ 5]:     (2) in space dimensions if more than 2D is available;
        [ 6]:     (3) in sequence dimensions if more than 2D is available.
        [ 7]:     '''
        [ 8]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 9]:
        [10]:     pivot = None
        [11]:     for t in [input]:
        [12]:         if isinstance(t, torch.Tensor): pivot = t; break
        [13]:     subclass = Tensor.get_tensor_subclass(pivot)
        [14]:
        [15]:     if input is None: ...
        [16]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [17]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [18]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [19]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [20]:
        [21]:     dim = linalg_dim[2](input, dim)
        [22]:
        [23]:     # Obtain available sized in arguments (which will be fed into size function).
        [24]:     input_shape=None if input is None else Size(input.shape)
        [25]:     # Use the given inner codes if they are provided.
        [26]:     torch_returned = False
        [27]:     has_batch = False
        [28]:     with input.hide_special(), torch._C.DisableTorchFunction():
        [29]:         A = input.move_dim(dim, -1)
        [30]:         if A.n_dim > 2:  A = A.flatten(0, -3); has_batch = True
        [31]:         if torch.__version__ >= Version('1.10'):
        [32]:             L, V = torch.linalg.eig(A)
        [33]:         else:
        [34]:             K, P = torch.eig(A, eigenvectors=True)
        [35]:             L = torch.complex(K[:, 0], K[:, 1])
        [36]:             Vr = torch.where((K[:, 1] < 0).reshape((1, -1)), torch.cat((P[:, :1], P[:, :-1]), 1), P)
        [37]:             Vi = (K[:, 1] > 0).reshape((1, -1)) * torch.cat((P[:, 1:], P[:, -1:]), 1) - (K[:, 1] < 0).reshape((1, -1)) * P
        [38]:             V = torch.complex(Vr, Vi)
        [39]:         L = Tensor.inherit_from(L, input, shape=[])
        [40]:         V = Tensor.inherit_from(V, input, shape=[])
        [41]:     dim_type = input.shape[dim[0]:dim[0]+1]
        [42]:     if has_batch:
        [43]:         L = L.split_dim(0, remove_dim(input.shape, dim))
        [44]:         V = V.split_dim(0, remove_dim(input.shape, dim))
        [45]:     L = L.move_dim(-1, dim[0]).add_special_dim(dim[0], dim_type)
        [46]:     V = V.move_dim([-2, -1], dim).add_special_dim(dim[0], dim_type).add_special_dim(dim[0]+1, dim_type)
        [47]:     obj = L, V # suppress:  special_from
        [48]:
        [49]:     def update_special(obj, updates):
        [50]:         if isinstance(obj, tuple):
        [51]:             for x in obj: update_special(x, updates)
        [52]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [53]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [54]:
        [55]:     if getattr(obj, 'grad_fn', None) is not None:
        [56]:         obj.grad_fn_name = "eig"
        [57]:     return obj
        [58]:
        The document of the original function is:
        None

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = linalg_dim[2](input, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    has_batch = False
    with input.hide_special(), torch._C.DisableTorchFunction():
        A = input.move_dim(dim, -1)
        if A.n_dim > 2:  A = A.flatten(0, -3); has_batch = True
        if torch.__version__ >= Version('1.10'):
            L, V = torch.linalg.eig(A)
        else:
            K, P = torch.eig(A, eigenvectors=True)
            L = torch.complex(K[:, 0], K[:, 1])
            Vr = torch.where((K[:, 1] < 0).reshape((1, -1)), torch.cat((P[:, :1], P[:, :-1]), 1), P)
            Vi = (K[:, 1] > 0).reshape((1, -1)) * torch.cat((P[:, 1:], P[:, -1:]), 1) - (K[:, 1] < 0).reshape((1, -1)) * P
            V = torch.complex(Vr, Vi)
        L = Tensor.inherit_from(L, input, shape=[])
        V = Tensor.inherit_from(V, input, shape=[])
    dim_type = input.shape[dim[0]:dim[0]+1]
    if has_batch:
        L = L.split_dim(0, remove_dim(input.shape, dim))
        V = V.split_dim(0, remove_dim(input.shape, dim))
    L = L.move_dim(-1, dim[0]).add_special_dim(dim[0], dim_type)
    V = V.move_dim([-2, -1], dim).add_special_dim(dim[0], dim_type).add_special_dim(dim[0]+1, dim_type)
    obj = L, V # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "eig"
    return obj

def matmul(input: 'Tensor', other: 'Tensor', *, dim1=None, dim2=None, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    perform matmul for the best linear dimensions, justlike other mat** functions do.

    Automatically inheritted method from 'torch.matmul'. The automatically generated codes are as follows:
        [ 1]: def matmul(input: 'Tensor', other: 'Tensor', *, dim1=None, dim2=None, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]: perform matmul for the best linear dimensions, justlike other mat** functions do.
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [input, other]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if input is None: ...
        [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [17]:
        [18]:     if other is None: ...
        [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
        [23]:
        [24]:     # Obtain available sized in arguments (which will be fed into size function).
        [25]:     input_shape=None if input is None else Size(input.shape)
        [26]:     other_shape=None if other is None else Size(other.shape)
        [27]:     # Use the given inner codes if they are provided.
        [28]:     torch_returned = False
        [29]:     if input.has_feature and other.has_feature:
        [30]:         dim1 = exist_dim(input, [])
        [31]:         dim2 = exist_dim(other, [])
        [32]:     elif input.has_space and other.has_space:
        [33]:         dim1 = exist_dim(input, ...)
        [34]:         dim2 = exist_dim(other, ...)
        [35]:     elif input.has_sequence and other.has_sequence:
        [36]:         dim1 = exist_dim(input, '')
        [37]:         dim2 = exist_dim(other, '')
        [38]:     else:  raise TypeError(f"Cannot perform matmul alignment for shapes {input_shape} and {other_shape}. ")
        [39]:     dim1 = dim1[-2:]
        [40]:     dim2 = dim2[-2:]
        [41]:     size = 2 if len(dim1) == len(dim2) else 1
        [42]:     input = input.move_dim(dim1, -1)
        [43]:     other = other.move_dim(dim2, -1)
        [44]:     obj = (input @ other).movedim(list(range_(-size, 0)), dim2[0]) # suppress:  special_from
        [45]:
        [46]:     def update_special(obj, updates):
        [47]:         if isinstance(obj, tuple):
        [48]:             for x in obj: update_special(x, updates)
        [49]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [50]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [51]:
        [52]:     if getattr(obj, 'grad_fn', None) is not None:
        [53]:         obj.grad_fn_name = "matmul"
        [54]:     return obj
        [55]:
        The document of the original function is:

        matmul(input, other, *, out=None) -> Tensor

        Matrix product of two tensors.

        The behavior depends on the dimensionality of the tensors as follows:

        - If both tensors are 1-dimensional, the dot product (scalar) is returned.
        - If both arguments are 2-dimensional, the matrix-matrix product is returned.
        - If the first argument is 1-dimensional and the second argument is 2-dimensional,
          a 1 is prepended to its dimension for the purpose of the matrix multiply.
          After the matrix multiply, the prepended dimension is removed.
        - If the first argument is 2-dimensional and the second argument is 1-dimensional,
          the matrix-vector product is returned.
        - If both arguments are at least 1-dimensional and at least one argument is
          N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first
          argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the
          batched matrix multiply and removed after.  If the second argument is 1-dimensional, a
          1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.
          The non-matrix (i.e. batch) dimensions are :ref:`broadcasted <broadcasting-semantics>` (and thus
          must be broadcastable).  For example, if :attr:`input` is a
          :math:`(j \times 1 \times n \times n)` tensor and :attr:`other` is a :math:`(k \times n \times n)`
          tensor, :attr:`out` will be a :math:`(j \times k \times n \times n)` tensor.

          Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs
          are broadcastable, and not the matrix dimensions. For example, if :attr:`input` is a
          :math:`(j \times 1 \times n \times m)` tensor and :attr:`other` is a :math:`(k \times m \times p)`
          tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the
          matrix dimensions) are different. :attr:`out` will be a :math:`(j \times k \times n \times p)` tensor.

        This operation has support for arguments with :ref:`sparse layouts<sparse-docs>`. In particular the
        matrix-matrix (both arguments 2-dimensional) supports sparse arguments with the same restrictions
        as :func:`torch.mm`

        .. warning::
            Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported,
            or may not have autograd support. If you notice missing functionality please
            open a feature request.

        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.

        On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.

        .. note::

            The 1-dimensional dot product version of this function does not support an :attr:`out` parameter.

        Arguments:
            input (Tensor): the first tensor to be multiplied
            other (Tensor): the second tensor to be multiplied

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> # vector x vector
            >>> tensor1 = torch.randn(3)
            >>> tensor2 = torch.randn(3)
            >>> torch.matmul(tensor1, tensor2).size()
            torch.Size([])
            >>> # matrix x vector
            >>> tensor1 = torch.randn(3, 4)
            >>> tensor2 = torch.randn(4)
            >>> torch.matmul(tensor1, tensor2).size()
            torch.Size([3])
            >>> # batched matrix x broadcasted vector
            >>> tensor1 = torch.randn(10, 3, 4)
            >>> tensor2 = torch.randn(4)
            >>> torch.matmul(tensor1, tensor2).size()
            torch.Size([10, 3])
            >>> # batched matrix x batched matrix
            >>> tensor1 = torch.randn(10, 3, 4)
            >>> tensor2 = torch.randn(10, 4, 5)
            >>> torch.matmul(tensor1, tensor2).size()
            torch.Size([10, 3, 5])
            >>> # batched matrix x broadcasted matrix
            >>> tensor1 = torch.randn(10, 3, 4)
            >>> tensor2 = torch.randn(4, 5)
            >>> torch.matmul(tensor1, tensor2).size()
            torch.Size([10, 3, 5])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input, other]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    if other is None: ...
    elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
    if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if other.device.type == 'cpu': other = other.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    other_shape=None if other is None else Size(other.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    if input.has_feature and other.has_feature:
        dim1 = exist_dim(input, [])
        dim2 = exist_dim(other, [])
    elif input.has_space and other.has_space:
        dim1 = exist_dim(input, ...)
        dim2 = exist_dim(other, ...)
    elif input.has_sequence and other.has_sequence:
        dim1 = exist_dim(input, '')
        dim2 = exist_dim(other, '')
    else:  raise TypeError(f"Cannot perform matmul alignment for shapes {input_shape} and {other_shape}. ")
    dim1 = dim1[-2:]
    dim2 = dim2[-2:]
    size = 2 if len(dim1) == len(dim2) else 1
    input = input.move_dim(dim1, -1)
    other = other.move_dim(dim2, -1)
    obj = (input @ other).movedim(list(range_(-size, 0)), dim2[0]) # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "matmul"
    return obj

@alias("matrix_power")
def matpow(input: 'Tensor', k, *, dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    return a matrix power of A^k.

    Automatically inheritted method from 'torch.matpow'. The automatically generated codes are as follows:
        [ 1]: @alias("matrix_power")
        [ 2]: def matpow(input: 'Tensor', k, *, dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]: return a matrix power of A^k.
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in [input]:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     if input is None: ...
        [14]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [15]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [17]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [18]:
        [19]:     dim = linalg_dim[2](input, dim)
        [20]:
        [21]:     # Obtain available sized in arguments (which will be fed into size function).
        [22]:     input_shape=None if input is None else Size(input.shape)
        [23]:     # Use the given inner codes if they are provided.
        [24]:     torch_returned = False
        [25]:     L, V = eig(input, dim=dim)
        [26]:     L = L.move_dim(dim[0], -1)
        [27]:     V = V.move_dim(dim, -1)
        [28]:     L_k = where((L.real < 0) & (L.imag.abs() < 1e-6), -complex((-L.real) ** k, L.imag), L ** k)
        [29]:     R = V @ diag(L_k, dim=-1) @ V.inv()
        [30]:     if R.is_complex() and not input.is_complex():  R = R.real
        [31]:     obj = R.move_dim([-2, -1], dim).type(input.dtype) # suppress:  special_from
        [32]:
        [33]:     def update_special(obj, updates):
        [34]:         if isinstance(obj, tuple):
        [35]:             for x in obj: update_special(x, updates)
        [36]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [37]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [38]:
        [39]:     if getattr(obj, 'grad_fn', None) is not None:
        [40]:         obj.grad_fn_name = "matpow"
        [41]:     return obj
        [42]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = linalg_dim[2](input, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    L, V = eig(input, dim=dim)
    L = L.move_dim(dim[0], -1)
    V = V.move_dim(dim, -1)
    L_k = where((L.real < 0) & (L.imag.abs() < 1e-6), -complex((-L.real) ** k, L.imag), L ** k)
    R = V @ diag(L_k, dim=-1) @ V.inv()
    if R.is_complex() and not input.is_complex():  R = R.real
    obj = R.move_dim([-2, -1], dim).type(input.dtype) # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "matpow"
    return obj

@alias("matrix_exp")
def matexp(input: 'Tensor', *, dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    return a matrix exponential of e^A.

    Automatically inheritted method from 'torch.matexp'. The automatically generated codes are as follows:
        [ 1]: @alias("matrix_exp")
        [ 2]: def matexp(input: 'Tensor', *, dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]: return a matrix exponential of e^A.
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in [input]:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     if input is None: ...
        [14]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [15]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [17]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [18]:
        [19]:     dim = linalg_dim[2](input, dim)
        [20]:
        [21]:     # Obtain available sized in arguments (which will be fed into size function).
        [22]:     input_shape=None if input is None else Size(input.shape)
        [23]:     # Use the given inner codes if they are provided.
        [24]:     torch_returned = False
        [25]:     L, V = eig(input, dim=dim)
        [26]:     L = L.move_dim(dim[0], -1)
        [27]:     V = V.move_dim(dim, -1)
        [28]:     R = V @ diag(exp(L), dim=-1) @ V.inv()
        [29]:     if R.is_complex() and not input.is_complex():  R = R.real
        [30]:     obj = R.move_dim([-2, -1], dim).type(input.dtype) # suppress:  special_from
        [31]:
        [32]:     def update_special(obj, updates):
        [33]:         if isinstance(obj, tuple):
        [34]:             for x in obj: update_special(x, updates)
        [35]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [36]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [37]:
        [38]:     if getattr(obj, 'grad_fn', None) is not None:
        [39]:         obj.grad_fn_name = "matexp"
        [40]:     return obj
        [41]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = linalg_dim[2](input, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    L, V = eig(input, dim=dim)
    L = L.move_dim(dim[0], -1)
    V = V.move_dim(dim, -1)
    R = V @ diag(exp(L), dim=-1) @ V.inv()
    if R.is_complex() and not input.is_complex():  R = R.real
    obj = R.move_dim([-2, -1], dim).type(input.dtype) # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "matexp"
    return obj

@alias("matrix_log")
def matlog(input: 'Tensor', *, dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    return a matrix exponential of e^A.

    Automatically inheritted method from 'torch.matlog'. The automatically generated codes are as follows:
        [ 1]: @alias("matrix_log")
        [ 2]: def matlog(input: 'Tensor', *, dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]: return a matrix exponential of e^A.
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in [input]:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     if input is None: ...
        [14]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [15]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [17]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [18]:
        [19]:     dim = linalg_dim[2](input, dim)
        [20]:
        [21]:     # Obtain available sized in arguments (which will be fed into size function).
        [22]:     input_shape=None if input is None else Size(input.shape)
        [23]:     # Use the given inner codes if they are provided.
        [24]:     torch_returned = False
        [25]:     L, V = eig(input, dim=dim)
        [26]:     L = L.move_dim(dim[0], -1)
        [27]:     V = V.move_dim(dim, -1)
        [28]:     R = V @ diag(log(L), dim=-1) @ V.inv()
        [29]:     if R.is_complex() and not input.is_complex():  R = R.real
        [30]:     obj = R.move_dim([-2, -1], dim).type(input.dtype) # suppress:  special_from
        [31]:
        [32]:     def update_special(obj, updates):
        [33]:         if isinstance(obj, tuple):
        [34]:             for x in obj: update_special(x, updates)
        [35]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [36]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [37]:
        [38]:     if getattr(obj, 'grad_fn', None) is not None:
        [39]:         obj.grad_fn_name = "matlog"
        [40]:     return obj
        [41]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = linalg_dim[2](input, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    L, V = eig(input, dim=dim)
    L = L.move_dim(dim[0], -1)
    V = V.move_dim(dim, -1)
    R = V @ diag(log(L), dim=-1) @ V.inv()
    if R.is_complex() and not input.is_complex():  R = R.real
    obj = R.move_dim([-2, -1], dim).type(input.dtype) # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "matlog"
    return obj

@alias("matrix_rank")
def rank(input: 'Tensor', *, atol=None, rtol=None, hermitian=False, dim: linalg_dim[2]=None, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.rank'. The automatically generated codes are as follows:
        [ 1]: @alias("matrix_rank")
        [ 2]: def rank(input: 'Tensor', *, atol=None, rtol=None, hermitian=False, dim: linalg_dim[2]=None, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:         ***
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in [input]:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     if input is None: ...
        [14]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [15]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [17]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [18]:
        [19]:     dim = linalg_dim[2](input, dim)
        [20]:
        [21]:     # Obtain available sized in arguments (which will be fed into size function).
        [22]:     input_shape=None if input is None else Size(input.shape)
        [23]:     # Use the given inner codes if they are provided.
        [24]:     torch_returned = False
        [25]:     A = input.move_dim(dim, -1)
        [26]:     with torch._C.DisableTorchFunction():
        [27]:         obj = Tensor.inherit_from(torch.linalg.matrix_rank(A, atol=atol, rtol=rtol, hermitian=hermitian, **dict(out=out) if locals().get('out', None) is not None else {}), input, shape=A.shape[:-2])
        [28]:
        [29]:     def update_special(obj, updates):
        [30]:         if isinstance(obj, tuple):
        [31]:             for x in obj: update_special(x, updates)
        [32]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [33]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [34]:
        [35]:     if getattr(obj, 'grad_fn', None) is not None:
        [36]:         obj.grad_fn_name = "rank"
        [37]:     return obj
        [38]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = linalg_dim[2](input, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    A = input.move_dim(dim, -1)
    with torch._C.DisableTorchFunction():
        obj = Tensor.inherit_from(torch.linalg.matrix_rank(A, atol=atol, rtol=rtol, hermitian=hermitian, **dict(out=out) if locals().get('out', None) is not None else {}), input, shape=A.shape[:-2])

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "rank"
    return obj

@alias("matrix_norm")
def matnorm(input: 'Tensor', ord='fro', dim: linalg_dim[2]=None, keepdim=False, *, dtype=None, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.matnorm'. The automatically generated codes are as follows:
        [ 1]: @alias("matrix_norm")
        [ 2]: def matnorm(input: 'Tensor', ord='fro', dim: linalg_dim[2]=None, keepdim=False, *, dtype=None, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:         ***
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in [input]:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     if input is None: ...
        [14]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [15]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [17]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [18]:
        [19]:     dim = linalg_dim[2](input, dim)
        [20]:
        [21]:     # Obtain available sized in arguments (which will be fed into size function).
        [22]:     input_shape=None if input is None else Size(input.shape)
        [23]:     # Use the given inner codes if they are provided.
        [24]:     torch_returned = False
        [25]:     A = input.move_dim(dim, -1)
        [26]:     with torch._C.DisableTorchFunction():
        [27]:         obj = Tensor.inherit_from(torch.linalg.matrix_norm(A, ord=ord, dim=dim, keepdim=keepdim, dtype=dtype, **dict(out=out) if locals().get('out', None) is not None else {}), input, shape=A.shape if keepdim else A.shape[:-2])
        [28]:
        [29]:     def update_special(obj, updates):
        [30]:         if isinstance(obj, tuple):
        [31]:             for x in obj: update_special(x, updates)
        [32]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [33]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [34]:
        [35]:     if getattr(obj, 'grad_fn', None) is not None:
        [36]:         obj.grad_fn_name = "matnorm"
        [37]:     return obj
        [38]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    dim = linalg_dim[2](input, dim)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    A = input.move_dim(dim, -1)
    with torch._C.DisableTorchFunction():
        obj = Tensor.inherit_from(torch.linalg.matrix_norm(A, ord=ord, dim=dim, keepdim=keepdim, dtype=dtype, **dict(out=out) if locals().get('out', None) is not None else {}), input, shape=A.shape if keepdim else A.shape[:-2])

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "matnorm"
    return obj

### STOP BIWAY AUTO GENERATION

def broadcast(*sizes, with_size_updates=False):
    if len(sizes) == 0:
        broadcasted = Size()
        if with_size_updates: broadcasted.updated_sizes = []
    elif len(sizes) == 1:
        broadcasted = sizes[0]
        if with_size_updates: broadcasted.updated_sizes = [sizes[0]]
    else:
        x, y = sizes[0] ^ sizes[1]
        broadcasted = Size(max_(u, v) for u, v in zip(x, y)).special_from(y)
        updated_sizes = [x, y]
        for z in sizes[2:]:
            x, y = broadcasted ^ z
            broadcasted = Size(max_(u, v) for u, v in zip(x, y)).special_from(y)
            updated_sizes.append(y)
        if with_size_updates: broadcasted.updated_sizes = updated_sizes
    return broadcasted

def remove_dim(size, list_of_removals):
    if not isinstance(list_of_removals, list): list_of_removals = list(list_of_removals)
    list_of_removals.sort()
    return sum_([size[slice(x + 1, y)] for x, y in zip([-1] + list_of_removals, list_of_removals + [None])], Size())

def add_dim(size, list_of_adds):
    if not isinstance(list_of_adds, list): list_of_adds = list(list_of_adds)
    for i in list_of_adds: size = size[:i] + (1,) + size[i:]
    return size

class Tensor(torch.Tensor):
    
    @classmethod
    def get_tensor_subclass(cls, target):
        if not isinstance(target, type): target = target.__class__
        if not issubclass(target, Tensor): target = Tensor
        return target
    
    @classmethod
    def inherit_from(cls, data, target, shape=None):
        subclass = Tensor.get_tensor_subclass(target)
        if shape is None: shape = target.shape
        if shape == ...: result = data.as_subclass(subclass)
        elif len(shape) == 0: result = data.as_subclass(subclass).init_special()
        else: result = data.as_subclass(subclass).special_from(shape)
        result.inherited = getattr(target, "inherited", {})
        return result
    
    @staticmethod
    def _make_subclass(cls, 
        torch_tensor, requires_grad=None, device=None, 
        ref_size=None, **special_dims
    ):
        # Inherit the inheritting properties
        recorded_inherit = getattr(torch_tensor, 'inherited', {})
        # First move the data to device, eliminating argument 'device'
        if device is not None and torch_tensor.device != device:
            if isinstance(device, AutoDevice): device = device.main_device
            torch_tensor = torch_tensor.to(device)
        if requires_grad is None: requires_grad = torch_tensor.requires_grad
        # Create the resulting tensor
        if torch_tensor.__class__ == cls: self = torch_tensor.clone()
        else: self = torch.Tensor._make_subclass(cls, torch_tensor, requires_grad)
        
        # Deal with the special dimensions
        if not hasattr(self, 'special_dims'): new_special_dims = empty_special_dims()
        else: new_special_dims = self.special_dims.copy()
        if ref_size is not None: new_special_dims.update(ref_size.special_dims)
        self.special_dims = Size.update_special_dims(new_special_dims, **special_dims)
        
        self.inherited = recorded_inherit
        return self
    
    def as_subclass(self, *args, **kwargs):
        recorded_inherit = getattr(self, 'inherited', {})
        result = super().as_subclass(*args, **kwargs)
        if isinstance(result, Tensor): result = result.special_from(self)
        result.inherited = recorded_inherit
        return result
    
    @collect_memory
    def __new__(cls, *args, **kwargs):
        """bt.Tensor
        Usages:
            bt.Tensor(tensor: tuple/list/torch.Tensor/bt.Tensor/(tensor with 'shape')/(tensor with method '__tensor__'), requires_grad=None, device=None, **kwargs)
            bt.Tensor(shape: tuple, requires_grad=None, device=None, **kwargs)
            bt.Tensor(*shape: int, requires_grad=None, device=None, **kwargs)
            
        Args:
            tensor (tuple or list or Tensor): Create from a tensor or iterative object. 
            shape (tuple or *tuple): Create a tensor memory from a shape tuple. 
            **kwargs includes:
                func controllers:
                    with/has_func (bool): Whether the tensor has a functional dimension, at the first dimension. Defaults to False. 
                    func_dim (int:0/n_dim-1 or None): The number of function dimensions of the tensor. Defaults to None.
                batch controllers:
                    with_batch (bool): Whether the tensor has a batch dimension, at the first dimension (apart from functional dimension). Defaults to False. 
                    batch_dim (int or None): The number of batch dimensions of the tensor. Defaults to None.
                feature controllers:
                    with/has_channel (bool): Whether the tensor has a channel dimension, at the first dimension apart from batch. Defaults to False. 
                    with/has_feature (bool): Whether the tensor has a feature dimension, at the first dimension apart from batch. Defaults to False. 
                    channel_dim (int or None): The number of channel dimensions of the tensor. Defaults to None. 
                    feature_dim (int or None): The number of feature dimensions of the tensor. Defaults to None. 
                    n_feature_dim (int+): The number of feature dimensions, on the left part of the size. Defaults to 0. 
                sequence controllers:
                    with/has_sequence (bool): Whether the tensor has one sequence dimension, at the first dimension apart from batch and feature. Defaults to False. 
                    sequence_dim (int or None): The number of sequence dimensions of the tensor. Defaults to None. 
                    n_sequence_dim (int+): The number of sequence dimensions, on the left part of the size. Defaults to 0. 
        """
        if len(args) >= 1 and isinstance(args[0], torch.Tensor): self = Tensor._make_subclass(cls, *args, **kwargs)
        elif len(args) >= 1 and hasattr(args[0], '__tensor__'): self = Tensor._make_subclass(cls, args[0].__tensor__(), *args[1:], **kwargs)
        else:
            device = kwargs.pop('device', _device)
            if isinstance(device, AutoDevice): device = device.main_device
            if len(args) >= 1 and hasattr(args[0], 'shape') or isinstance(args[0], (list, tuple)):
                self = Tensor._make_subclass(cls, torch.tensor(args[0], requires_grad=False, device=device), *args[1:], **kwargs)
            else:
                shape = Size(*args)
                if shape.has_special:
                    special_dims = shape.special_dims.copy()
                    special_dims.update(kwargs)
                else: special_dims = kwargs
                self = Tensor._make_subclass(cls, super().__new__(torch.Tensor, *shape.tuple(), device=device), **special_dims)
        
        return self

    def __init__(self, *args, **kwargs): ...
    
    def add_special_dim(self, *args, **kwargs):
        self.shape = Size.add_special_dim(self.shape, *args, **kwargs)
        return self
    def change_special_dim(self, *args, **kwargs):
        self.shape = Size.change_special_dim(self.shape, *args, **kwargs)
        return self
    init_special = Size.init_special
    update_special_from = Size.update_special_from
    def special_from(self, other, allow_view=False):
        if self.n_dim != getattr(other, 'n_dim', self.n_dim) and allow_view: return self.view(other)
        return Size.special_from(self, other)
    is_specialdim = is_special_dim = Size.is_special_dim
    nele = n_ele = property(lambda self: Size.n_ele.fget(self.shape))
    ndim = n_dim = property(lambda self: super().ndim)
    python_repr = property(lambda self: self.shape.python_repr)
    
    @alias("free_special")
    def hide_special(self):
        class hide_special_operator:
            def __init__(self, this):
                self.this = this
            def __enter__(self):
                self.special_dims = self.this.special_dims
                self.this.init_special()
            def __exit__(self, exc_type, exc_value, traceback):
                self.this.special_dims = self.special_dims
        return hide_special_operator(self)
    
    def numel(self, *dim: exist_dim):
        dim = exist_dim(self, *dim)
        if len(dim) > 1: return prod_(self.size(*dim))
        elif len(dim) > 0: return self.size(dim[0])
        return 1
    def dim(self): return super().dim()

    # Get and assign the shape property. The assignment is only accessible for special dimensions. Assigning inconsistent shape would result in an error. 
    @property
    def shape(self):
        if not hasattr(self, 'special_dims'):
            pyfile, lineno, line = get_reference_line(search_more=True, with_line_info=True)
            raise AttributeError(f"Getting batorch shape from an uninitialized Tensor object of size {super().shape}, in line {lineno}, {pyfile}: \n{line}")
        return Size.__new_raw__(super().shape, **self.special_dims)
    
    @shape.setter
    def shape(self, *x): return self.with_shape(*x)

    def size(self, *dim: exist_dim):
        dim = exist_dim(self, dim)
        with torch._C.DisableTorchFunction():
            sizes = tuple(torch_super(self, 'size')(d) for d in dim)
        if len(dim) > 1: return sizes
        else: return sizes[0]
    
    def with_shape(self, *x):
        x = arg_extract(x)
        if isinstance(x, Tensor): x = x.shape
        if not isinstance(x, Size): x = Size(x)
        with torch._C.DisableTorchFunction():
            avouch(all_(u == v or v == -1 for u, v in zip(super().shape, x.tuple())), f"Cannot assign shape {x} to tensor with data shape {tuple(super().shape)}, due to unequal sizes. ")
        self.special_from(x)
        return self
    
    # Control the slicing system. 
    def __getitem__(self, indices):
        shapes = []
        if isinstance(indices, (slice, torch.Tensor)) or indices is ...: indices = (indices,)
        if isinstance(indices, tuple):
            squeeze_dims = []
            unsqueeze_dims = []
            offset = 0
            for i, x in enumerate(indices):
                i += offset
                if x is ...: offset += self.n_dim - len(indices); continue
                if isinstance(x, int_): squeeze_dims.append(i); continue
                if isinstance(x, slice): continue
                if isinstance(x, torch.Tensor):
                    if issubclass(x.dtype, dtype_(bool)):
                        avouch(self.shape[i:i+x.n_dim] == x.shape, TypeError("Bool indices for tensor should be of exact same size as the input tensor. "))
                        offset += x.n_dim
                        unsqueeze_dims.append((i + len(unsqueeze_dims) - len(squeeze_dims), x.shape[:1]))
                        squeeze_dims.extend(range_(i, i + x.n_dim))
                        continue
                    shapes.append(x.shape)
                else: shapes.append(Tensor(x).shape)
                squeeze_dims.append(i)
        elif isinstance(indices, int_): squeeze_dims = [0]; unsqueeze_dims = []
        else: raise TypeError(f"Invalid indices = {indices} for tensor indexing. ")
        if squeeze_dims and all_(y - x == 1 for x, y in zip(squeeze_dims[:-1], squeeze_dims[1:])):
            new_shape = self.shape[:squeeze_dims[0]] + broadcast(*shapes) + self.shape[squeeze_dims[-1]+1:]
        else: new_shape = broadcast(*shapes) + remove_dim(self.shape, squeeze_dims)
        for i, x in unsqueeze_dims:
            new_shape = new_shape[:i] + x + new_shape[i:]
        with torch._C.DisableTorchFunction():
            return Tensor.inherit_from(torch.Tensor.__getitem__(self, indices), self, shape=new_shape).grad_fn_name_("indexing")
            # return Tensor._make_subclass(Tensor, super().__getitem__(indices).as_subclass(torch.Tensor), ref_size=new_shape).grad_fn_name_("indexing")
    
    def __iter__(self):
        i = 0
        while True:
            try: yield self[i]
            except IndexError: break
            i += 1
        
    # Manipulate dimensions.
    @property
    def T(self: 'Tensor', dim: linalg_dim[1:]=None):
        shape = self.shape
        dim = linalg_dim[1:](self.shape, dim)
        dim_order = [i if i not in dim else dim[-dim.index(i) - 1] for i in range_(self.n_dim)]
        unsq_dim = self.shape[dim[0]:dim[0]+1].with_dim_size(0, dim[0]).python_repr
        if len(dim) == 1: return self.permute(*dim_order).unsqueeze(unsq_dim).grad_fn_name_('transpose')
        return self.permute(*dim_order).grad_fn_name_('transpose')
    
    def t_(self: 'Tensor', dim: linalg_dim[1:]=None):
        shape = self.shape
        dim = linalg_dim[1:](self.shape, dim)
        dim_order = [i if i not in dim else dim[-dim.index(i) - 1] for i in range_(self.n_dim)]
        unsq_dim = self.shape[dim[0]:dim[0]+1].with_dim_size(0, dim[0]).python_repr
        if len(dim) == 1: return self.permute(*dim_order).unsqueeze(unsq_dim).grad_fn_name_('transpose')
        return self.permute_(*dim_order).grad_fn_name_('transpose_')
    
    def permute_(self: 'Tensor', *dims: exist_dim):
        dims = exist_dim(self, *dims)
        avouch(len(dims) == self.ndim, RuntimeError(f"permute_(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = {self.n_dim} is not equal to len(dims) = {len(dims)}"))
        cur_order = list(range_(self.ndim))
        special_shape = self.shape
        self.init_special()
        for i in range_(len(cur_order)):
            j = cur_order.index(dims[i])
            if j == i: continue
            cur_order[i], cur_order[j] = cur_order[j], cur_order[i]
            self.transpose_(i, j)
        return self.special_from(special_shape.permute(*dims)).grad_fn_name_('permute_')

    @alias("movedim")
    def move_dim(self, dim1: del_dim, dim2: new_dim):
        """
        movedim(self, dim1, dim2) -> Tensor

        move dim1 to dim2 (specified in the targeting size)
        data of size (2, 3, 4, 5) can be transform to (2, 4, 5, 3) by data.movedim(1, -1) or data.movedim(1, 3)
        """
        dim1 = del_dim(self.shape, dim1)
        ref_size = sum_([self.shape[d:d+1] for d in dim1], Size())
        dim2_size = Size(dim2)
        if dim2 == ...: ref_size.init_special()
        if isinstance(dim2, int_): dim2_size = dim2_size.special_from(self.shape[dim1[0]:dim1[0]+1])
        elif isinstance(dim2, (list, tuple)) and all_(isinstance(d, int_) for d in dim2) and len(dim1) == len(dim2):
            dim2_size = dim2_size.special_from(ref_size)
        if dim2_size.has_special: dim2 = new_dim(remove_dim(self.shape, dim1), dim2_size.python_repr)
        else: dim2 = new_dim(remove_dim(self.shape, dim1), dim2)
        avouch(len(dim1) == len(dim2) or len(dim2)== 1, "Tensor.move_dim only takes dimension of same size or one target dim.")
        if len(dim2) == 1:
            d2 = dim2[0]
            if all_(d > d2 for d in dim1):
                dimensions = list(range_(d2)) + list(dim1) + [i for i in range_(d2, self.n_dim) if i not in dim1]
            else:
                nondim1 = [i for i in range_(self.n_dim) if i not in dim1]
                dimensions = nondim1[:d2] + list(dim1) + nondim1[d2:]
            res_shape = sum_([self.shape[d:d+1].special_from(ref_size[i-d2:i-d2+1]) if d2 <= i < d2 + len(dim1) else self.shape[d:d+1] for i, d in enumerate(dimensions)], Size())
        else:
            dimensions = [0] * self.n_dim
            assigned = [False] * self.n_dim
            for i in dim1:
                j = dim2[dim1.index(i)]
                dimensions[j] = i
                assigned[j] = True
            for i in range_(self.n_dim):
                if i in dim1: continue
                j = assigned.index(False)
                dimensions[j] = i
                assigned[j] = True
            avouch(all_(assigned), RuntimeError(f"Not permute for dimension move if dim1={dim1} and dim2={dim2}. "))
            res_shape = sum_([self.shape[d:d+1].special_from(ref_size[dim2.index(i):dim2.index(i)+1]) if i in dim2 else self.shape[d:d+1] for i, d in enumerate(dimensions)], Size())
        with self.hide_special():
            res = self.permute(*dimensions).special_from(res_shape)
        return res.grad_fn_name_('move_dim')
            
        # d1 = dim1[0]
        # d2 = dim2[0]

        # if d1 < d2: return self.permute(*range(d1), *range(d1+1, d2+1), d1, *range(d2+1, self.n_dim)).add_special_dim(d2, dim2)
        # elif d1 > d2: return self.permute(*range(d2), d1, *range(d2, d1), *range(d1+1, self.n_dim)).add_special_dim(d2, dim2)
        # return self.add_special_dim(d2, dim2).grad_fn_name_('move_dim')

    @alias("movedim_")
    def move_dim_(self, dim1: del_dim, dim2: exist_dim):
        """
        In-place operation for movedim
        """
        dim1 = del_dim(self.shape, dim1)
        dim2 = exist_dim(self.shape, dim2)
        avouch(len(dim1) == len(dim2) == 1, "Tensor.move_dim only takes integers as inputs.")
        d1 = dim1[0]
        d2 = dim2[0]

        if d1 < d2: return self.permute_(*range(d1), *range(d1+1, d2+1), d1, *range(d2+1, self.n_dim)).add_special_dim(d2, dim2)
        elif d1 > d2: return self.permute_(*range(d2), d1, *range(d2, d1), *range(d1+1, self.n_dim)).add_special_dim(d2, dim2)
        return self.add_special_dim(d2, dim2).grad_fn_name_('move_dim_')

    @alias("joindims", "join_dims", "mergedims")
    def merge_dims(self, *dims: exist_dim, target: new_dim=None):
        """
        mergedims(self, *source, target) -> Tensor

        merge dims into one dimension: target (the last argument)
        data of size (2, 3, 4, 5) can be transform to (24, 5) with a Cartesian of 3 x 2 x 4 by:
            data.mergedims([1, 0, 2], target=0) / data.mergedims(1, 0, 2, target=0)
        Note that one can only omit the target dimension if no order of dimension is changed. 
            the automatically chosen target is the new position of the last dimension one gives. 
            e.g. data.mergedims(1, 0, 3) result in (4, 30) and it follows a Cartesian of 2 x 3 x 5.
        """
        input_dims = dims
        dims = exist_dim(self.shape, *dims)
        if target is None:
            target_repr = Size(dims[-1] - sum_([1 if d < dims[-1] else 0 for d in dims[:-1]])).update_special_from(Size(input_dims[-1])).python_repr
            dims = sorted(dims)
        else: target_repr = (target,)
        target = new_dim(remove_dim(self.shape, dims), *target_repr)
        avouch(len(dims) >= 2, f"Please input at least two dims to be merged for method 'mergedims', not {dims}. ")
        avouch(len(target) == 1, f"At most one 'target' argument is allowed for method 'mergedims', not {target_repr}. ")

        res = self.clone()
        other_dims = [i for i in range_(self.n_dim) if i not in dims]
        out_dims = other_dims[:target[0]] + dims + other_dims[target[0]:]
        prev_shape = res.shape
        with res.hide_special():
            res.permute_(out_dims)
            res = res.flatten(target[0], target[0] + len(dims) - 1)
        post_shape = sum_((prev_shape[i:i+1] for i in other_dims[:target[0]]), Size())
        post_shape += res.shape.special_from(target)[target[0]:target[0]+1]
        post_shape += sum_((prev_shape[i:i+1] for i in other_dims[target[0]:]), Size())
        return res.special_from(post_shape).grad_fn_name_('merge_dims')

    @alias("splitdim")
    def split_dim(self, source: del_dim, *size: Size):
        """
        splitdim(self, source, *target_size) -> Tensor

        split one dimension source into multiple dimensions: target
        data of size (2, 4, 5) can be transform to (2, 2, 2, 5) with data.splitdim(1, 2, 2).
        Note that batch representations for source and target are different
            (splitdim([1], [2], 2) means split the batchdim at index 1 into a size of ([2], 2), which is 2x2 with batchdim at index 0).
            One can use -1 for arbitrary size. 
        """
        size = Size(*size)
        source = del_dim(self, source)
        # avouch(len(size) >= 2, f"Please input an at-least-two-dim-shape to split dimension {source} into in method 'splitdim', not {size}. ")
        if len(source) > 1: self = self.merge_dims(*source, target=source[0])

        new_size = self.shape[:source[0]] + size.with_n_ele(self.shape[source[0]]) + self.shape[source[0] + 1:]
        return self.view(new_size).grad_fn_name_('split_dim')
    
    def expand(self, *sizes: Size):
        return self.expand_to(*sizes).grad_fn_name_('expand')

    def expand_as(self, other: 'Tensor'):
        return self.expand_to(other).grad_fn_name_('expand_as')
        
    def expand_to(self, *target, assign_to_dims: exist_dim=None, dims_allow_mismatch: exist_dim=None):
        if len(target) == 1 and isinstance(target[0], torch.Tensor): target = target[0].shape
        avouch(isinstance(target, tuple), TypeError(f"Invalid input for bt.Tensor.expand_to: {target}, should be a 'tuple' or 'Size'."))
        if not isinstance(target, Size): target = Size(*target)
        if assign_to_dims is None:
            new_shape, _ = self.shape ^ target
            avouch(len(new_shape) == len(target), TypeError(f"Cannot expand tensor with shape {self.shape} to {target}. "))
        else:
            assign_to_dims = list(exist_dim(target, assign_to_dims))
            new_shape = Size(*(self.shape[assign_to_dims.index[i]] if i in assign_to_dims else 1 for i in range_(len(target))).special_from(target))
        if dims_allow_mismatch is None: dims_allow_mismatch = tuple()
        else: dims_allow_mismatch = tuple(exist_dim(target, dims_allow_mismatch))
        avouch(all_(i in dims_allow_mismatch or x == y or x == 1 or y in (1, -1) for i, (x, y) in enumerate(zip(new_shape, target))), 
               TypeError(f"Size mismatch in 'expand_to': {self.shape} (expanded to {new_shape}) and {target}. "))
        n_repeats = tuple(y if i not in dims_allow_mismatch and x == 1 else 1 for i, (x, y) in enumerate(zip(new_shape, target)))
        print(n_repeats)
        if len(n_repeats) > 0:
            return self.view(new_shape).repeat(*n_repeats).grad_fn_name_('expand_to')
        else: return self.view(new_shape).grad_fn_name_('expand_to')
    
    def unsqueeze_to(self: 'Tensor', *target:Size, assign_to_dims: exist_dim=None):
        return self.expand_to(*target, assign_to_dims=assign_to_dims, dims_allow_mismatch=tuple()).grad_fn_name_('unsqueeze_to')

    # Control the output of the tensor. 
    def tobytes(self): return self.detach().cpu().numpy().tobytes()
    
    def __hash__(self): return super().__hash__()
    
    @classmethod
    def __block_repr__(cls, rpr: str, by=' '):
        n_col = max_(len(line) for line in rpr.split('\n'))
        return '\n'.join(l + by * (n_col - len(l)) for l in rpr.split('\n'))
    
    @classmethod
    def __corner_block_repr__(cls, rpr: str, by=' '):
        lines = rpr.split('\n')
        n_col = max_(len(line) for line in lines)
        n_row = len(lines)
        return '\n'.join(
            ('' if i == 0 else (' ' if i == n_row - 1 else '|')) + 
            l + by * (n_col - len(l)) + 
            ('' if i == n_row - 1 else (' ' if i == 0 else '|'))
        for i, l in enumerate(rpr.split('\n')))
    
    @classmethod
    def __shift_repr__(cls, rpr: str, shift: int=1, ignore_first: bool=True, by=' '):
        if ignore_first: return ('\n' + by * shift).join(rpr.split('\n'))
        return '\n'.join(by * shift + l for l in rpr.split('\n'))
    
    def __raw_repr__(self, cell_format=None):
        criteria = {
            # in the form of (max_size, (pad_left, pad_right))
            # s.t. '...' appears for `>max_size` elements and when it happens, 
            # first `pad_left` and last `pad_right` elements are shown arround '...'. 
            'batch': (1, (1, 0)),
            'sequence': (2, 1),
            '<n-4': (4, 1),
            '<n-2': (6, 2),
            '<n-1': (10, 4),
            '<n': (20, 8)
        }
        cell_len_exp = 8
        cell_len_str = 3

        if cell_format is None:
            if self.n_ele == 0: return "[]"

            display_tensor = None
            for d in range_(self.n_dim):
                if self.is_batch_dim(d): max_size, pad_size = criteria['batch']
                elif self.is_sequence_dim(d): max_size, pad_size = criteria['sequence']
                elif d < self.n_dim - 4: max_size, pad_size = criteria['<n-4']
                elif d < self.n_dim - 2: max_size, pad_size = criteria['<n-2']
                elif d < self.n_dim - 1: max_size, pad_size = criteria['<n-1']
                else: max_size, pad_size = criteria['<n']
                if self.size(d) <= max_size: continue
                if isinstance(pad_size, int_): pad_size = (pad_size, pad_size)
                if display_tensor is None: display_tensor = cat(self[(slice(None),) * d + (slice(None, pad_size[0]),)], self[(slice(None),) * d + (slice(self.size(d)-pad_size[1], None),)], d)
                else: display_tensor = cat(display_tensor[(slice(None),) * d + (slice(None, pad_size[0]),)], display_tensor[(slice(None),) * d + (slice(self.size(d)-pad_size[1], None),)], d)
            if display_tensor is None: display_tensor = self.flatten()
            else: display_tensor = display_tensor.flatten()
            if display_tensor.is_complex(): display_tensor = cat(display_tensor.real, display_tensor.imag)
            str_ele = False
            if any(isnan(display_tensor)):
                display_tensor = display_tensor[~isnan(display_tensor)]
                str_ele = True
            if any(isinf(display_tensor)):
                display_tensor = display_tensor[~isinf(display_tensor)]
                str_ele = True
            if display_tensor.n_ele == 0:
                if not str_ele: raise RuntimeError("str_ele=False after eliminating nan/inf to an empty tensor.")
                cell_format = ('int', cell_len_str + any(self < 0), self.shape)
            elif issubclass(display_tensor.dtype, dtype_(bool)):
                cell_format = ('bool', 4 if all(display_tensor) else 5, self.shape)
            elif not display_tensor.dtype.is_floating_point:
                cell_len = int_(max(display_tensor.clamp(min=1).log(10).floor()).item()) + 1
                cell_len = max_(cell_len, int_(max(display_tensor.clamp(max=-1e-1).abs().log(10).floor()).item()) + 1 + 1)
                cell_len = min_(cell_len, cell_len_exp)
                if str_ele: cell_len = max_(cell_len, cell_len_str)
                cell_format = ('int', cell_len, self.shape)
            else:
                zero_to_one = lambda x: ones(1)[0] if x == 0 else x.log(10)
                if sum((display_tensor.abs() > 1e-64) & (display_tensor.abs() < 1e-4)) > display_tensor.n_ele / 2 or \
                    any(display_tensor >= 1e6) or any(display_tensor <= -1e5): cell_format = ('exp', cell_len_exp, self.shape)
                elif abs(zero_to_one(display_tensor.abs().max()) - zero_to_one(display_tensor.abs().min())).item() > 5: cell_format = ('exp', cell_len_exp, self.shape)
                elif all((display_tensor - display_tensor.round()).abs() < 1e-4) or any(display_tensor >= 1e4) or any(display_tensor <= -1e3):
                    cell_len = int_(max((-display_tensor.sign()).clamp(min=0) + display_tensor.abs().clamp(min=1).log(10).floor()).item()) + 1 + 1
                    # cell_len = max_(cell_len, int_(max(display_tensor.clamp(max=-1e-1).abs().log(10).floor()).item()) + 1 + 1 + 1)
                    cell_len = min_(cell_len, cell_len_exp)
                    if str_ele: cell_len = max_(cell_len, cell_len_str)
                    cell_format = ('.0f', cell_len, self.shape)
                elif all((display_tensor - display_tensor.round(decimals=2)).abs() < 1e-4) or any(display_tensor >= 1e2) or any(display_tensor <= -1e1):
                    cell_len = int_(max((-display_tensor.sign()).clamp(min=0) + display_tensor.abs().clamp(min=1).log(10).floor()).item()) + 1 + 3
                    # cell_len = max_(cell_len, int_(max(display_tensor.clamp(max=-1e-1).abs().log(10).floor()).item()) + 1 + 1 + 3)
                    cell_len = min_(cell_len, cell_len_exp)
                    if str_ele: cell_len = max_(cell_len, cell_len_str)
                    cell_format = ('.2f', cell_len, self.shape)
                else:
                    cell_len = int_(max((-display_tensor.sign()).clamp(min=0) + display_tensor.abs().clamp(min=1).log(10).floor()).item()) + 1 + 5
                    # cell_len = max_(cell_len, int_(max(display_tensor.clamp(max=-1e-1).abs().log(10).floor()).item()) + 1 + 1 + 5)
                    cell_len = min_(cell_len, cell_len_exp)
                    if str_ele: cell_len = max_(cell_len, cell_len_str)
                    cell_format = ('.4f', cell_len, self.shape)
        cell_fname, cell_len, total_size = cell_format
        elps = ("{:^%ds}"%cell_len).format('...')
        if self.n_dim == 0:
            val = self.item()
            def val_str(val):
                if repr(val) == 'nan': return ("{:>%ds}"%cell_len).format("NaN")
                elif repr(val) == 'inf': return ("{:>%ds}"%cell_len).format("Inf")
                elif repr(val) == '-inf': return ("{:>%ds}"%cell_len).format("-Inf")
                elif cell_fname == 'bool': return ("{:^%ds}"%cell_len).format(str(val))
                elif cell_fname == 'int': return ("{:^%dd}"%cell_len).format(val)
                elif cell_fname == '.0f': return ("{:%ds}"%cell_len).format(str(round_(val)) + '.')
                elif cell_fname == '.2f': return ("{:%d.2f}"%cell_len).format(val)
                elif cell_fname == '.4f': return ("{:%d.4f}"%cell_len).format(val)
                elif cell_fname == 'exp':
                    if val == 0: lev = 0
                    else: lev = int_(math.log(abs_(val), 10))
                    base = val / (10 ** lev)
                    lev_str = f'{lev:2d}'.replace(' ', '+') if lev >= 0 else f'{lev:1d}'
                    return f"{base:5.2f}e{lev_str}"
            if isinstance(val, complex_):
                return val_str(val.real) + '+' + val_str(val.imag) + 'i'
            else:
                return val_str(val)
        if self.n_dim == 1:
            max_size, pad_size = criteria['<n']
            if isinstance(pad_size, int_): pad_size = (pad_size, pad_size)
            n_size = self.size(0)
            display_samples = range_(n_size) if n_size <= max_size else list(range_(pad_size[0])) + [...] + list(range_(n_size - pad_size[1], n_size))
            if self.has_func: return f"({' '.join(elps if i == ... else self[i].__raw_repr__(cell_format=cell_format) for i in display_samples)})"
            elif self.has_batch:
                if total_size.n_func_dim + total_size.n_batch_dim == total_size.n_dim:
                    return f"{{{self[0].__raw_repr__(cell_format=cell_format)}, ...}}"
                else: return f"{{{self[0].__raw_repr__(cell_format=cell_format)}}}"
            elif self.has_feature:
                if total_size.n_feature_dim == 1:
                    return Tensor.__corner_block_repr__('\n'.join(elps if i == ... else self[i].__raw_repr__(cell_format=cell_format) for i in display_samples))
                return f"{' '.join(elps if i == ... else self[i].__raw_repr__(cell_format=cell_format) for i in display_samples)}"
            elif self.has_sequence:
                if total_size.n_space_dim > 0: return f">{self[0].__raw_repr__(cell_format=cell_format)}>"
                elif n_size == 1: return f"> {self[0].__raw_repr__(cell_format=cell_format)} >"
                elif n_size == 2: return f"'{self[0].__raw_repr__(cell_format=cell_format)} > {self[-1].__raw_repr__(cell_format=cell_format)}'"
                else: return f"'{self[0].__raw_repr__(cell_format=cell_format)} > ... > {self[-1].__raw_repr__(cell_format=cell_format)}'"
            else: return f"[{', '.join(elps if i == ... else self[i].__raw_repr__(cell_format=cell_format) for i in display_samples)}]"
        if self.n_dim > 4: max_size, pad_size = criteria['<n-4']
        elif self.n_dim > 2: max_size, pad_size = criteria['<n-2']
        else: max_size, pad_size = criteria['<n-1']
        if isinstance(pad_size, int_): pad_size = (pad_size, pad_size)
        n_size = self.size(0)
        display_samples = range_(n_size) if n_size <= max_size else list(range_(pad_size[0])) + [...] + list(range_(n_size - pad_size[1], n_size))
        if self.shape[:1].has_func: return '(' + '\n '.join(' ' + elps if i == ... else Tensor.__shift_repr__(self[i].__raw_repr__(cell_format=cell_format)) for i in display_samples) + ')'
        elif self.shape[:1].has_batch:
            if len(self.shape) <= 2 or len(self.shape) == 3 and self.shape.has_sequence:
                return f"{{{Tensor.__shift_repr__(self[0].__raw_repr__(cell_format=cell_format))}, ...}}"
            return f"{{{Tensor.__shift_repr__(self[0].__raw_repr__(cell_format=cell_format))},\n...}}"
        elif self.shape[:1].has_feature:
            if total_size.n_feature_dim <= 2:
                return Tensor.__corner_block_repr__('\n'.join(elps if i == ... else self[i].__raw_repr__(cell_format=cell_format) for i in display_samples))
            elif total_size.n_dim - len(self.shape) == total_size.feature_stop-1 and total_size.special_dims[FeatDim.var] < 0 and not self.shape[1:].has_func:
                if total_size.n_feature_dim > 1:
                    return f"{' '.join(elps if i == ... else self[i].__raw_repr__(cell_format=cell_format) for i in display_samples)}"
                return f"[{' '.join(elps if i == ... else Tensor.__shift_repr__(self[i].__raw_repr__(cell_format=cell_format)) for i in display_samples)}]"
            else: return '' + '\n '.join(' ' + elps if i == ... else Tensor.__shift_repr__(self[i].__raw_repr__(cell_format=cell_format)) for i in display_samples) + ''
        elif self.shape[:1].has_sequence:
            if total_size.n_dim - len(self.shape) == total_size.sequence_stop-1 and total_size.special_dims[SequeDim.var] < 0:
                return f">{Tensor.__shift_repr__(self[0].__raw_repr__(cell_format=cell_format))}>"
            elif n_size == 1: return '> ' + Tensor.__shift_repr__(self[0].__raw_repr__(cell_format=cell_format), 3) + ' >'
            else:
                item1 = Tensor.__shift_repr__(self[0].__raw_repr__(cell_format=cell_format))
                itemn = Tensor.__shift_repr__(self[-1].__raw_repr__(cell_format=cell_format))
                if n_size == 2: return "'''>" + item1 + '\n ' + itemn + ">'''"
                else: return "'''" + item1 + '\n ' + ' ' * re.search(r'\d', item1).span()[0] + 'v...v\n ' + itemn + "'''"
        else:
            if total_size.n_dim - len(self.shape) == total_size.space_stop-1:
                columns = []
                n_row = None
                for i in display_samples:
                    if i == ...: columns.append('\n'.join([' ' * cell_format[1]] * (n_row - 1) + [elps])); continue
                    columns.append(Tensor.__block_repr__(self[i].__raw_repr__(cell_format=cell_format)))
                    if n_row is None: n_row = len(columns[0].split('\n'))
                return '[' + Tensor.__shift_repr__('\n'.join((', ' if i == n_row - 1 else '  ').join(rows) for i, rows in enumerate(zip(*[c.split('\n') for c in columns])))) + ']'
            return '[' + ',\n '.join(Tensor.__shift_repr__(elps if i == ... else self[i].__raw_repr__(cell_format=cell_format)) for i in display_samples) + ']'
    
    @property
    def mean_std(self):
        return f"{self.mean().detach().cpu().item():.6f}  {self.std().detach().cpu().item():.6f}"
    
    def __str__(self):
        if not hasattr(self, 'special_dims'):
            raise RuntimeError(f"Not initialized batorch.Tensor of shape: {self.as_subclass(torch.Tensor).shape}")
        if self.n_dim == 0:
            val = self.item()
            if not self.dtype.is_floating_point: return str(val)
            elif abs_(val - self.floor().item()) < 1e-4: return f"{int_(val)}."
            elif repr(val) == 'nan': return "NaN"
            elif repr(val) == 'inf': return "Inf"
            return "%6.4f"%self.item()
        prefix = "Tensor("
        parts = [Tensor.__shift_repr__(self.__raw_repr__(), len(prefix))]
        raw_shape_str = str(self.shape).split('Size')[-1]
        parts.append(f"shape={raw_shape_str}")
        if self.device.type != 'cpu':    
            device_str = 'cpu' if self.device.type == 'cpu' else f"{self.device.type}:{self.device.index}"
            parts.append(f"device=[{device_str}]")
        if self.grad_fn: parts.append(f"grad_fn={self.grad_fn}")
        if self.requires_grad: parts.append(f"requires_grad={self.requires_grad}")
        return prefix + ', '.join(parts) + ')'

    def __repr__(self):
        if not hasattr(self, 'special_dims'):
            raise RuntimeError(f"<Not initialized batorch.Tensor of shape: {self.as_subclass(torch.Tensor).shape}>")
        num_nans = isnan(self).sum()
        num_infs = isinf(self).sum()
        special_dim_str = f"[{self.n_special_dim}]+" if self.n_special_dim > 0 else ''
        device_str = 'cpu' if self.device.type == 'cpu' else f"{self.device.type}:{self.device.index}"
        raw_shape_str = str(self.shape).split('Size')[-1]
        valid_nums = self.flatten()
        valid_nums = valid_nums[~isnan(valid_nums)]
        valid_nums = valid_nums[~isinf(valid_nums)]
        if valid_nums.n_ele == 0:
            valid_val_str = 'nothing'
        elif self.is_complex():
            valid_val_str = f"Re(min:{valid_nums.real.min()}, med:{valid_nums.real.median()}, max:{valid_nums.real.max()}), "
            valid_val_str += f"Im(min:{valid_nums.imag.min()}, med:{valid_nums.imag.median()}, max:{valid_nums.imag.max()})"
        elif self.dtype == bool:
            if valid_nums.min() == valid_nums.max():
                valid_val_str = f"{valid_nums.min()}"
            else: valid_val_str = "True, False"
        else:
            valid_val_str = f"min:{valid_nums.min()}, med:{valid_nums.median()}, max:{valid_nums.max()}"
        nan_str = ''; inf_str = ''
        if num_nans > 0: nan_str = f"{num_nans} NaN"
        if num_nans > 1: nan_str += 's'
        if num_infs > 0: inf_str = f"{num_infs} Inf"
        if num_infs > 1: inf_str += 's'
        error_val_str = ', '.join([x for x in (nan_str, inf_str) if x])
        if num_nans + num_infs == 0: val_range_str = valid_val_str
        elif (num_nans + num_infs) / self.n_ele < 0.5: val_range_str = f"{valid_val_str}, {error_val_str}"
        else: val_range_str = error_val_str
        return f"<{special_dim_str}{self.n_space_dim}D {str(self.dtype).split('.')[-1]} Tensor on {device_str}: shape={raw_shape_str}, requires_grad={self.requires_grad}, val=[{val_range_str}]>"
    
    ## Other utilities
    def byte_size(self):
        return ByteSize(self.element_size() * self.numel())

    def rename(self, *args, **kwargs):
        with torch._C.DisableTorchFunction():
            output = Tensor.inherit_from(torch_super(self, 'rename')(*args, **kwargs), self)
        for i, n in enumerate(output.names):
            if n is None: continue
            if 'func' in n.lower(): output.add_special_dim(i, FuncDim)
            if 'batch' in n.lower(): output.add_special_dim(i, {})
            elif 'channel' in n.lower() or 'feature' in n.lower(): output.add_special_dim(i, [])
            elif 'time' in n.lower() or 'series' in n.lower() or 'sequence' in n.lower(): output.add_special_dim(i, '')
        return output.grad_fn_name_('rename')

    def refine_names(self, *args):
        with torch._C.DisableTorchFunction():
            output = Tensor.inherit_from(torch_super(self, 'refine_names')(*args), self)
        for i, n in enumerate(output.names):
            if n is None: continue
            if 'func' in n.lower(): output.add_special_dim(i, FuncDim)
            if 'batch' in n.lower(): output.add_special_dim(i, {})
            elif 'channel' in n.lower() or 'feature' in n.lower(): output.add_special_dim(i, [])
            elif 'time' in n.lower() or 'series' in n.lower() or 'sequence' in n.lower(): output.add_special_dim(i, '')
        return output.update_special_from(self)

    def normalize_(self):
        m, M = self.min(), self.max()
        if m == M:
            if M >= 1: return self.zero_().add_(1)
            if m <= 0: return self.zero_()
            return self
        self.sub_(m)
        self.div_(M-m)
        return self.grad_fn_name_('normalize_')

    def normalize(self):
        m, M = self.min(), self.max()
        if m == M:
            if M >= 1: return ones_like(self)
            if m <= 0: return zeros_like(self)
            return self
        return ((self - m) / (M - m)).grad_fn_name_('normalize')

    @alias('extend')
    def append(self, value):
        avouch(self.n_dim == 1, "Only 1-dimensional tensor can use 'append' to concat. ")
        if isinstance(value, torch.Tensor):
            if not isinstance(value, Tensor): tensor_value = value
            else: tensor_value = value.as_subclass(Tensor, device=self.device, dtype=self.dtype)
        else: tensor_value = tensor(value, device=self.device, dtype=self.dtype)
        avouch(tensor_value.n_dim <= 1, "Only scalar or 1-dimensional tensors can by concat using append/extend. ")
        return cat(self, tensor_value, dim=0).grad_fn_name_('append')
    
    def setitem_(self, ind, val):
        self[ind] = val
        return self.grad_fn_name_('setitem_')
    
    def grad_fn_name_(self, name):
        self.grad_fn_name = name
        return self
    
    @alias('concatenate')
    def cat(self, *other, dim=0):
        return cat(self, *other, dim=dim).grad_fn_name_('cat')

    def stack(self, *other, dim=0):
        return stack(self, *other, dim=dim).grad_fn_name_('stack')
    
    ## dtypes
    @alias("as_type")
    def astype(self, dt):
        """
            numpy dtype v.s. torch dtype:
            ==============================
            numpy type // torch type
            ------------------------------
            void0, void::void // 
            object0, object_::object // 
            bool8, bool_::bool // torch.bool
            byte, int8::int8 // torch.int8
            short, int16::int16 // torch.short, torch.int16
            int32, intc::int32 // torch.int, torch.int32
            int0, int_, int64, intp, longlong, signedinteger::int64 // torch.long, torch.int64
            ubyte, uint8::uint8 // torch.uint8
            ushort, uint16::uint16 // 
            uint32, uintc::uint32 // 
            uint, uint0, uint64, Uint64, uintp, ulonglong::uint64 // 
            // torch.bfloat16 # 16bit, 32bit
            half, float16::float16 // torch.half, torch.float16
            single, float32::float32 // torch.float, torch.float32
            double, float64, float_, longdouble, longfloat, number::float64 // torch.double, torch.float64
            // torch.complex32
            csingle, complex64, singlecomplex::complex64 // torch.cfloat, torch.complex64
            cdouble, cfloat, clongdouble, clongfloat, complex_, complex128, longcomplex::complex128 // torch.cdouble, torch.complex128
            str0, str_, Str0::str // 
            bytes0, bytes_, string_::bytes // 
            datetime64::datetime64 // 
            timedelta64::timedelta64 // 
            # 
            // torch.qint8
            // torch.qint32
            // torch.quint8
            // torch.quint4x2
        """
        torch_dt = to_torch_dtype(dt)
        with torch._C.DisableTorchFunction():
            return Tensor.inherit_from(torch_super(self, 'type')(torch_dt), self).grad_fn_name_('astype')
        # if isinstance(dt, str): return Tensor.inherit_from(super().type(dt.replace('bt.', 'torch.')), self, shape=...)
        # if hasattr(dt, 'dtype'): dt = dt.dtype
        # if isinstance(dt, torch.dtype): return Tensor.inherit_from(super().type(dt), self, shape=...)
        # import numpy as np
        # dt_name = np.dtype(dt).name
        # dtype_map = {'uint16': "int32", 'uint32': "int64", 'uint64': "int64"}
        # torch_dt = getattr(torch, dtype_map.get(dt_name, dt_name), None)
        # avouch(torch_dt is not None, f"Invalid dtype {dt}: {dt_name} cannot be converted into torch dtype.")
        # return Tensor.inherit_from(super().type(torch_dt), self, shape=...)

    def type(self, dt=None):
        with torch._C.DisableTorchFunction():
            if dt is None: return torch_super(self, 'type')().replace("torch.", "batorch.")
            else: return self.astype(dt)

    def __getattribute__(self, key):
        with torch._C.DisableTorchFunction():
            g = super(Tensor, self).__getattribute__(key)
        if key in ('grad', 'real', 'imag'):
            if not isinstance(g, torch.Tensor): return g
            return Tensor.inherit_from(g, self)
        elif key == 'grad_fn':
            if g is None: return g
            class gfn:
                def __init__(self, fn, fn_name): self.fn = fn; self.fn_name = fn_name
                def __call__(self, *args, **kwargs): return self.fn(*args, **kwargs)
                def __getattribute__(self, key):
                    if key in ('fn', 'fn_name', '__init__', '__call__', '__getattribute__', '__repr__'):
                        return super().__getattribute__(key)
                    attr = getattr(self.fn, key, None)
                    if attr is not None: return attr
                    return super().__getattribute__(key)
                def __repr__(self): return f"<backward: {self.fn_name} at 0x{id(self.fn):0x}>"
            return gfn(g, getattr(self, 'grad_fn_name', 'Unknown'))
        return g
    
    def __setattr__(self, key, value):
        if key in ('grad', 'real', 'imag'):
            if isinstance(value, Tensor):
                value = value.as_subclass(torch.Tensor)
        super().__setattr__(key, value)
        
    def as_func_tensor(self):
        avouch(self.n_dim == 1, TypeError("Only 1D tensor can be converted to functional tensor. "))
        self.init_special()
        self.special_dims[FuncDim.var] = self.n_dim
        return self

    def as_batch_tensor(self):
        avouch(self.n_dim == 1, TypeError("Only 1D tensor can be converted to batch tensor. "))
        self.init_special()
        self.special_dims[BatchDim.var] = self.n_dim
        return self

    def as_feature_tensor(self):
        self.init_special()
        self.special_dims[FeatDim.var] = self.n_dim
        return self

    def as_sequence_tensor(self):
        self.init_special()
        self.special_dims[SequeDim.var] = self.n_dim
        return self

    def auto_device(self):
        global _device
        return self.to(_device.main_device)
    
    def numpy(self):
        return torch.Tensor.numpy(super().detach().cpu())
    
    ### START METHOD AUTO GENERATION
    # operators
    def __add__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__add__'. The automatically generated codes are as follows:
            [ 1]: def __add__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__add__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__add__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__add__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__add__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__add__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__add__"
        return obj
    
    def __iadd__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__iadd__'. The automatically generated codes are as follows:
            [ 1]: def __iadd__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__iadd__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__iadd__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__iadd__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__iadd__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__iadd__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__iadd__"
        return obj
    
    def __sub__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__sub__'. The automatically generated codes are as follows:
            [ 1]: def __sub__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__sub__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__sub__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__sub__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__sub__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__sub__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__sub__"
        return obj
    
    def __isub__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__isub__'. The automatically generated codes are as follows:
            [ 1]: def __isub__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__isub__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__isub__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__isub__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__isub__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__isub__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__isub__"
        return obj
    
    def __mul__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__mul__'. The automatically generated codes are as follows:
            [ 1]: def __mul__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__mul__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__mul__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__mul__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__mul__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__mul__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__mul__"
        return obj
    
    def __imul__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__imul__'. The automatically generated codes are as follows:
            [ 1]: def __imul__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__imul__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__imul__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__imul__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__imul__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__imul__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__imul__"
        return obj
    
    def __div__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__div__'. The automatically generated codes are as follows:
            [ 1]: def __div__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__div__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__div__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__div__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__div__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__div__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__div__"
        return obj
    
    def __idiv__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__idiv__'. The automatically generated codes are as follows:
            [ 1]: def __idiv__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__idiv__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__idiv__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__idiv__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__idiv__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__idiv__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__idiv__"
        return obj
    
    def __pow__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__pow__'. The automatically generated codes are as follows:
            [ 1]: def __pow__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__pow__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__pow__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__pow__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__pow__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__pow__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__pow__"
        return obj
    
    def __ipow__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__ipow__'. The automatically generated codes are as follows:
            [ 1]: def __ipow__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__ipow__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__ipow__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__ipow__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__ipow__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__ipow__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__ipow__"
        return obj
    
    def __mod__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__mod__'. The automatically generated codes are as follows:
            [ 1]: def __mod__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__mod__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__mod__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__mod__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__mod__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__mod__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__mod__"
        return obj
    
    def __imod__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__imod__'. The automatically generated codes are as follows:
            [ 1]: def __imod__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__imod__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__imod__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__imod__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__imod__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__imod__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__imod__"
        return obj
    
    def __truediv__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__truediv__'. The automatically generated codes are as follows:
            [ 1]: def __truediv__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__truediv__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__truediv__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__truediv__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__truediv__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__truediv__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__truediv__"
        return obj
    
    def __itruediv__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__itruediv__'. The automatically generated codes are as follows:
            [ 1]: def __itruediv__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__itruediv__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__itruediv__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__itruediv__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__itruediv__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__itruediv__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__itruediv__"
        return obj
    
    def __floordiv__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__floordiv__'. The automatically generated codes are as follows:
            [ 1]: def __floordiv__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__floordiv__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__floordiv__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__floordiv__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__floordiv__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__floordiv__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__floordiv__"
        return obj
    
    def __ifloordiv__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__ifloordiv__'. The automatically generated codes are as follows:
            [ 1]: def __ifloordiv__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__ifloordiv__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__ifloordiv__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__ifloordiv__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__ifloordiv__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__ifloordiv__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__ifloordiv__"
        return obj
    
    def __neg__(self: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__neg__'. The automatically generated codes are as follows:
            [ 1]: def __neg__(self: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     self_shape=None if self is None else Size(self.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(self, '__neg__')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['__neg__'](self_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "__neg__"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            neg() -> Tensor
    
            See :func:`torch.neg`
    
            which is:
    
            neg(input, *, out=None) -> Tensor
    
            Returns a new tensor with the negative of the elements of :attr:`input`.
    
            .. math::
                \text{out} = -1 \times \text{input}
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.randn(5)
                >>> a
                tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])
                >>> torch.neg(a)
                tensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(self, '__neg__')(*auto_generated_args, )
        ref_shape = size_mapping['__neg__'](self_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__neg__"
        return obj
    
    def __eq__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__eq__'. The automatically generated codes are as follows:
            [ 1]: def __eq__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__eq__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__eq__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__eq__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__eq__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__eq__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__eq__"
        return obj
    
    def __ne__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__ne__'. The automatically generated codes are as follows:
            [ 1]: def __ne__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__ne__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__ne__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__ne__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__ne__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__ne__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__ne__"
        return obj
    
    def __or__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__or__'. The automatically generated codes are as follows:
            [ 1]: def __or__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__or__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__or__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__or__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__or__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__or__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__or__"
        return obj
    
    def __ior__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__ior__'. The automatically generated codes are as follows:
            [ 1]: def __ior__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__ior__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__ior__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__ior__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__ior__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__ior__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__ior__"
        return obj
    
    def __and__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__and__'. The automatically generated codes are as follows:
            [ 1]: def __and__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__and__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__and__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__and__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__and__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__and__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__and__"
        return obj
    
    def __iand__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__iand__'. The automatically generated codes are as follows:
            [ 1]: def __iand__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__iand__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__iand__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__iand__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__iand__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__iand__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__iand__"
        return obj
    
    def __xor__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__xor__'. The automatically generated codes are as follows:
            [ 1]: def __xor__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__xor__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__xor__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__xor__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__xor__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__xor__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__xor__"
        return obj
    
    def __ixor__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__ixor__'. The automatically generated codes are as follows:
            [ 1]: def __ixor__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__ixor__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__ixor__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__ixor__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__ixor__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__ixor__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__ixor__"
        return obj
    
    def __invert__(self: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__invert__'. The automatically generated codes are as follows:
            [ 1]: def __invert__(self: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     self_shape=None if self is None else Size(self.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(self, '__invert__')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['__invert__'](self_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "__invert__"
            [35]:     return obj
            [36]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(self, '__invert__')(*auto_generated_args, )
        ref_shape = size_mapping['__invert__'](self_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__invert__"
        return obj
    
    def __lt__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__lt__'. The automatically generated codes are as follows:
            [ 1]: def __lt__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__lt__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__lt__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__lt__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__lt__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__lt__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__lt__"
        return obj
    
    def __le__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__le__'. The automatically generated codes are as follows:
            [ 1]: def __le__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__le__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__le__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__le__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__le__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__le__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__le__"
        return obj
    
    def __gt__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__gt__'. The automatically generated codes are as follows:
            [ 1]: def __gt__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__gt__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__gt__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__gt__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__gt__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__gt__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__gt__"
        return obj
    
    def __ge__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__ge__'. The automatically generated codes are as follows:
            [ 1]: def __ge__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__ge__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__ge__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__ge__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__ge__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__ge__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__ge__"
        return obj
    
    def __matmul__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__matmul__'. The automatically generated codes are as follows:
            [ 1]: def __matmul__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['__matmul__'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, '__matmul__')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "__matmul__"
            [44]:     return obj
            [45]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['__matmul__'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, '__matmul__')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__matmul__"
        return obj
    
    # reversed operators
    def __radd__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__radd__'. The automatically generated codes are as follows:
            [ 1]: def __radd__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     torch_returned = False
            [29]:     self_shape, other_shape = self_shape ^ other_shape
            [30]:     self = self.view(self_shape)
            [31]:     other = other.view(other_shape)
            [32]:     ref_shape = self_shape
            [33]:     with torch._C.DisableTorchFunction():
            [34]:         obj = torch_super(other, '__add__')(self)
            [35]:     ref_shape, *_ = size_mapping_op['__radd__'](self_shape, other_shape)
            [36]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [37]:
            [38]:     def update_special(obj, updates):
            [39]:         if isinstance(obj, tuple):
            [40]:             for x in obj: update_special(x, updates)
            [41]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [42]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [43]:
            [44]:     if getattr(obj, 'grad_fn', None) is not None:
            [45]:         obj.grad_fn_name = "__radd__"
            [46]:     return obj
            [47]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        self_shape, other_shape = self_shape ^ other_shape
        self = self.view(self_shape)
        other = other.view(other_shape)
        ref_shape = self_shape
        with torch._C.DisableTorchFunction():
            obj = torch_super(other, '__add__')(self)
        ref_shape, *_ = size_mapping_op['__radd__'](self_shape, other_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__radd__"
        return obj
    
    def __rsub__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__rsub__'. The automatically generated codes are as follows:
            [ 1]: def __rsub__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     torch_returned = False
            [29]:     self_shape, other_shape = self_shape ^ other_shape
            [30]:     self = self.view(self_shape)
            [31]:     other = other.view(other_shape)
            [32]:     ref_shape = self_shape
            [33]:     with torch._C.DisableTorchFunction():
            [34]:         obj = torch_super(other, '__sub__')(self)
            [35]:     ref_shape, *_ = size_mapping_op['__rsub__'](self_shape, other_shape)
            [36]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [37]:
            [38]:     def update_special(obj, updates):
            [39]:         if isinstance(obj, tuple):
            [40]:             for x in obj: update_special(x, updates)
            [41]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [42]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [43]:
            [44]:     if getattr(obj, 'grad_fn', None) is not None:
            [45]:         obj.grad_fn_name = "__rsub__"
            [46]:     return obj
            [47]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        self_shape, other_shape = self_shape ^ other_shape
        self = self.view(self_shape)
        other = other.view(other_shape)
        ref_shape = self_shape
        with torch._C.DisableTorchFunction():
            obj = torch_super(other, '__sub__')(self)
        ref_shape, *_ = size_mapping_op['__rsub__'](self_shape, other_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__rsub__"
        return obj
    
    def __rmul__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__rmul__'. The automatically generated codes are as follows:
            [ 1]: def __rmul__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     torch_returned = False
            [29]:     self_shape, other_shape = self_shape ^ other_shape
            [30]:     self = self.view(self_shape)
            [31]:     other = other.view(other_shape)
            [32]:     ref_shape = self_shape
            [33]:     with torch._C.DisableTorchFunction():
            [34]:         obj = torch_super(other, '__mul__')(self)
            [35]:     ref_shape, *_ = size_mapping_op['__rmul__'](self_shape, other_shape)
            [36]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [37]:
            [38]:     def update_special(obj, updates):
            [39]:         if isinstance(obj, tuple):
            [40]:             for x in obj: update_special(x, updates)
            [41]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [42]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [43]:
            [44]:     if getattr(obj, 'grad_fn', None) is not None:
            [45]:         obj.grad_fn_name = "__rmul__"
            [46]:     return obj
            [47]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        self_shape, other_shape = self_shape ^ other_shape
        self = self.view(self_shape)
        other = other.view(other_shape)
        ref_shape = self_shape
        with torch._C.DisableTorchFunction():
            obj = torch_super(other, '__mul__')(self)
        ref_shape, *_ = size_mapping_op['__rmul__'](self_shape, other_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__rmul__"
        return obj
    
    def __rdiv__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__rdiv__'. The automatically generated codes are as follows:
            [ 1]: def __rdiv__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     torch_returned = False
            [29]:     self_shape, other_shape = self_shape ^ other_shape
            [30]:     self = self.view(self_shape)
            [31]:     other = other.view(other_shape)
            [32]:     ref_shape = self_shape
            [33]:     with torch._C.DisableTorchFunction():
            [34]:         obj = torch_super(other, '__div__')(self)
            [35]:     ref_shape, *_ = size_mapping_op['__rdiv__'](self_shape, other_shape)
            [36]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [37]:
            [38]:     def update_special(obj, updates):
            [39]:         if isinstance(obj, tuple):
            [40]:             for x in obj: update_special(x, updates)
            [41]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [42]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [43]:
            [44]:     if getattr(obj, 'grad_fn', None) is not None:
            [45]:         obj.grad_fn_name = "__rdiv__"
            [46]:     return obj
            [47]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        self_shape, other_shape = self_shape ^ other_shape
        self = self.view(self_shape)
        other = other.view(other_shape)
        ref_shape = self_shape
        with torch._C.DisableTorchFunction():
            obj = torch_super(other, '__div__')(self)
        ref_shape, *_ = size_mapping_op['__rdiv__'](self_shape, other_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__rdiv__"
        return obj
    
    def __rpow__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__rpow__'. The automatically generated codes are as follows:
            [ 1]: def __rpow__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     torch_returned = False
            [29]:     self_shape, other_shape = self_shape ^ other_shape
            [30]:     self = self.view(self_shape)
            [31]:     other = other.view(other_shape)
            [32]:     ref_shape = self_shape
            [33]:     with torch._C.DisableTorchFunction():
            [34]:         obj = torch_super(other, '__pow__')(self)
            [35]:     ref_shape, *_ = size_mapping_op['__rpow__'](self_shape, other_shape)
            [36]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [37]:
            [38]:     def update_special(obj, updates):
            [39]:         if isinstance(obj, tuple):
            [40]:             for x in obj: update_special(x, updates)
            [41]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [42]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [43]:
            [44]:     if getattr(obj, 'grad_fn', None) is not None:
            [45]:         obj.grad_fn_name = "__rpow__"
            [46]:     return obj
            [47]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        self_shape, other_shape = self_shape ^ other_shape
        self = self.view(self_shape)
        other = other.view(other_shape)
        ref_shape = self_shape
        with torch._C.DisableTorchFunction():
            obj = torch_super(other, '__pow__')(self)
        ref_shape, *_ = size_mapping_op['__rpow__'](self_shape, other_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__rpow__"
        return obj
    
    def __rmod__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__rmod__'. The automatically generated codes are as follows:
            [ 1]: def __rmod__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     torch_returned = False
            [29]:     self_shape, other_shape = self_shape ^ other_shape
            [30]:     self = self.view(self_shape)
            [31]:     other = other.view(other_shape)
            [32]:     ref_shape = self_shape
            [33]:     with torch._C.DisableTorchFunction():
            [34]:         obj = torch_super(other, '__mod__')(self)
            [35]:     ref_shape, *_ = size_mapping_op['__rmod__'](self_shape, other_shape)
            [36]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [37]:
            [38]:     def update_special(obj, updates):
            [39]:         if isinstance(obj, tuple):
            [40]:             for x in obj: update_special(x, updates)
            [41]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [42]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [43]:
            [44]:     if getattr(obj, 'grad_fn', None) is not None:
            [45]:         obj.grad_fn_name = "__rmod__"
            [46]:     return obj
            [47]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        self_shape, other_shape = self_shape ^ other_shape
        self = self.view(self_shape)
        other = other.view(other_shape)
        ref_shape = self_shape
        with torch._C.DisableTorchFunction():
            obj = torch_super(other, '__mod__')(self)
        ref_shape, *_ = size_mapping_op['__rmod__'](self_shape, other_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__rmod__"
        return obj
    
    def __rtruediv__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__rtruediv__'. The automatically generated codes are as follows:
            [ 1]: def __rtruediv__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     torch_returned = False
            [29]:     self_shape, other_shape = self_shape ^ other_shape
            [30]:     self = self.view(self_shape)
            [31]:     other = other.view(other_shape)
            [32]:     ref_shape = self_shape
            [33]:     with torch._C.DisableTorchFunction():
            [34]:         obj = torch_super(other, '__truediv__')(self)
            [35]:     ref_shape, *_ = size_mapping_op['__rtruediv__'](self_shape, other_shape)
            [36]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [37]:
            [38]:     def update_special(obj, updates):
            [39]:         if isinstance(obj, tuple):
            [40]:             for x in obj: update_special(x, updates)
            [41]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [42]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [43]:
            [44]:     if getattr(obj, 'grad_fn', None) is not None:
            [45]:         obj.grad_fn_name = "__rtruediv__"
            [46]:     return obj
            [47]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        self_shape, other_shape = self_shape ^ other_shape
        self = self.view(self_shape)
        other = other.view(other_shape)
        ref_shape = self_shape
        with torch._C.DisableTorchFunction():
            obj = torch_super(other, '__truediv__')(self)
        ref_shape, *_ = size_mapping_op['__rtruediv__'](self_shape, other_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__rtruediv__"
        return obj
    
    def __rfloordiv__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__rfloordiv__'. The automatically generated codes are as follows:
            [ 1]: def __rfloordiv__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     torch_returned = False
            [29]:     self_shape, other_shape = self_shape ^ other_shape
            [30]:     self = self.view(self_shape)
            [31]:     other = other.view(other_shape)
            [32]:     ref_shape = self_shape
            [33]:     with torch._C.DisableTorchFunction():
            [34]:         obj = torch_super(other, '__floordiv__')(self)
            [35]:     ref_shape, *_ = size_mapping_op['__rfloordiv__'](self_shape, other_shape)
            [36]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [37]:
            [38]:     def update_special(obj, updates):
            [39]:         if isinstance(obj, tuple):
            [40]:             for x in obj: update_special(x, updates)
            [41]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [42]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [43]:
            [44]:     if getattr(obj, 'grad_fn', None) is not None:
            [45]:         obj.grad_fn_name = "__rfloordiv__"
            [46]:     return obj
            [47]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        self_shape, other_shape = self_shape ^ other_shape
        self = self.view(self_shape)
        other = other.view(other_shape)
        ref_shape = self_shape
        with torch._C.DisableTorchFunction():
            obj = torch_super(other, '__floordiv__')(self)
        ref_shape, *_ = size_mapping_op['__rfloordiv__'](self_shape, other_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__rfloordiv__"
        return obj
    
    def __ror__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__ror__'. The automatically generated codes are as follows:
            [ 1]: def __ror__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     torch_returned = False
            [29]:     self_shape, other_shape = self_shape ^ other_shape
            [30]:     self = self.view(self_shape)
            [31]:     other = other.view(other_shape)
            [32]:     ref_shape = self_shape
            [33]:     with torch._C.DisableTorchFunction():
            [34]:         obj = torch_super(other, '__or__')(self)
            [35]:     ref_shape, *_ = size_mapping_op['__ror__'](self_shape, other_shape)
            [36]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [37]:
            [38]:     def update_special(obj, updates):
            [39]:         if isinstance(obj, tuple):
            [40]:             for x in obj: update_special(x, updates)
            [41]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [42]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [43]:
            [44]:     if getattr(obj, 'grad_fn', None) is not None:
            [45]:         obj.grad_fn_name = "__ror__"
            [46]:     return obj
            [47]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        self_shape, other_shape = self_shape ^ other_shape
        self = self.view(self_shape)
        other = other.view(other_shape)
        ref_shape = self_shape
        with torch._C.DisableTorchFunction():
            obj = torch_super(other, '__or__')(self)
        ref_shape, *_ = size_mapping_op['__ror__'](self_shape, other_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__ror__"
        return obj
    
    def __rand__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__rand__'. The automatically generated codes are as follows:
            [ 1]: def __rand__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     torch_returned = False
            [29]:     self_shape, other_shape = self_shape ^ other_shape
            [30]:     self = self.view(self_shape)
            [31]:     other = other.view(other_shape)
            [32]:     ref_shape = self_shape
            [33]:     with torch._C.DisableTorchFunction():
            [34]:         obj = torch_super(other, '__and__')(self)
            [35]:     ref_shape, *_ = size_mapping_op['__rand__'](self_shape, other_shape)
            [36]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [37]:
            [38]:     def update_special(obj, updates):
            [39]:         if isinstance(obj, tuple):
            [40]:             for x in obj: update_special(x, updates)
            [41]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [42]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [43]:
            [44]:     if getattr(obj, 'grad_fn', None) is not None:
            [45]:         obj.grad_fn_name = "__rand__"
            [46]:     return obj
            [47]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        self_shape, other_shape = self_shape ^ other_shape
        self = self.view(self_shape)
        other = other.view(other_shape)
        ref_shape = self_shape
        with torch._C.DisableTorchFunction():
            obj = torch_super(other, '__and__')(self)
        ref_shape, *_ = size_mapping_op['__rand__'](self_shape, other_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__rand__"
        return obj
    
    def __rxor__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__rxor__'. The automatically generated codes are as follows:
            [ 1]: def __rxor__(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     torch_returned = False
            [29]:     self_shape, other_shape = self_shape ^ other_shape
            [30]:     self = self.view(self_shape)
            [31]:     other = other.view(other_shape)
            [32]:     ref_shape = self_shape
            [33]:     with torch._C.DisableTorchFunction():
            [34]:         obj = torch_super(other, '__xor__')(self)
            [35]:     ref_shape, *_ = size_mapping_op['__rxor__'](self_shape, other_shape)
            [36]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [37]:
            [38]:     def update_special(obj, updates):
            [39]:         if isinstance(obj, tuple):
            [40]:             for x in obj: update_special(x, updates)
            [41]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [42]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [43]:
            [44]:     if getattr(obj, 'grad_fn', None) is not None:
            [45]:         obj.grad_fn_name = "__rxor__"
            [46]:     return obj
            [47]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        self_shape, other_shape = self_shape ^ other_shape
        self = self.view(self_shape)
        other = other.view(other_shape)
        ref_shape = self_shape
        with torch._C.DisableTorchFunction():
            obj = torch_super(other, '__xor__')(self)
        ref_shape, *_ = size_mapping_op['__rxor__'](self_shape, other_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__rxor__"
        return obj
    
    # inplace methods: operations
    def add_(self: 'Tensor', other: 'Tensor', *, alpha=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.add_'. The automatically generated codes are as follows:
            [ 1]: def add_(self: 'Tensor', other: 'Tensor', *, alpha=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['add'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         torch_super(self, 'add_')(*auto_generated_args, alpha=alpha)
            [34]:         obj = self
            [35]:     obj.special_from(ref_shape)
            [36]:
            [37]:     def update_special(obj, updates):
            [38]:         if isinstance(obj, tuple):
            [39]:             for x in obj: update_special(x, updates)
            [40]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [41]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [42]:
            [43]:     if getattr(obj, 'grad_fn', None) is not None:
            [44]:         obj.grad_fn_name = "add_"
            [45]:     return obj
            [46]:
            The document of the original function is:
    
            add_(other, *, alpha=1) -> Tensor
    
            In-place version of :meth:`~Tensor.add`
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['add'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            torch_super(self, 'add_')(*auto_generated_args, alpha=alpha)
            obj = self
        obj.special_from(ref_shape)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "add_"
        return obj
    
    def sub_(self: 'Tensor', other: 'Tensor', *, alpha=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.sub_'. The automatically generated codes are as follows:
            [ 1]: def sub_(self: 'Tensor', other: 'Tensor', *, alpha=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['sub'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         torch_super(self, 'sub_')(*auto_generated_args, alpha=alpha)
            [34]:         obj = self
            [35]:     obj.special_from(ref_shape)
            [36]:
            [37]:     def update_special(obj, updates):
            [38]:         if isinstance(obj, tuple):
            [39]:             for x in obj: update_special(x, updates)
            [40]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [41]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [42]:
            [43]:     if getattr(obj, 'grad_fn', None) is not None:
            [44]:         obj.grad_fn_name = "sub_"
            [45]:     return obj
            [46]:
            The document of the original function is:
    
            sub_(other, *, alpha=1) -> Tensor
    
            In-place version of :meth:`~Tensor.sub`
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['sub'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            torch_super(self, 'sub_')(*auto_generated_args, alpha=alpha)
            obj = self
        obj.special_from(ref_shape)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "sub_"
        return obj
    
    def multiply(self: 'Tensor', value: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.multiply'. The automatically generated codes are as follows:
            [ 1]: def multiply(self: 'Tensor', value: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, value]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if value is None: ...
            [19]:     elif not isinstance(value, torch.Tensor): value = torch.tensor(value)
            [20]:     if not isinstance(value, subclass): value = value.as_subclass(subclass).special_from(value.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if value.device.type == 'cpu': value = value.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     value_shape=None if value is None else Size(value.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, value_shape = size_mapping_op['multiply'](self_shape, value_shape)
            [29]:     self = self.view(self_shape)
            [30]:     value = value.view(value_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [value] if x is not None)
            [33]:         obj = torch_super(self, 'multiply')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "multiply"
            [44]:     return obj
            [45]:
            The document of the original function is:
    
            multiply(value) -> Tensor
    
            See :func:`torch.multiply`.
    
            which is:
    
            multiply(input, other, *, out=None)
    
            Alias for :func:`torch.mul`.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, value]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if value is None: ...
        elif not isinstance(value, torch.Tensor): value = torch.tensor(value)
        if not isinstance(value, subclass): value = value.as_subclass(subclass).special_from(value.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if value.device.type == 'cpu': value = value.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        value_shape=None if value is None else Size(value.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, value_shape = size_mapping_op['multiply'](self_shape, value_shape)
        self = self.view(self_shape)
        value = value.view(value_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [value] if x is not None)
            obj = torch_super(self, 'multiply')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "multiply"
        return obj
    
    def mul_(self: 'Tensor', value: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.mul_'. The automatically generated codes are as follows:
            [ 1]: def mul_(self: 'Tensor', value: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, value]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if value is None: ...
            [19]:     elif not isinstance(value, torch.Tensor): value = torch.tensor(value)
            [20]:     if not isinstance(value, subclass): value = value.as_subclass(subclass).special_from(value.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if value.device.type == 'cpu': value = value.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     value_shape=None if value is None else Size(value.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, value_shape = size_mapping_op['mul'](self_shape, value_shape)
            [29]:     self = self.view(self_shape)
            [30]:     value = value.view(value_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [value] if x is not None)
            [33]:         torch_super(self, 'mul_')(*auto_generated_args, )
            [34]:         obj = self
            [35]:     obj.special_from(ref_shape)
            [36]:
            [37]:     def update_special(obj, updates):
            [38]:         if isinstance(obj, tuple):
            [39]:             for x in obj: update_special(x, updates)
            [40]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [41]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [42]:
            [43]:     if getattr(obj, 'grad_fn', None) is not None:
            [44]:         obj.grad_fn_name = "mul_"
            [45]:     return obj
            [46]:
            The document of the original function is:
    
            mul_(value) -> Tensor
    
            In-place version of :meth:`~Tensor.mul`.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, value]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if value is None: ...
        elif not isinstance(value, torch.Tensor): value = torch.tensor(value)
        if not isinstance(value, subclass): value = value.as_subclass(subclass).special_from(value.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if value.device.type == 'cpu': value = value.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        value_shape=None if value is None else Size(value.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, value_shape = size_mapping_op['mul'](self_shape, value_shape)
        self = self.view(self_shape)
        value = value.view(value_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [value] if x is not None)
            torch_super(self, 'mul_')(*auto_generated_args, )
            obj = self
        obj.special_from(ref_shape)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "mul_"
        return obj
    
    def div_(self: 'Tensor', value: 'Tensor', *, rounding_mode=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.div_'. The automatically generated codes are as follows:
            [ 1]: def div_(self: 'Tensor', value: 'Tensor', *, rounding_mode=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, value]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if value is None: ...
            [19]:     elif not isinstance(value, torch.Tensor): value = torch.tensor(value)
            [20]:     if not isinstance(value, subclass): value = value.as_subclass(subclass).special_from(value.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if value.device.type == 'cpu': value = value.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     value_shape=None if value is None else Size(value.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, value_shape = size_mapping_op['div'](self_shape, value_shape)
            [29]:     self = self.view(self_shape)
            [30]:     value = value.view(value_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [value] if x is not None)
            [33]:         torch_super(self, 'div_')(*auto_generated_args, rounding_mode=rounding_mode)
            [34]:         obj = self
            [35]:     obj.special_from(ref_shape)
            [36]:
            [37]:     def update_special(obj, updates):
            [38]:         if isinstance(obj, tuple):
            [39]:             for x in obj: update_special(x, updates)
            [40]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [41]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [42]:
            [43]:     if getattr(obj, 'grad_fn', None) is not None:
            [44]:         obj.grad_fn_name = "div_"
            [45]:     return obj
            [46]:
            The document of the original function is:
    
            div_(value, *, rounding_mode=None) -> Tensor
    
            In-place version of :meth:`~Tensor.div`
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, value]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if value is None: ...
        elif not isinstance(value, torch.Tensor): value = torch.tensor(value)
        if not isinstance(value, subclass): value = value.as_subclass(subclass).special_from(value.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if value.device.type == 'cpu': value = value.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        value_shape=None if value is None else Size(value.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, value_shape = size_mapping_op['div'](self_shape, value_shape)
        self = self.view(self_shape)
        value = value.view(value_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [value] if x is not None)
            torch_super(self, 'div_')(*auto_generated_args, rounding_mode=rounding_mode)
            obj = self
        obj.special_from(ref_shape)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "div_"
        return obj
    
    def pow_(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.pow_'. The automatically generated codes are as follows:
            [ 1]: def pow_(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['pow'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         torch_super(self, 'pow_')(*auto_generated_args, )
            [34]:         obj = self
            [35]:     obj.special_from(ref_shape)
            [36]:
            [37]:     def update_special(obj, updates):
            [38]:         if isinstance(obj, tuple):
            [39]:             for x in obj: update_special(x, updates)
            [40]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [41]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [42]:
            [43]:     if getattr(obj, 'grad_fn', None) is not None:
            [44]:         obj.grad_fn_name = "pow_"
            [45]:     return obj
            [46]:
            The document of the original function is:
    
            pow_(exponent) -> Tensor
    
            In-place version of :meth:`~Tensor.pow`
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['pow'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            torch_super(self, 'pow_')(*auto_generated_args, )
            obj = self
        obj.special_from(ref_shape)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "pow_"
        return obj
    
    def fmod_(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.fmod_'. The automatically generated codes are as follows:
            [ 1]: def fmod_(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['fmod'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         torch_super(self, 'fmod_')(*auto_generated_args, )
            [34]:         obj = self
            [35]:     obj.special_from(ref_shape)
            [36]:
            [37]:     def update_special(obj, updates):
            [38]:         if isinstance(obj, tuple):
            [39]:             for x in obj: update_special(x, updates)
            [40]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [41]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [42]:
            [43]:     if getattr(obj, 'grad_fn', None) is not None:
            [44]:         obj.grad_fn_name = "fmod_"
            [45]:     return obj
            [46]:
            The document of the original function is:
    
            fmod_(divisor) -> Tensor
    
            In-place version of :meth:`~Tensor.fmod`
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['fmod'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            torch_super(self, 'fmod_')(*auto_generated_args, )
            obj = self
        obj.special_from(ref_shape)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "fmod_"
        return obj
    
    # inplace methods: initializers
    def zero_(self: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.zero_'. The automatically generated codes are as follows:
            [ 1]: def zero_(self: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     self_shape=None if self is None else Size(self.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         torch_super(self, 'zero_')(*auto_generated_args, )
            [24]:         obj = self
            [25]:     ref_shape = size_mapping['zero'](self_shape)
            [26]:     obj.special_from(ref_shape)
            [27]:
            [28]:     def update_special(obj, updates):
            [29]:         if isinstance(obj, tuple):
            [30]:             for x in obj: update_special(x, updates)
            [31]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [32]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [33]:
            [34]:     if getattr(obj, 'grad_fn', None) is not None:
            [35]:         obj.grad_fn_name = "zero_"
            [36]:     return obj
            [37]:
            The document of the original function is:
    
            zero_() -> Tensor
    
            Fills :attr:`self` tensor with zeros.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            torch_super(self, 'zero_')(*auto_generated_args, )
            obj = self
        ref_shape = size_mapping['zero'](self_shape)
        obj.special_from(ref_shape)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "zero_"
        return obj
    
    def one_(self, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.one_'. The automatically generated codes are as follows:
            [ 1]: def one_(self, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in []:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     # Obtain available sized in arguments (which will be fed into size function).
            [13]:
            [14]:     # Use the given inner codes if they are provided.
            [15]:     torch_returned = False
            [16]:     obj = self.fill_(1) # suppress:  special_from
            [17]:
            [18]:     def update_special(obj, updates):
            [19]:         if isinstance(obj, tuple):
            [20]:             for x in obj: update_special(x, updates)
            [21]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [22]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [23]:
            [24]:     if getattr(obj, 'grad_fn', None) is not None:
            [25]:         obj.grad_fn_name = "one_"
            [26]:     return obj
            [27]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in []:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        # Obtain available sized in arguments (which will be fed into size function).
    
        # Use the given inner codes if they are provided.
        torch_returned = False
        obj = self.fill_(1) # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "one_"
        return obj
    
    def fill_(self: 'Tensor', value, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.fill_'. The automatically generated codes are as follows:
            [ 1]: def fill_(self: 'Tensor', value, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     self_shape=None if self is None else Size(self.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [value] if x is not None)
            [23]:         torch_super(self, 'fill_')(*auto_generated_args, )
            [24]:         obj = self
            [25]:     ref_shape = size_mapping['fill'](self_shape)
            [26]:     obj.special_from(ref_shape)
            [27]:
            [28]:     def update_special(obj, updates):
            [29]:         if isinstance(obj, tuple):
            [30]:             for x in obj: update_special(x, updates)
            [31]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [32]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [33]:
            [34]:     if getattr(obj, 'grad_fn', None) is not None:
            [35]:         obj.grad_fn_name = "fill_"
            [36]:     return obj
            [37]:
            The document of the original function is:
    
            fill_(value) -> Tensor
    
            Fills :attr:`self` tensor with the specified value.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [value] if x is not None)
            torch_super(self, 'fill_')(*auto_generated_args, )
            obj = self
        ref_shape = size_mapping['fill'](self_shape)
        obj.special_from(ref_shape)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "fill_"
        return obj
    
    def normal_(self: 'Tensor', mean=0, std=1, *, generator=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.normal_'. The automatically generated codes are as follows:
            [ 1]: def normal_(self: 'Tensor', mean=0, std=1, *, generator=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     self_shape=None if self is None else Size(self.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         torch_super(self, 'normal_')(*auto_generated_args, mean=mean,std=std,generator=generator)
            [24]:         obj = self
            [25]:     ref_shape = size_mapping['normal'](self_shape)
            [26]:     obj.special_from(ref_shape)
            [27]:
            [28]:     def update_special(obj, updates):
            [29]:         if isinstance(obj, tuple):
            [30]:             for x in obj: update_special(x, updates)
            [31]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [32]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [33]:
            [34]:     if getattr(obj, 'grad_fn', None) is not None:
            [35]:         obj.grad_fn_name = "normal_"
            [36]:     return obj
            [37]:
            The document of the original function is:
    
            normal_(mean=0, std=1, *, generator=None) -> Tensor
    
            Fills :attr:`self` tensor with elements samples from the normal distribution
            parameterized by :attr:`mean` and :attr:`std`.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            torch_super(self, 'normal_')(*auto_generated_args, mean=mean,std=std,generator=generator)
            obj = self
        ref_shape = size_mapping['normal'](self_shape)
        obj.special_from(ref_shape)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "normal_"
        return obj
    
    # inplace methods: dimension manipulations
    def unsqueeze_(self: 'Tensor', *dims: new_dim[...], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.unsqueeze_'. The automatically generated codes are as follows:
            [ 1]: def unsqueeze_(self: 'Tensor', *dims: new_dim[...], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     dims = new_dim[...](self, *dims)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     self_shape=None if self is None else Size(self.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     with torch._C.DisableTorchFunction():
            [24]:         for dims_iter, in zip(dims):
            [25]:             auto_generated_args = tuple(x for x in [] if x is not None)
            [26]:             torch_super(self, 'unsqueeze_')(*auto_generated_args, dims_iter)
            [27]:     obj = self
            [28]:     ref_shape = size_mapping['unsqueeze'](self_shape, dims)
            [29]:     obj.special_from(ref_shape)
            [30]:
            [31]:     def update_special(obj, updates):
            [32]:         if isinstance(obj, tuple):
            [33]:             for x in obj: update_special(x, updates)
            [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [36]:
            [37]:     if getattr(obj, 'grad_fn', None) is not None:
            [38]:         obj.grad_fn_name = "unsqueeze_"
            [39]:     return obj
            [40]:
            The document of the original function is:
    
            unsqueeze_(dim) -> Tensor
    
            In-place version of :meth:`~Tensor.unsqueeze`
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        dims = new_dim[...](self, *dims)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            for dims_iter, in zip(dims):
                auto_generated_args = tuple(x for x in [] if x is not None)
                torch_super(self, 'unsqueeze_')(*auto_generated_args, dims_iter)
        obj = self
        ref_shape = size_mapping['unsqueeze'](self_shape, dims)
        obj.special_from(ref_shape)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "unsqueeze_"
        return obj
    
    def squeeze_(self: 'Tensor', *dims: del_dim[...], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.squeeze_'. The automatically generated codes are as follows:
            [ 1]: def squeeze_(self: 'Tensor', *dims: del_dim[...], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     dims = del_dim[...](self, *dims)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     self_shape=None if self is None else Size(self.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     torch_returned = False
            [24]:     valid_dims = []
            [25]:     with torch._C.DisableTorchFunction():
            [26]:         for d in dims[::-1]:
            [27]:             if self.size(d) == 1:
            [28]:                 valid_dims.append(d)
            [29]:                 torch_super(self, 'squeeze_')(d)
            [30]:     dims = tuple(valid_dims)
            [31]:     obj = self
            [32]:     ref_shape = size_mapping['squeeze'](self_shape, dims)
            [33]:     obj.special_from(ref_shape)
            [34]:
            [35]:     def update_special(obj, updates):
            [36]:         if isinstance(obj, tuple):
            [37]:             for x in obj: update_special(x, updates)
            [38]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [39]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [40]:
            [41]:     if getattr(obj, 'grad_fn', None) is not None:
            [42]:         obj.grad_fn_name = "squeeze_"
            [43]:     return obj
            [44]:
            The document of the original function is:
    
            squeeze_(dim=None) -> Tensor
    
            In-place version of :meth:`~Tensor.squeeze`
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        dims = del_dim[...](self, *dims)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        valid_dims = []
        with torch._C.DisableTorchFunction():
            for d in dims[::-1]:
                if self.size(d) == 1:
                    valid_dims.append(d)
                    torch_super(self, 'squeeze_')(d)
        dims = tuple(valid_dims)
        obj = self
        ref_shape = size_mapping['squeeze'](self_shape, dims)
        obj.special_from(ref_shape)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "squeeze_"
        return obj
    
    def transpose_(self: 'Tensor', dim0: exist_dim[1], dim1: exist_dim[1], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.transpose_'. The automatically generated codes are as follows:
            [ 1]: def transpose_(self: 'Tensor', dim0: exist_dim[1], dim1: exist_dim[1], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     dim0 = exist_dim[1](self, dim0)
            [19]:     dim1 = exist_dim[1](self, dim1)
            [20]:
            [21]:     # Obtain available sized in arguments (which will be fed into size function).
            [22]:     self_shape=None if self is None else Size(self.shape)
            [23]:     # Use the given inner codes if they are provided.
            [24]:     with torch._C.DisableTorchFunction():
            [25]:         for dim0_iter, dim1_iter in zip(dim0, dim1):
            [26]:             auto_generated_args = tuple(x for x in [dim0_iter,dim1_iter] if x is not None)
            [27]:             torch_super(self, 'transpose_')(*auto_generated_args, )
            [28]:     obj = self
            [29]:     ref_shape = size_mapping['transpose'](self_shape, dim0, dim1)
            [30]:     obj.special_from(ref_shape)
            [31]:
            [32]:     def update_special(obj, updates):
            [33]:         if isinstance(obj, tuple):
            [34]:             for x in obj: update_special(x, updates)
            [35]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [36]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [37]:
            [38]:     if getattr(obj, 'grad_fn', None) is not None:
            [39]:         obj.grad_fn_name = "transpose_"
            [40]:     return obj
            [41]:
            The document of the original function is:
    
            transpose_(dim0, dim1) -> Tensor
    
            In-place version of :meth:`~Tensor.transpose`
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        dim0 = exist_dim[1](self, dim0)
        dim1 = exist_dim[1](self, dim1)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            for dim0_iter, dim1_iter in zip(dim0, dim1):
                auto_generated_args = tuple(x for x in [dim0_iter,dim1_iter] if x is not None)
                torch_super(self, 'transpose_')(*auto_generated_args, )
        obj = self
        ref_shape = size_mapping['transpose'](self_shape, dim0, dim1)
        obj.special_from(ref_shape)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "transpose_"
        return obj
    
    # properties
    @collect_memory
    def to(self: 'Tensor', *args, **kwargs):
        """
    
        Automatically inheritted method from 'torch.Tensor.to'. The automatically generated codes are as follows:
            [ 1]: @collect_memory
            [ 2]: def to(self: 'Tensor', *args, **kwargs):
            [ 3]:     '''
            [ 4]:         ***
            [ 5]:     '''
            [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 7]:
            [ 8]:     pivot = None
            [ 9]:     for t in [self]:
            [10]:         if isinstance(t, torch.Tensor): pivot = t; break
            [11]:     subclass = Tensor.get_tensor_subclass(pivot)
            [12]:
            [13]:     if self is None: ...
            [14]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [15]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [17]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [18]:
            [19]:     # Obtain available sized in arguments (which will be fed into size function).
            [20]:     self_shape=None if self is None else Size(self.shape)
            [21]:     # Use the given inner codes if they are provided.
            [22]:     with torch._C.DisableTorchFunction():
            [23]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [24]:         obj = torch_super(self, 'to')(*auto_generated_args, *args,**kwargs)
            [25]:     ref_shape = size_mapping['to'](self_shape)
            [26]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [27]:
            [28]:     def update_special(obj, updates):
            [29]:         if isinstance(obj, tuple):
            [30]:             for x in obj: update_special(x, updates)
            [31]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [32]:     update_special(obj, {k: v for k, v in dict(function_dim=kwargs.get('function_dim', None), batch_dim=kwargs.get('batch_dim', None), sequence_dim=kwargs.get('sequence_dim', None), feature_dim=kwargs.get('feature_dim', None)).items() if v != None})
            [33]:
            [34]:     if getattr(obj, 'grad_fn', None) is not None:
            [35]:         obj.grad_fn_name = "to"
            [36]:     return obj
            [37]:
            The document of the original function is:
    
            to(*args, **kwargs) -> Tensor
    
            Performs Tensor dtype and/or device conversion. A :class:`torch.dtype` and :class:`torch.device` are
            inferred from the arguments of ``self.to(*args, **kwargs)``.
    
            .. note::
    
                If the ``self`` Tensor already
                has the correct :class:`torch.dtype` and :class:`torch.device`, then ``self`` is returned.
                Otherwise, the returned tensor is a copy of ``self`` with the desired
                :class:`torch.dtype` and :class:`torch.device`.
    
            Here are the ways to call ``to``:
    
            .. method:: to(dtype, non_blocking=False, copy=False, memory_format=torch.preserve_format) -> Tensor
               :noindex:
    
                Returns a Tensor with the specified :attr:`dtype`
    
                Args:
                    memory_format (:class:`torch.memory_format`, optional): the desired memory format of
                    returned Tensor. Default: ``torch.preserve_format``.
    
            .. method:: to(device=None, dtype=None, non_blocking=False, copy=False, memory_format=torch.preserve_format) -> Tensor
               :noindex:
    
                Returns a Tensor with the specified :attr:`device` and (optional)
                :attr:`dtype`. If :attr:`dtype` is ``None`` it is inferred to be ``self.dtype``.
                When :attr:`non_blocking`, tries to convert asynchronously with respect to
                the host if possible, e.g., converting a CPU Tensor with pinned memory to a
                CUDA Tensor.
                When :attr:`copy` is set, a new Tensor is created even when the Tensor
                already matches the desired conversion.
    
                Args:
                    memory_format (:class:`torch.memory_format`, optional): the desired memory format of
                    returned Tensor. Default: ``torch.preserve_format``.
    
            .. method:: to(other, non_blocking=False, copy=False) -> Tensor
               :noindex:
    
                Returns a Tensor with same :class:`torch.dtype` and :class:`torch.device` as
                the Tensor :attr:`other`. When :attr:`non_blocking`, tries to convert
                asynchronously with respect to the host if possible, e.g., converting a CPU
                Tensor with pinned memory to a CUDA Tensor.
                When :attr:`copy` is set, a new Tensor is created even when the Tensor
                already matches the desired conversion.
    
            Example::
    
                >>> tensor = torch.randn(2, 2)  # Initially dtype=float32, device=cpu
                >>> tensor.to(torch.float64)
                tensor([[-0.5044,  0.0005],
                        [ 0.3310, -0.0584]], dtype=torch.float64)
    
                >>> cuda0 = torch.device('cuda:0')
                >>> tensor.to(cuda0)
                tensor([[-0.5044,  0.0005],
                        [ 0.3310, -0.0584]], device='cuda:0')
    
                >>> tensor.to(cuda0, dtype=torch.float64)
                tensor([[-0.5044,  0.0005],
                        [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')
    
                >>> other = torch.randn((), dtype=torch.float64, device=cuda0)
                >>> tensor.to(other, non_blocking=True)
                tensor([[-0.5044,  0.0005],
                        [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(self, 'to')(*auto_generated_args, *args,**kwargs)
        ref_shape = size_mapping['to'](self_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=kwargs.get('function_dim', None), batch_dim=kwargs.get('batch_dim', None), sequence_dim=kwargs.get('sequence_dim', None), feature_dim=kwargs.get('feature_dim', None)).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "to"
        return obj
    
    def clone(self: 'Tensor', *, memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.clone'. The automatically generated codes are as follows:
            [ 1]: def clone(self: 'Tensor', *, memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     self_shape=None if self is None else Size(self.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(self, 'clone')(*auto_generated_args, memory_format=memory_format)
            [24]:     ref_shape = size_mapping['clone'](self_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "clone"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            clone(*, memory_format=torch.preserve_format) -> Tensor
    
            See :func:`torch.clone`
    
            which is:
    
            clone(input, *, memory_format=torch.preserve_format) -> Tensor
    
            Returns a copy of :attr:`input`.
    
            .. note::
    
                This function is differentiable, so gradients will flow back from the
                result of this operation to :attr:`input`. To create a tensor without an
                autograd relationship to :attr:`input` see :meth:`~Tensor.detach`.
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                memory_format (:class:`torch.memory_format`, optional): the desired memory format of
                    returned tensor. Default: ``torch.preserve_format``.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(self, 'clone')(*auto_generated_args, memory_format=memory_format)
        ref_shape = size_mapping['clone'](self_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "clone"
        return obj
    
    def int(self: 'Tensor', memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.int'. The automatically generated codes are as follows:
            [ 1]: def int(self: 'Tensor', memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     self_shape=None if self is None else Size(self.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(self, 'int')(*auto_generated_args, memory_format=memory_format)
            [24]:     ref_shape = size_mapping['int'](self_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "int"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            int(memory_format=torch.preserve_format) -> Tensor
    
            ``self.int()`` is equivalent to ``self.to(torch.int32)``. See :func:`to`.
    
            Args:
                memory_format (:class:`torch.memory_format`, optional): the desired memory format of
                    returned Tensor. Default: ``torch.preserve_format``.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(self, 'int')(*auto_generated_args, memory_format=memory_format)
        ref_shape = size_mapping['int'](self_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "int"
        return obj
    
    def long(self: 'Tensor', memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.long'. The automatically generated codes are as follows:
            [ 1]: def long(self: 'Tensor', memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     self_shape=None if self is None else Size(self.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(self, 'long')(*auto_generated_args, memory_format=memory_format)
            [24]:     ref_shape = size_mapping['long'](self_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "long"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            long(memory_format=torch.preserve_format) -> Tensor
    
            ``self.long()`` is equivalent to ``self.to(torch.int64)``. See :func:`to`.
    
            Args:
                memory_format (:class:`torch.memory_format`, optional): the desired memory format of
                    returned Tensor. Default: ``torch.preserve_format``.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(self, 'long')(*auto_generated_args, memory_format=memory_format)
        ref_shape = size_mapping['long'](self_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "long"
        return obj
    
    def float(self: 'Tensor', memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.float'. The automatically generated codes are as follows:
            [ 1]: def float(self: 'Tensor', memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     self_shape=None if self is None else Size(self.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(self, 'float')(*auto_generated_args, memory_format=memory_format)
            [24]:     ref_shape = size_mapping['float'](self_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "float"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            float(memory_format=torch.preserve_format) -> Tensor
    
            ``self.float()`` is equivalent to ``self.to(torch.float32)``. See :func:`to`.
    
            Args:
                memory_format (:class:`torch.memory_format`, optional): the desired memory format of
                    returned Tensor. Default: ``torch.preserve_format``.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(self, 'float')(*auto_generated_args, memory_format=memory_format)
        ref_shape = size_mapping['float'](self_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "float"
        return obj
    
    def double(self: 'Tensor', memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.double'. The automatically generated codes are as follows:
            [ 1]: def double(self: 'Tensor', memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     self_shape=None if self is None else Size(self.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(self, 'double')(*auto_generated_args, memory_format=memory_format)
            [24]:     ref_shape = size_mapping['double'](self_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "double"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            double(memory_format=torch.preserve_format) -> Tensor
    
            ``self.double()`` is equivalent to ``self.to(torch.float64)``. See :func:`to`.
    
            Args:
                memory_format (:class:`torch.memory_format`, optional): the desired memory format of
                    returned Tensor. Default: ``torch.preserve_format``.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(self, 'double')(*auto_generated_args, memory_format=memory_format)
        ref_shape = size_mapping['double'](self_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "double"
        return obj
    
    def cpu(self: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.cpu'. The automatically generated codes are as follows:
            [ 1]: def cpu(self: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     self_shape=None if self is None else Size(self.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(self, 'cpu')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['cpu'](self_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "cpu"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            cpu(memory_format=torch.preserve_format) -> Tensor
    
            Returns a copy of this object in CPU memory.
    
            If this object is already in CPU memory and on the correct device,
            then no copy is performed and the original object is returned.
    
            Args:
                memory_format (:class:`torch.memory_format`, optional): the desired memory format of
                    returned Tensor. Default: ``torch.preserve_format``.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(self, 'cpu')(*auto_generated_args, )
        ref_shape = size_mapping['cpu'](self_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "cpu"
        return obj
    
    def cuda(self: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.cuda'. The automatically generated codes are as follows:
            [ 1]: def cuda(self: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     self_shape=None if self is None else Size(self.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(self, 'cuda')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['cuda'](self_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "cuda"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            cuda(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor
    
            Returns a copy of this object in CUDA memory.
    
            If this object is already in CUDA memory and on the correct device,
            then no copy is performed and the original object is returned.
    
            Args:
                device (:class:`torch.device`): The destination GPU device.
                    Defaults to the current CUDA device.
                non_blocking (bool): If ``True`` and the source is in pinned memory,
                    the copy will be asynchronous with respect to the host.
                    Otherwise, the argument has no effect. Default: ``False``.
                memory_format (:class:`torch.memory_format`, optional): the desired memory format of
                    returned Tensor. Default: ``torch.preserve_format``.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(self, 'cuda')(*auto_generated_args, )
        ref_shape = size_mapping['cuda'](self_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "cuda"
        return obj
    
    # shapes:
    def reshape(self, *size: Size, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.reshape'. The automatically generated codes are as follows:
            [ 1]: def reshape(self, *size: Size, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in []:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     # Obtain available sized in arguments (which will be fed into size function).
            [13]:     size=Size(*size)
            [14]:     # Use the given inner codes if they are provided.
            [15]:     with torch._C.DisableTorchFunction():
            [16]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [17]:         obj = torch_super(self, 'reshape')(*auto_generated_args, *size)
            [18]:     ref_shape = size_mapping['reshape'](size)
            [19]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [20]:
            [21]:     def update_special(obj, updates):
            [22]:         if isinstance(obj, tuple):
            [23]:             for x in obj: update_special(x, updates)
            [24]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [25]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [26]:
            [27]:     if getattr(obj, 'grad_fn', None) is not None:
            [28]:         obj.grad_fn_name = "reshape"
            [29]:     return obj
            [30]:
            The document of the original function is:
    
            reshape(*shape) -> Tensor
    
            Returns a tensor with the same data and number of elements as :attr:`self`
            but with the specified shape. This method returns a view if :attr:`shape` is
            compatible with the current shape. See :meth:`torch.Tensor.view` on when it is
            possible to return a view.
    
            See :func:`torch.reshape`
    
            Args:
                shape (tuple of ints or int...): the desired shape
    
            which is:
    
            reshape(input, shape) -> Tensor
    
            Returns a tensor with the same data and number of elements as :attr:`input`,
            but with the specified shape. When possible, the returned tensor will be a view
            of :attr:`input`. Otherwise, it will be a copy. Contiguous inputs and inputs
            with compatible strides can be reshaped without copying, but you should not
            depend on the copying vs. viewing behavior.
    
            See :meth:`torch.Tensor.view` on when it is possible to return a view.
    
            A single dimension may be -1, in which case it's inferred from the remaining
            dimensions and the number of elements in :attr:`input`.
    
            Args:
                input (Tensor): the tensor to be reshaped
                shape (tuple of int): the new shape
    
            Example::
    
                >>> a = torch.arange(4.)
                >>> torch.reshape(a, (2, 2))
                tensor([[ 0.,  1.],
                        [ 2.,  3.]])
                >>> b = torch.tensor([[0, 1], [2, 3]])
                >>> torch.reshape(b, (-1,))
                tensor([ 0,  1,  2,  3])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in []:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        # Obtain available sized in arguments (which will be fed into size function).
        size=Size(*size)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(self, 'reshape')(*auto_generated_args, *size)
        ref_shape = size_mapping['reshape'](size)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "reshape"
        return obj
    
    def reshape_as(self, other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.reshape_as'. The automatically generated codes are as follows:
            [ 1]: def reshape_as(self, other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if other is None: ...
            [13]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [14]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     other_shape=None if other is None else Size(other.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [23]:         obj = torch_super(self, 'reshape_as')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['reshape_as'](other_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "reshape_as"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            reshape_as(other) -> Tensor
    
            Returns this tensor as the same shape as :attr:`other`.
            ``self.reshape_as(other)`` is equivalent to ``self.reshape(other.sizes())``.
            This method returns a view if ``other.sizes()`` is compatible with the current
            shape. See :meth:`torch.Tensor.view` on when it is possible to return a view.
    
            Please see :meth:`reshape` for more information about ``reshape``.
    
            Args:
                other (:class:`torch.Tensor`): The result tensor has the same shape
                    as :attr:`other`.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, 'reshape_as')(*auto_generated_args, )
        ref_shape = size_mapping['reshape_as'](other_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "reshape_as"
        return obj
    
    def view(self, *size, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.view'. The automatically generated codes are as follows:
            [ 1]: def view(self, *size, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in []:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     # Obtain available sized in arguments (which will be fed into size function).
            [13]:
            [14]:     # Use the given inner codes if they are provided.
            [15]:     torch_returned = False
            [16]:     if len(size) == 1 and isinstance(size[0], tuple):  size = size[0]
            [17]:     if ... in size:
            [18]:         i = size.index(...)
            [19]:         assert ... not in size[i+1:], "Only one ellipsis is allowed in view shape. "
            [20]:         size_l = Size(size[:i])
            [21]:         size_r = Size(size[i+1:])
            [22]:         size = size_l + self.shape[len(size_l):self.n_dim-len(size_r)] + size_r
            [23]:     else:  size = Size(size)
            [24]:     with torch._C.DisableTorchFunction():
            [25]:         obj = Tensor.inherit_from(torch_super(self, 'view')(size), self, shape=size)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "view"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            view(*shape) -> Tensor
    
            Returns a new tensor with the same data as the :attr:`self` tensor but of a
            different :attr:`shape`.
    
            The returned tensor shares the same data and must have the same number
            of elements, but may have a different size. For a tensor to be viewed, the new
            view size must be compatible with its original size and stride, i.e., each new
            view dimension must either be a subspace of an original dimension, or only span
            across original dimensions :math:`d, d+1, \dots, d+k` that satisfy the following
            contiguity-like condition that :math:`\forall i = d, \dots, d+k-1`,
    
            .. math::
    
              \text{stride}[i] = \text{stride}[i+1] \times \text{size}[i+1]
    
            Otherwise, it will not be possible to view :attr:`self` tensor as :attr:`shape`
            without copying it (e.g., via :meth:`contiguous`). When it is unclear whether a
            :meth:`view` can be performed, it is advisable to use :meth:`reshape`, which
            returns a view if the shapes are compatible, and copies (equivalent to calling
            :meth:`contiguous`) otherwise.
    
            Args:
                shape (torch.Size or int...): the desired size
    
            Example::
    
                >>> x = torch.randn(4, 4)
                >>> x.size()
                torch.Size([4, 4])
                >>> y = x.view(16)
                >>> y.size()
                torch.Size([16])
                >>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions
                >>> z.size()
                torch.Size([2, 8])
    
                >>> a = torch.randn(1, 2, 3, 4)
                >>> a.size()
                torch.Size([1, 2, 3, 4])
                >>> b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension
                >>> b.size()
                torch.Size([1, 3, 2, 4])
                >>> c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory
                >>> c.size()
                torch.Size([1, 3, 2, 4])
                >>> torch.equal(b, c)
                False
    
            .. method:: view(dtype) -> Tensor
               :noindex:
    
            Returns a new tensor with the same data as the :attr:`self` tensor but of a
            different :attr:`dtype`.
    
            If the element size of :attr:`dtype` is different than that of ``self.dtype``,
            then the size of the last dimension of the output will be scaled
            proportionally.  For instance, if :attr:`dtype` element size is twice that of
            ``self.dtype``, then each pair of elements in the last dimension of
            :attr:`self` will be combined, and the size of the last dimension of the output
            will be half that of :attr:`self`. If :attr:`dtype` element size is half that
            of ``self.dtype``, then each element in the last dimension of :attr:`self` will
            be split in two, and the size of the last dimension of the output will be
            double that of :attr:`self`. For this to be possible, the following conditions
            must be true:
    
                * ``self.dim()`` must be greater than 0.
                * ``self.stride(-1)`` must be 1.
    
            Additionally, if the element size of :attr:`dtype` is greater than that of
            ``self.dtype``, the following conditions must be true as well:
    
                * ``self.size(-1)`` must be divisible by the ratio between the element
                  sizes of the dtypes.
                * ``self.storage_offset()`` must be divisible by the ratio between the
                  element sizes of the dtypes.
                * The strides of all dimensions, except the last dimension, must be
                  divisible by the ratio between the element sizes of the dtypes.
    
            If any of the above conditions are not met, an error is thrown.
    
            .. warning::
    
                This overload is not supported by TorchScript, and using it in a Torchscript
                program will cause undefined behavior.
    
            Args:
                dtype (:class:`torch.dtype`): the desired dtype
    
            Example::
    
                >>> x = torch.randn(4, 4)
                >>> x
                tensor([[ 0.9482, -0.0310,  1.4999, -0.5316],
                        [-0.1520,  0.7472,  0.5617, -0.8649],
                        [-2.4724, -0.0334, -0.2976, -0.8499],
                        [-0.2109,  1.9913, -0.9607, -0.6123]])
                >>> x.dtype
                torch.float32
    
                >>> y = x.view(torch.int32)
                >>> y
                tensor([[ 1064483442, -1124191867,  1069546515, -1089989247],
                        [-1105482831,  1061112040,  1057999968, -1084397505],
                        [-1071760287, -1123489973, -1097310419, -1084649136],
                        [-1101533110,  1073668768, -1082790149, -1088634448]],
                    dtype=torch.int32)
                >>> y[0, 0] = 1000000000
                >>> x
                tensor([[ 0.0047, -0.0310,  1.4999, -0.5316],
                        [-0.1520,  0.7472,  0.5617, -0.8649],
                        [-2.4724, -0.0334, -0.2976, -0.8499],
                        [-0.2109,  1.9913, -0.9607, -0.6123]])
    
                >>> x.view(torch.cfloat)
                tensor([[ 0.0047-0.0310j,  1.4999-0.5316j],
                        [-0.1520+0.7472j,  0.5617-0.8649j],
                        [-2.4724-0.0334j, -0.2976-0.8499j],
                        [-0.2109+1.9913j, -0.9607-0.6123j]])
                >>> x.view(torch.cfloat).size()
                torch.Size([4, 2])
    
                >>> x.view(torch.uint8)
                tensor([[  0, 202, 154,  59, 182, 243, 253, 188, 185, 252, 191,  63, 240,  22,
                           8, 191],
                        [227, 165,  27, 190, 128,  72,  63,  63, 146, 203,  15,  63,  22, 106,
                          93, 191],
                        [205,  59,  30, 192, 112, 206,   8, 189,   7,  95, 152, 190,  12, 147,
                          89, 191],
                        [ 43, 246,  87, 190, 235, 226, 254,  63, 111, 240, 117, 191, 177, 191,
                          28, 191]], dtype=torch.uint8)
                >>> x.view(torch.uint8).size()
                torch.Size([4, 16])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in []:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        # Obtain available sized in arguments (which will be fed into size function).
    
        # Use the given inner codes if they are provided.
        torch_returned = False
        if len(size) == 1 and isinstance(size[0], tuple):  size = size[0]
        if ... in size:
            i = size.index(...)
            assert ... not in size[i+1:], "Only one ellipsis is allowed in view shape. "
            size_l = Size(size[:i])
            size_r = Size(size[i+1:])
            size = size_l + self.shape[len(size_l):self.n_dim-len(size_r)] + size_r
        else:  size = Size(size)
        with torch._C.DisableTorchFunction():
            obj = Tensor.inherit_from(torch_super(self, 'view')(size), self, shape=size)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "view"
        return obj
    
    def view_as(self, other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.view_as'. The automatically generated codes are as follows:
            [ 1]: def view_as(self, other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if other is None: ...
            [13]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [14]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     other_shape=None if other is None else Size(other.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [23]:         obj = torch_super(self, 'view_as')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['view_as'](other_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "view_as"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            view_as(other) -> Tensor
    
            View this tensor as the same size as :attr:`other`.
            ``self.view_as(other)`` is equivalent to ``self.view(other.size())``.
    
            Please see :meth:`~Tensor.view` for more information about ``view``.
    
            Args:
                other (:class:`torch.Tensor`): The result tensor has the same size
                    as :attr:`other`.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, 'view_as')(*auto_generated_args, )
        ref_shape = size_mapping['view_as'](other_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "view_as"
        return obj
    
    def where(self: 'Tensor', condition: 'Tensor', other: 'Tensor'=None, *, equals: 'Tensor'=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.where'. The automatically generated codes are as follows:
            [ 1]: def where(self: 'Tensor', condition: 'Tensor', other: 'Tensor'=None, *, equals: 'Tensor'=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, condition, other, equals]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if condition is None: ...
            [19]:     elif not isinstance(condition, torch.Tensor): condition = torch.tensor(condition)
            [20]:     if not isinstance(condition, subclass): condition = condition.as_subclass(subclass).special_from(condition.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if condition.device.type == 'cpu': condition = condition.to(pivot.device)
            [23]:
            [24]:     if other is None: ...
            [25]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [26]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [27]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [28]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [29]:
            [30]:     if equals is None: ...
            [31]:     elif not isinstance(equals, torch.Tensor): equals = torch.tensor(equals)
            [32]:     if not isinstance(equals, subclass): equals = equals.as_subclass(subclass).special_from(equals.shape)
            [33]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [34]:         if equals.device.type == 'cpu': equals = equals.to(pivot.device)
            [35]:
            [36]:     # Obtain available sized in arguments (which will be fed into size function).
            [37]:     self_shape=None if self is None else Size(self.shape)
            [38]:     condition_shape=None if condition is None else Size(condition.shape)
            [39]:     other_shape=None if other is None else Size(other.shape)
            [40]:     equals_shape=None if equals is None else Size(equals.shape)
            [41]:     # Use the given inner codes if they are provided.
            [42]:     torch_returned = False
            [43]:     if equals is None:
            [44]:         ref_shape = broadcast(self_shape, condition_shape, other_shape, with_size_updates=True)
            [45]:         self = self.view(ref_shape.updated_sizes[0])
            [46]:         condition = condition.view(ref_shape.updated_sizes[1])
            [47]:         other = other.view(ref_shape.updated_sizes[2])
            [48]:         with torch._C.DisableTorchFunction():
            [49]:             obj = torch_super(self, 'where')(condition, other)
            [50]:         obj = Tensor.inherit_from(obj, self, shape=ref_shape)
            [51]:     else:
            [52]:         ref_shape = broadcast(self_shape, condition_shape, equals_shape, with_size_updates=True)
            [53]:         self = self.view(ref_shape.updated_sizes[0])
            [54]:         condition = condition.view(ref_shape.updated_sizes[1])
            [55]:         equals = equals.view(ref_shape.updated_sizes[2])
            [56]:         with torch._C.DisableTorchFunction():
            [57]:             obj = torch_super(self, 'where')(~condition, equals)
            [58]:         obj = Tensor.inherit_from(obj, self, shape=ref_shape)
            [59]:
            [60]:     def update_special(obj, updates):
            [61]:         if isinstance(obj, tuple):
            [62]:             for x in obj: update_special(x, updates)
            [63]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [64]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [65]:
            [66]:     if getattr(obj, 'grad_fn', None) is not None:
            [67]:         obj.grad_fn_name = "where"
            [68]:     return obj
            [69]:
            The document of the original function is:
    
            where(condition, y) -> Tensor
    
            ``self.where(condition, y)`` is equivalent to ``torch.where(condition, self, y)``.
            See :func:`torch.where`
    
            which is:
    
            where(condition, input, other, *, out=None) -> Tensor
    
            Return a tensor of elements selected from either :attr:`input` or :attr:`other`, depending on :attr:`condition`.
    
            The operation is defined as:
    
            .. math::
                \text{out}_i = \begin{cases}
                    \text{input}_i & \text{if } \text{condition}_i \\
                    \text{other}_i & \text{otherwise} \\
                \end{cases}
    
            .. note::
                The tensors :attr:`condition`, :attr:`input`, :attr:`other` must be :ref:`broadcastable <broadcasting-semantics>`.
    
            Arguments:
                condition (BoolTensor): When True (nonzero), yield input, otherwise yield other
                input (Tensor or Scalar): value (if :attr:`input` is a scalar) or values selected at indices
                                      where :attr:`condition` is ``True``
                other (Tensor or Scalar): value (if :attr:`other` is a scalar) or values selected at indices
                                      where :attr:`condition` is ``False``
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Returns:
                Tensor: A tensor of shape equal to the broadcasted shape of :attr:`condition`, :attr:`input`, :attr:`other`
    
            Example::
    
                >>> x = torch.randn(3, 2)
                >>> y = torch.ones(3, 2)
                >>> x
                tensor([[-0.4620,  0.3139],
                        [ 0.3898, -0.7197],
                        [ 0.0478, -0.1657]])
                >>> torch.where(x > 0, 1.0, 0.0)
                tensor([[0., 1.],
                        [1., 0.],
                        [1., 0.]])
                >>> torch.where(x > 0, x, y)
                tensor([[ 1.0000,  0.3139],
                        [ 0.3898,  1.0000],
                        [ 0.0478,  1.0000]])
                >>> x = torch.randn(2, 2, dtype=torch.double)
                >>> x
                tensor([[ 1.0779,  0.0383],
                        [-0.8785, -1.1089]], dtype=torch.float64)
                >>> torch.where(x > 0, x, 0.)
                tensor([[1.0779, 0.0383],
                        [0.0000, 0.0000]], dtype=torch.float64)
    
            .. function:: where(condition) -> tuple of LongTensor
               :noindex:
    
            ``torch.where(condition)`` is identical to
            ``torch.nonzero(condition, as_tuple=True)``.
    
            .. note::
                See also :func:`torch.nonzero`.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, condition, other, equals]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if condition is None: ...
        elif not isinstance(condition, torch.Tensor): condition = torch.tensor(condition)
        if not isinstance(condition, subclass): condition = condition.as_subclass(subclass).special_from(condition.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if condition.device.type == 'cpu': condition = condition.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        if equals is None: ...
        elif not isinstance(equals, torch.Tensor): equals = torch.tensor(equals)
        if not isinstance(equals, subclass): equals = equals.as_subclass(subclass).special_from(equals.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if equals.device.type == 'cpu': equals = equals.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        condition_shape=None if condition is None else Size(condition.shape)
        other_shape=None if other is None else Size(other.shape)
        equals_shape=None if equals is None else Size(equals.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        if equals is None:
            ref_shape = broadcast(self_shape, condition_shape, other_shape, with_size_updates=True)
            self = self.view(ref_shape.updated_sizes[0])
            condition = condition.view(ref_shape.updated_sizes[1])
            other = other.view(ref_shape.updated_sizes[2])
            with torch._C.DisableTorchFunction():
                obj = torch_super(self, 'where')(condition, other)
            obj = Tensor.inherit_from(obj, self, shape=ref_shape)
        else:
            ref_shape = broadcast(self_shape, condition_shape, equals_shape, with_size_updates=True)
            self = self.view(ref_shape.updated_sizes[0])
            condition = condition.view(ref_shape.updated_sizes[1])
            equals = equals.view(ref_shape.updated_sizes[2])
            with torch._C.DisableTorchFunction():
                obj = torch_super(self, 'where')(~condition, equals)
            obj = Tensor.inherit_from(obj, self, shape=ref_shape)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "where"
        return obj
    
    def down_scale(self: 'Tensor', factor=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.down_scale'. The automatically generated codes are as follows:
            [ 1]: def down_scale(self: 'Tensor', factor=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     self_shape=None if self is None else Size(self.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     torch_returned = False
            [22]:     if isinstance(factor, int_):  factor = (factor,) * self.n_space_dim
            [23]:     obj = self[(slice(None),) * self.space_start + tuple(slice(None, None, f) for f in factor)] # suppress:  special_from
            [24]:
            [25]:     def update_special(obj, updates):
            [26]:         if isinstance(obj, tuple):
            [27]:             for x in obj: update_special(x, updates)
            [28]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [29]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [30]:
            [31]:     if getattr(obj, 'grad_fn', None) is not None:
            [32]:         obj.grad_fn_name = "down_scale"
            [33]:     return obj
            [34]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        if isinstance(factor, int_):  factor = (factor,) * self.n_space_dim
        obj = self[(slice(None),) * self.space_start + tuple(slice(None, None, f) for f in factor)] # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "down_scale"
        return obj
    
    def up_scale(self: 'Tensor', factor=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.up_scale'. The automatically generated codes are as follows:
            [ 1]: def up_scale(self: 'Tensor', factor=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     self_shape=None if self is None else Size(self.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     torch_returned = False
            [22]:     if isinstance(factor, int_):  factor = (factor,) * self.n_space_dim
            [23]:     for f, d in zip(factor, range_(*self.space_range)):
            [24]:         self = self.amplify(f, d)
            [25]:     obj = self # suppress:  special_from
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "up_scale"
            [35]:     return obj
            [36]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        if isinstance(factor, int_):  factor = (factor,) * self.n_space_dim
        for f, d in zip(factor, range_(*self.space_range)):
            self = self.amplify(f, d)
        obj = self # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "up_scale"
        return obj
    
    ### STOP METHOD AUTO GENERATION
    @alias("inv")
    def inverse(input: 'Tensor', dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
        conpute the inverse matrix for dimensions (the first available condition):
        (1) the last 2 feature dimensions (if n_feature_dim >= 2);
        (2) the last 2 space dimensions (if n_space_dim >= 2);
        (3) the last 2 sequence dimensions (if n_sequence_dim >= 2).
    
        Automatically inheritted method from 'torch.Tensor.inverse'. The automatically generated codes are as follows:
            [ 1]: @alias("inv")
            [ 2]: def inverse(input: 'Tensor', dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 3]:     '''
            [ 4]:     conpute the inverse matrix for dimensions (the first available condition):
            [ 5]:     (1) the last 2 feature dimensions (if n_feature_dim >= 2);
            [ 6]:     (2) the last 2 space dimensions (if n_space_dim >= 2);
            [ 7]:     (3) the last 2 sequence dimensions (if n_sequence_dim >= 2).
            [ 8]:     '''
            [ 9]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [10]:
            [11]:     pivot = None
            [12]:     for t in [input]:
            [13]:         if isinstance(t, torch.Tensor): pivot = t; break
            [14]:     subclass = Tensor.get_tensor_subclass(pivot)
            [15]:
            [16]:     if input is None: ...
            [17]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [18]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [19]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [20]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [21]:
            [22]:     dim = linalg_dim[2](input, dim)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     input_shape=None if input is None else Size(input.shape)
            [26]:     # Use the given inner codes if they are provided.
            [27]:     torch_returned = False
            [28]:     avouch(len(dim) == 2 and dim[0] != dim[1], TypeError("bt.inverse accepts only two dimensions for inversion. "))
            [29]:     if not input.dtype.is_floating_point:  input = input.type(bt.float)
            [30]:     with input.hide_special(), torch._C.DisableTorchFunction():
            [31]:         inv_output = Tensor.inherit_from(torch.linalg.inv(input.move_dim(dim, -1)), input, shape=[])
            [32]:     obj = inv_output.move_dim([-2, -1], dim).special_from(input)
            [33]:
            [34]:     def update_special(obj, updates):
            [35]:         if isinstance(obj, tuple):
            [36]:             for x in obj: update_special(x, updates)
            [37]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [38]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [39]:
            [40]:     if getattr(obj, 'grad_fn', None) is not None:
            [41]:         obj.grad_fn_name = "inverse"
            [42]:     return obj
            [43]:
            The document of the original function is:
    
            inverse() -> Tensor
    
            See :func:`torch.inverse`
    
            which is:
    
            inverse(input, *, out=None) -> Tensor
    
            Alias for :func:`torch.linalg.inv`
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = linalg_dim[2](input, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        avouch(len(dim) == 2 and dim[0] != dim[1], TypeError("bt.inverse accepts only two dimensions for inversion. "))
        if not input.dtype.is_floating_point:  input = input.type(bt.float)
        with input.hide_special(), torch._C.DisableTorchFunction():
            inv_output = Tensor.inherit_from(torch.linalg.inv(input.move_dim(dim, -1)), input, shape=[])
        obj = inv_output.move_dim([-2, -1], dim).special_from(input)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "inverse"
        return obj
    
    def diag(input: 'Tensor', diagonal=0, dim: linalg_dim[1,2]=None, *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
        Compute the diagonal of a 2D matrix or a 2D matrix with diagonal elements from 1D input.
        Regarding the shape of input, the first available condition is performed:
        (1) create 2D feature for 1D feature;
        (2) get 1D diagonal for the last 2 feature dimensions;
        (3) create 2D space for 1D space;
        (4) get 1D diagonal for the last 2 space dimensions;
        (5) create 2D sequence for 1D sequence;
        (6) get 1D diagonal for the last 2 sequence dimensions.
    
        `diagonal` controls the i-th diagonal, positive for those above the main diagonal, e.g. `diagonal=1` means:
        [0 * 0 0 0 0 0]
        [0 0 * 0 0 0 0]
        [0 0 0 * 0 0 0]
        [0 0 0 0 * 0 0]
        [0 0 0 0 0 * 0]
        [0 0 0 0 0 0 *]
        [0 0 0 0 0 0 0]
        this argument works for both 1->2D and 2->1D.
    
        Automatically inheritted method from 'torch.Tensor.diag'. The automatically generated codes are as follows:
            [ 1]: def diag(input: 'Tensor', diagonal=0, dim: linalg_dim[1,2]=None, *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:     Compute the diagonal of a 2D matrix or a 2D matrix with diagonal elements from 1D input.
            [ 4]:     Regarding the shape of input, the first available condition is performed:
            [ 5]:     (1) create 2D feature for 1D feature;
            [ 6]:     (2) get 1D diagonal for the last 2 feature dimensions;
            [ 7]:     (3) create 2D space for 1D space;
            [ 8]:     (4) get 1D diagonal for the last 2 space dimensions;
            [ 9]:     (5) create 2D sequence for 1D sequence;
            [10]:     (6) get 1D diagonal for the last 2 sequence dimensions.
            [11]:
            [12]:     `diagonal` controls the i-th diagonal, positive for those above the main diagonal, e.g. `diagonal=1` means:
            [13]:     [0 * 0 0 0 0 0]
            [14]:     [0 0 * 0 0 0 0]
            [15]:     [0 0 0 * 0 0 0]
            [16]:     [0 0 0 0 * 0 0]
            [17]:     [0 0 0 0 0 * 0]
            [18]:     [0 0 0 0 0 0 *]
            [19]:     [0 0 0 0 0 0 0]
            [20]:     this argument works for both 1->2D and 2->1D.
            [21]:     '''
            [22]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [23]:
            [24]:     pivot = None
            [25]:     for t in [input]:
            [26]:         if isinstance(t, torch.Tensor): pivot = t; break
            [27]:     subclass = Tensor.get_tensor_subclass(pivot)
            [28]:
            [29]:     if input is None: ...
            [30]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [31]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [32]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [33]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [34]:
            [35]:     dim = linalg_dim[1,2](input, dim)
            [36]:
            [37]:     # Obtain available sized in arguments (which will be fed into size function).
            [38]:     input_shape=None if input is None else Size(input.shape)
            [39]:     # Use the given inner codes if they are provided.
            [40]:     torch_returned = False
            [41]:     size = input.shape
            [42]:     if len(dim) == 1:
            [43]:         n = size[dim[0]]
            [44]:         design_mat = cat(zeros(size.with_dim_size(dim[0], 1), device=input.device, dtype=input.dtype), input, dim[0])
            [45]:         index_mat = zeros(n + abs_(diagonal), n + abs_(diagonal), device=input.device).long()
            [46]:         if diagonal >= 0:  index_mat[arange(n, device=input.device), arange(diagonal, diagonal+n, device=input.device)] = arange(n, device=input.device) + 1
            [47]:         else:  index_mat[arange(-diagonal, -diagonal+n, device=input.device), arange(n, device=input.device)] = arange(n, device=input.device) + 1
            [48]:         index_mat.add_special_dim(0, size[dim[0]:dim[0]+1])
            [49]:         index_mat.add_special_dim(1, size[dim[0]:dim[0]+1])
            [50]:         obj = design_mat[(slice(None),) * dim[0] + (index_mat,)]
            [51]:     if len(dim) == 2:
            [52]:         n = min_(size[dim[0]], size[dim[1]]) - abs_(diagonal)
            [53]:         if dim[0] > dim[1]:  dim = dim[::-1]
            [54]:         if diagonal >= 0:  index_x = arange(n, device=input.device).special_from(size[dim[0]:dim[0]+1]); index_y = arange(diagonal, diagonal+n, device=input.device).special_from(size[dim[1]:dim[1]+1])
            [55]:         else:  index_x = arange(-diagonal, -diagonal+n, device=input.device).special_from(size[dim[0]:dim[0]+1]); index_y = arange(n, device=input.device).special_from(size[dim[1]:dim[1]+1])
            [56]:         with input.hide_special(), index_x.hide_special(), index_y.hide_special():
            [57]:             diag_mat = input[(slice(None),) * dim[0] + (index_x,) + (slice(None),) * (dim[1] - dim[0] - 1) + (index_y,)]
            [58]:         if dim[1] - dim[0] - 1 == 0: torch_returned = True; obj = diag_mat.special_from(remove_dim(input.shape, [dim[1]]))
            [59]:         else: torch_returned = True; obj = diag_mat.move_dim(0, dim[0]).special_from(remove_dim(input.shape, [dim[1]]))
            [60]:
            [61]:     def update_special(obj, updates):
            [62]:         if isinstance(obj, tuple):
            [63]:             for x in obj: update_special(x, updates)
            [64]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [65]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [66]:
            [67]:     if getattr(obj, 'grad_fn', None) is not None:
            [68]:         obj.grad_fn_name = "diag"
            [69]:     return obj
            [70]:
            The document of the original function is:
    
            diag(diagonal=0) -> Tensor
    
            See :func:`torch.diag`
    
            which is:
    
            diag(input, diagonal=0, *, out=None) -> Tensor
    
            - If :attr:`input` is a vector (1-D tensor), then returns a 2-D square tensor
              with the elements of :attr:`input` as the diagonal.
            - If :attr:`input` is a matrix (2-D tensor), then returns a 1-D tensor with
              the diagonal elements of :attr:`input`.
    
            The argument :attr:`diagonal` controls which diagonal to consider:
    
            - If :attr:`diagonal` = 0, it is the main diagonal.
            - If :attr:`diagonal` > 0, it is above the main diagonal.
            - If :attr:`diagonal` < 0, it is below the main diagonal.
    
            Args:
                input (Tensor): the input tensor.
                diagonal (int, optional): the diagonal to consider
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            .. seealso::
    
                    :func:`torch.diagonal` always returns the diagonal of its input.
    
                    :func:`torch.diagflat` always constructs a tensor with diagonal elements
                    specified by the input.
    
            Examples:
    
            Get the square matrix where the input vector is the diagonal::
    
                >>> a = torch.randn(3)
                >>> a
                tensor([ 0.5950,-0.0872, 2.3298])
                >>> torch.diag(a)
                tensor([[ 0.5950, 0.0000, 0.0000],
                        [ 0.0000,-0.0872, 0.0000],
                        [ 0.0000, 0.0000, 2.3298]])
                >>> torch.diag(a, 1)
                tensor([[ 0.0000, 0.5950, 0.0000, 0.0000],
                        [ 0.0000, 0.0000,-0.0872, 0.0000],
                        [ 0.0000, 0.0000, 0.0000, 2.3298],
                        [ 0.0000, 0.0000, 0.0000, 0.0000]])
    
            Get the k-th diagonal of a given matrix::
    
                >>> a = torch.randn(3, 3)
                >>> a
                tensor([[-0.4264, 0.0255,-0.1064],
                        [ 0.8795,-0.2429, 0.1374],
                        [ 0.1029,-0.6482,-1.6300]])
                >>> torch.diag(a, 0)
                tensor([-0.4264,-0.2429,-1.6300])
                >>> torch.diag(a, 1)
                tensor([ 0.0255, 0.1374])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = linalg_dim[1,2](input, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        size = input.shape
        if len(dim) == 1:
            n = size[dim[0]]
            design_mat = cat(zeros(size.with_dim_size(dim[0], 1), device=input.device, dtype=input.dtype), input, dim[0])
            index_mat = zeros(n + abs_(diagonal), n + abs_(diagonal), device=input.device).long()
            if diagonal >= 0:  index_mat[arange(n, device=input.device), arange(diagonal, diagonal+n, device=input.device)] = arange(n, device=input.device) + 1
            else:  index_mat[arange(-diagonal, -diagonal+n, device=input.device), arange(n, device=input.device)] = arange(n, device=input.device) + 1
            index_mat.add_special_dim(0, size[dim[0]:dim[0]+1])
            index_mat.add_special_dim(1, size[dim[0]:dim[0]+1])
            obj = design_mat[(slice(None),) * dim[0] + (index_mat,)]
        if len(dim) == 2:
            n = min_(size[dim[0]], size[dim[1]]) - abs_(diagonal)
            if dim[0] > dim[1]:  dim = dim[::-1]
            if diagonal >= 0:  index_x = arange(n, device=input.device).special_from(size[dim[0]:dim[0]+1]); index_y = arange(diagonal, diagonal+n, device=input.device).special_from(size[dim[1]:dim[1]+1])
            else:  index_x = arange(-diagonal, -diagonal+n, device=input.device).special_from(size[dim[0]:dim[0]+1]); index_y = arange(n, device=input.device).special_from(size[dim[1]:dim[1]+1])
            with input.hide_special(), index_x.hide_special(), index_y.hide_special():
                diag_mat = input[(slice(None),) * dim[0] + (index_x,) + (slice(None),) * (dim[1] - dim[0] - 1) + (index_y,)]
            if dim[1] - dim[0] - 1 == 0: torch_returned = True; obj = diag_mat.special_from(remove_dim(input.shape, [dim[1]]))
            else: torch_returned = True; obj = diag_mat.move_dim(0, dim[0]).special_from(remove_dim(input.shape, [dim[1]]))
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "diag"
        return obj
    
    def diagflat(input: 'Tensor', diagonal=0, dim: exist_dim=[], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.diagflat'. The automatically generated codes are as follows:
            [ 1]: def diagflat(input: 'Tensor', diagonal=0, dim: exist_dim=[], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     dim = exist_dim(input, dim)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     input_shape=None if input is None else Size(input.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     torch_returned = False
            [24]:     obj = diag(input.flatten(*dim), diagonal=diagonal) # suppress:  special_from
            [25]:
            [26]:     def update_special(obj, updates):
            [27]:         if isinstance(obj, tuple):
            [28]:             for x in obj: update_special(x, updates)
            [29]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [30]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [31]:
            [32]:     if getattr(obj, 'grad_fn', None) is not None:
            [33]:         obj.grad_fn_name = "diagflat"
            [34]:     return obj
            [35]:
            The document of the original function is:
    
            diagflat(offset=0) -> Tensor
    
            See :func:`torch.diagflat`
    
            which is:
    
            diagflat(input, offset=0) -> Tensor
    
            - If :attr:`input` is a vector (1-D tensor), then returns a 2-D square tensor
              with the elements of :attr:`input` as the diagonal.
            - If :attr:`input` is a tensor with more than one dimension, then returns a
              2-D tensor with diagonal elements equal to a flattened :attr:`input`.
    
            The argument :attr:`offset` controls which diagonal to consider:
    
            - If :attr:`offset` = 0, it is the main diagonal.
            - If :attr:`offset` > 0, it is above the main diagonal.
            - If :attr:`offset` < 0, it is below the main diagonal.
    
            Args:
                input (Tensor): the input tensor.
                offset (int, optional): the diagonal to consider. Default: 0 (main
                    diagonal).
    
            Examples::
    
                >>> a = torch.randn(3)
                >>> a
                tensor([-0.2956, -0.9068,  0.1695])
                >>> torch.diagflat(a)
                tensor([[-0.2956,  0.0000,  0.0000],
                        [ 0.0000, -0.9068,  0.0000],
                        [ 0.0000,  0.0000,  0.1695]])
                >>> torch.diagflat(a, 1)
                tensor([[ 0.0000, -0.2956,  0.0000,  0.0000],
                        [ 0.0000,  0.0000, -0.9068,  0.0000],
                        [ 0.0000,  0.0000,  0.0000,  0.1695],
                        [ 0.0000,  0.0000,  0.0000,  0.0000]])
    
                >>> a = torch.randn(2, 2)
                >>> a
                tensor([[ 0.2094, -0.3018],
                        [-0.1516,  1.9342]])
                >>> torch.diagflat(a)
                tensor([[ 0.2094,  0.0000,  0.0000,  0.0000],
                        [ 0.0000, -0.3018,  0.0000,  0.0000],
                        [ 0.0000,  0.0000, -0.1516,  0.0000],
                        [ 0.0000,  0.0000,  0.0000,  1.9342]])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = exist_dim(input, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        obj = diag(input.flatten(*dim), diagonal=diagonal) # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "diagflat"
        return obj
    
    def diagonal(input: 'Tensor', diagonal=0, dim1=0, dim2=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.diagonal'. The automatically generated codes are as follows:
            [ 1]: def diagonal(input: 'Tensor', diagonal=0, dim1=0, dim2=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     torch_returned = False
            [22]:     obj = diag(input, diagonal=diagonal, dim=(dim1, dim2)) # suppress:  special_from
            [23]:
            [24]:     def update_special(obj, updates):
            [25]:         if isinstance(obj, tuple):
            [26]:             for x in obj: update_special(x, updates)
            [27]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [28]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [29]:
            [30]:     if getattr(obj, 'grad_fn', None) is not None:
            [31]:         obj.grad_fn_name = "diagonal"
            [32]:     return obj
            [33]:
            The document of the original function is:
    
            diagonal(offset=0, dim1=0, dim2=1) -> Tensor
    
            See :func:`torch.diagonal`
    
            which is:
    
            diagonal(input, offset=0, dim1=0, dim2=1) -> Tensor
    
            Returns a partial view of :attr:`input` with the its diagonal elements
            with respect to :attr:`dim1` and :attr:`dim2` appended as a dimension
            at the end of the shape.
    
            The argument :attr:`offset` controls which diagonal to consider:
    
            - If :attr:`offset` = 0, it is the main diagonal.
            - If :attr:`offset` > 0, it is above the main diagonal.
            - If :attr:`offset` < 0, it is below the main diagonal.
    
            Applying :meth:`torch.diag_embed` to the output of this function with
            the same arguments yields a diagonal matrix with the diagonal entries
            of the input. However, :meth:`torch.diag_embed` has different default
            dimensions, so those need to be explicitly specified.
    
            Args:
                input (Tensor): the input tensor. Must be at least 2-dimensional.
                offset (int, optional): which diagonal to consider. Default: 0
                    (main diagonal).
                dim1 (int, optional): first dimension with respect to which to
                    take diagonal. Default: 0.
                dim2 (int, optional): second dimension with respect to which to
                    take diagonal. Default: 1.
    
            .. note::  To take a batch diagonal, pass in dim1=-2, dim2=-1.
    
            Examples::
    
                >>> a = torch.randn(3, 3)
                >>> a
                tensor([[-1.0854,  1.1431, -0.1752],
                        [ 0.8536, -0.0905,  0.0360],
                        [ 0.6927, -0.3735, -0.4945]])
    
                >>> torch.diagonal(a, 0)
                tensor([-1.0854, -0.0905, -0.4945])
    
                >>> torch.diagonal(a, 1)
                tensor([ 1.1431,  0.0360])
    
                >>> x = torch.randn(2, 5, 4, 2)
                >>> torch.diagonal(x, offset=-1, dim1=1, dim2=2)
                tensor([[[-1.2631,  0.3755, -1.5977, -1.8172],
                         [-1.1065,  1.0401, -0.2235, -0.7938]],
    
                        [[-1.7325, -0.3081,  0.6166,  0.2335],
                         [ 1.0500,  0.7336, -0.3836, -1.1015]]])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        obj = diag(input, diagonal=diagonal, dim=(dim1, dim2)) # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "diagonal"
        return obj
    
    @alias('tr')
    def trace(input: 'Tensor', dim:linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.trace'. The automatically generated codes are as follows:
            [ 1]: @alias('tr')
            [ 2]: def trace(input: 'Tensor', dim:linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 3]:     '''
            [ 4]:         ***
            [ 5]:     '''
            [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 7]:
            [ 8]:     pivot = None
            [ 9]:     for t in [input]:
            [10]:         if isinstance(t, torch.Tensor): pivot = t; break
            [11]:     subclass = Tensor.get_tensor_subclass(pivot)
            [12]:
            [13]:     if input is None: ...
            [14]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [15]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [17]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [18]:
            [19]:     dim = linalg_dim[2](input, dim)
            [20]:
            [21]:     # Obtain available sized in arguments (which will be fed into size function).
            [22]:     input_shape=None if input is None else Size(input.shape)
            [23]:     # Use the given inner codes if they are provided.
            [24]:     torch_returned = False
            [25]:     obj = diag(input, dim=dim).sum(dim[0]) # suppress:  special_from
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "trace"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            trace() -> Tensor
    
            See :func:`torch.trace`
    
            which is:
    
            trace(input) -> Tensor
    
            Returns the sum of the elements of the diagonal of the input 2-D matrix.
    
            Example::
    
                >>> x = torch.arange(1., 10.).view(3, 3)
                >>> x
                tensor([[ 1.,  2.,  3.],
                        [ 4.,  5.,  6.],
                        [ 7.,  8.,  9.]])
                >>> torch.trace(x)
                tensor(15.)
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = linalg_dim[2](input, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        obj = diag(input, dim=dim).sum(dim[0]) # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "trace"
        return obj
    
    def det(input: 'Tensor', *, dim:linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.det'. The automatically generated codes are as follows:
            [ 1]: def det(input: 'Tensor', *, dim:linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     dim = linalg_dim[2](input, dim)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     input_shape=None if input is None else Size(input.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     torch_returned = False
            [24]:     avouch(len(dim) == 2 and dim[0] != dim[1], TypeError("bt.det accepts only two dimensions for determinant. "))
            [25]:     ref_shape = remove_dim(input.shape, dim)
            [26]:     with input.hide_special(), torch._C.DisableTorchFunction():
            [27]:         obj = Tensor.inherit_from(torch.det(input.move_dim(dim, -1)), input, shape=ref_shape) # suppress:  special_from
            [28]:
            [29]:     def update_special(obj, updates):
            [30]:         if isinstance(obj, tuple):
            [31]:             for x in obj: update_special(x, updates)
            [32]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [33]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [34]:
            [35]:     if getattr(obj, 'grad_fn', None) is not None:
            [36]:         obj.grad_fn_name = "det"
            [37]:     return obj
            [38]:
            The document of the original function is:
    
            det() -> Tensor
    
            See :func:`torch.det`
    
            which is:
    
            det(input) -> Tensor
    
            Alias for :func:`torch.linalg.det`
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = linalg_dim[2](input, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        avouch(len(dim) == 2 and dim[0] != dim[1], TypeError("bt.det accepts only two dimensions for determinant. "))
        ref_shape = remove_dim(input.shape, dim)
        with input.hide_special(), torch._C.DisableTorchFunction():
            obj = Tensor.inherit_from(torch.det(input.move_dim(dim, -1)), input, shape=ref_shape) # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "det"
        return obj
    
    def add(self: 'Tensor', other: 'Tensor', *, alpha=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.add'. The automatically generated codes are as follows:
            [ 1]: def add(self: 'Tensor', other: 'Tensor', *, alpha=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['add'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, 'add')(*auto_generated_args, alpha=alpha)
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "add"
            [44]:     return obj
            [45]:
            The document of the original function is:
    
            add(other, *, alpha=1) -> Tensor
    
            Add a scalar or tensor to :attr:`self` tensor. If both :attr:`alpha`
            and :attr:`other` are specified, each element of :attr:`other` is scaled by
            :attr:`alpha` before being used.
    
            When :attr:`other` is a tensor, the shape of :attr:`other` must be
            :ref:`broadcastable <broadcasting-semantics>` with the shape of the underlying
            tensor
    
            See :func:`torch.add`
    
            which is:
    
            add(input, other, *, alpha=1, out=None) -> Tensor
    
            Adds :attr:`other`, scaled by :attr:`alpha`, to :attr:`input`.
    
            .. math::
                \text{{out}}_i = \text{{input}}_i + \text{{alpha}} \times \text{{other}}_i
    
            Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
            :ref:`type promotion <type-promotion-doc>`, and integer, float, and complex inputs.
    
            Args:
                input (Tensor): the input tensor.
                other (Tensor or Number): the tensor or number to add to :attr:`input`.
    
            Keyword arguments:
                alpha (Number): the multiplier for :attr:`other`.
                out (Tensor, optional): the output tensor.
    
            Examples::
    
                >>> a = torch.randn(4)
                >>> a
                tensor([ 0.0202,  1.0985,  1.3506, -0.6056])
                >>> torch.add(a, 20)
                tensor([ 20.0202,  21.0985,  21.3506,  19.3944])
    
                >>> b = torch.randn(4)
                >>> b
                tensor([-0.9732, -0.3497,  0.6245,  0.4022])
                >>> c = torch.randn(4, 1)
                >>> c
                tensor([[ 0.3743],
                        [-1.7724],
                        [-0.5811],
                        [-0.8017]])
                >>> torch.add(b, c, alpha=10)
                tensor([[  2.7695,   3.3930,   4.3672,   4.1450],
                        [-18.6971, -18.0736, -17.0994, -17.3216],
                        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],
                        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['add'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, 'add')(*auto_generated_args, alpha=alpha)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "add"
        return obj
    
    def sub(self: 'Tensor', other: 'Tensor', *, alpha=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.sub'. The automatically generated codes are as follows:
            [ 1]: def sub(self: 'Tensor', other: 'Tensor', *, alpha=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['sub'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, 'sub')(*auto_generated_args, alpha=alpha)
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "sub"
            [44]:     return obj
            [45]:
            The document of the original function is:
    
            sub(other, *, alpha=1) -> Tensor
    
            See :func:`torch.sub`.
    
            which is:
    
            sub(input, other, *, alpha=1, out=None) -> Tensor
    
            Subtracts :attr:`other`, scaled by :attr:`alpha`, from :attr:`input`.
    
            .. math::
                \text{{out}}_i = \text{{input}}_i - \text{{alpha}} \times \text{{other}}_i
    
            Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
            :ref:`type promotion <type-promotion-doc>`, and integer, float, and complex inputs.
    
            Args:
                input (Tensor): the input tensor.
                other (Tensor or Number): the tensor or number to subtract from :attr:`input`.
    
            Keyword args:
                alpha (Number): the multiplier for :attr:`other`.
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.tensor((1, 2))
                >>> b = torch.tensor((0, 1))
                >>> torch.sub(a, b, alpha=2)
                tensor([1, 0])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['sub'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, 'sub')(*auto_generated_args, alpha=alpha)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "sub"
        return obj
    
    def mul(self: 'Tensor', value: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.mul'. The automatically generated codes are as follows:
            [ 1]: def mul(self: 'Tensor', value: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, value]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if value is None: ...
            [19]:     elif not isinstance(value, torch.Tensor): value = torch.tensor(value)
            [20]:     if not isinstance(value, subclass): value = value.as_subclass(subclass).special_from(value.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if value.device.type == 'cpu': value = value.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     value_shape=None if value is None else Size(value.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, value_shape = size_mapping_op['mul'](self_shape, value_shape)
            [29]:     self = self.view(self_shape)
            [30]:     value = value.view(value_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [value] if x is not None)
            [33]:         obj = torch_super(self, 'mul')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "mul"
            [44]:     return obj
            [45]:
            The document of the original function is:
    
            mul(value) -> Tensor
    
            See :func:`torch.mul`.
    
            which is:
    
            mul(input, other, *, out=None) -> Tensor
    
            Multiplies :attr:`input` by :attr:`other`.
    
            .. math::
                \text{out}_i = \text{input}_i \times \text{other}_i
    
            Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
            :ref:`type promotion <type-promotion-doc>`, and integer, float, and complex inputs.
    
            Args:
                input (Tensor): the input tensor.
                other (Tensor or Number) - the tensor or number to multiply input by.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Examples::
    
                >>> a = torch.randn(3)
                >>> a
                tensor([ 0.2015, -0.4255,  2.6087])
                >>> torch.mul(a, 100)
                tensor([  20.1494,  -42.5491,  260.8663])
    
                >>> b = torch.randn(4, 1)
                >>> b
                tensor([[ 1.1207],
                        [-0.3137],
                        [ 0.0700],
                        [ 0.8378]])
                >>> c = torch.randn(1, 4)
                >>> c
                tensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])
                >>> torch.mul(b, c)
                tensor([[ 0.5767,  0.1363, -0.5877,  2.5083],
                        [-0.1614, -0.0382,  0.1645, -0.7021],
                        [ 0.0360,  0.0085, -0.0367,  0.1567],
                        [ 0.4312,  0.1019, -0.4394,  1.8753]])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, value]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if value is None: ...
        elif not isinstance(value, torch.Tensor): value = torch.tensor(value)
        if not isinstance(value, subclass): value = value.as_subclass(subclass).special_from(value.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if value.device.type == 'cpu': value = value.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        value_shape=None if value is None else Size(value.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, value_shape = size_mapping_op['mul'](self_shape, value_shape)
        self = self.view(self_shape)
        value = value.view(value_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [value] if x is not None)
            obj = torch_super(self, 'mul')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "mul"
        return obj
    
    def div(self: 'Tensor', value: 'Tensor', *, rounding_mode=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.div'. The automatically generated codes are as follows:
            [ 1]: def div(self: 'Tensor', value: 'Tensor', *, rounding_mode=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, value]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if value is None: ...
            [19]:     elif not isinstance(value, torch.Tensor): value = torch.tensor(value)
            [20]:     if not isinstance(value, subclass): value = value.as_subclass(subclass).special_from(value.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if value.device.type == 'cpu': value = value.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     value_shape=None if value is None else Size(value.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, value_shape = size_mapping_op['div'](self_shape, value_shape)
            [29]:     self = self.view(self_shape)
            [30]:     value = value.view(value_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [value] if x is not None)
            [33]:         obj = torch_super(self, 'div')(*auto_generated_args, rounding_mode=rounding_mode)
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "div"
            [44]:     return obj
            [45]:
            The document of the original function is:
    
            div(value, *, rounding_mode=None) -> Tensor
    
            See :func:`torch.div`
    
            which is:
    
            div(input, other, *, rounding_mode=None, out=None) -> Tensor
    
            Divides each element of the input ``input`` by the corresponding element of
            :attr:`other`.
    
            .. math::
                \text{out}_i = \frac{\text{input}_i}{\text{other}_i}
    
            .. note::
                By default, this performs a "true" division like Python 3.
                See the :attr:`rounding_mode` argument for floor division.
    
            Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
            :ref:`type promotion <type-promotion-doc>`, and integer, float, and complex inputs.
            Always promotes integer types to the default scalar type.
    
            Args:
                input (Tensor): the dividend
                other (Tensor or Number): the divisor
    
            Keyword args:
                rounding_mode (str, optional): Type of rounding applied to the result:
    
                    * None - default behavior. Performs no rounding and, if both :attr:`input` and
                      :attr:`other` are integer types, promotes the inputs to the default scalar type.
                      Equivalent to true division in Python (the ``/`` operator) and NumPy's ``np.true_divide``.
                    * ``"trunc"`` - rounds the results of the division towards zero.
                      Equivalent to C-style integer division.
                    * ``"floor"`` - rounds the results of the division down.
                      Equivalent to floor division in Python (the ``//`` operator) and NumPy's ``np.floor_divide``.
    
                out (Tensor, optional): the output tensor.
    
            Examples::
    
                >>> x = torch.tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])
                >>> torch.div(x, 0.5)
                tensor([ 0.7620,  2.5548, -0.5944, -0.7438,  0.9274])
    
                >>> a = torch.tensor([[-0.3711, -1.9353, -0.4605, -0.2917],
                ...                   [ 0.1815, -1.0111,  0.9805, -1.5923],
                ...                   [ 0.1062,  1.4581,  0.7759, -1.2344],
                ...                   [-0.1830, -0.0313,  1.1908, -1.4757]])
                >>> b = torch.tensor([ 0.8032,  0.2930, -0.8113, -0.2308])
                >>> torch.div(a, b)
                tensor([[-0.4620, -6.6051,  0.5676,  1.2639],
                        [ 0.2260, -3.4509, -1.2086,  6.8990],
                        [ 0.1322,  4.9764, -0.9564,  5.3484],
                        [-0.2278, -0.1068, -1.4678,  6.3938]])
    
                >>> torch.div(a, b, rounding_mode='trunc')
                tensor([[-0., -6.,  0.,  1.],
                        [ 0., -3., -1.,  6.],
                        [ 0.,  4., -0.,  5.],
                        [-0., -0., -1.,  6.]])
    
                >>> torch.div(a, b, rounding_mode='floor')
                tensor([[-1., -7.,  0.,  1.],
                        [ 0., -4., -2.,  6.],
                        [ 0.,  4., -1.,  5.],
                        [-1., -1., -2.,  6.]])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, value]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if value is None: ...
        elif not isinstance(value, torch.Tensor): value = torch.tensor(value)
        if not isinstance(value, subclass): value = value.as_subclass(subclass).special_from(value.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if value.device.type == 'cpu': value = value.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        value_shape=None if value is None else Size(value.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, value_shape = size_mapping_op['div'](self_shape, value_shape)
        self = self.view(self_shape)
        value = value.view(value_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [value] if x is not None)
            obj = torch_super(self, 'div')(*auto_generated_args, rounding_mode=rounding_mode)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "div"
        return obj
    
    def pow(input: 'Tensor', exponent, *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.pow'. The automatically generated codes are as follows:
            [ 1]: def pow(input: 'Tensor', exponent, *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [exponent] if x is not None)
            [23]:         obj = torch_super(input, 'pow')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['pow'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "pow"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            pow(exponent) -> Tensor
    
            See :func:`torch.pow`
    
            which is:
    
            pow(input, exponent, *, out=None) -> Tensor
    
            Takes the power of each element in :attr:`input` with :attr:`exponent` and
            returns a tensor with the result.
    
            :attr:`exponent` can be either a single ``float`` number or a `Tensor`
            with the same number of elements as :attr:`input`.
    
            When :attr:`exponent` is a scalar value, the operation applied is:
    
            .. math::
                \text{out}_i = x_i ^ \text{exponent}
    
            When :attr:`exponent` is a tensor, the operation applied is:
    
            .. math::
                \text{out}_i = x_i ^ {\text{exponent}_i}
    
            When :attr:`exponent` is a tensor, the shapes of :attr:`input`
            and :attr:`exponent` must be :ref:`broadcastable <broadcasting-semantics>`.
    
            Args:
                input (Tensor): the input tensor.
                exponent (float or tensor): the exponent value
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.randn(4)
                >>> a
                tensor([ 0.4331,  1.2475,  0.6834, -0.2791])
                >>> torch.pow(a, 2)
                tensor([ 0.1875,  1.5561,  0.4670,  0.0779])
                >>> exp = torch.arange(1., 5.)
    
                >>> a = torch.arange(1., 5.)
                >>> a
                tensor([ 1.,  2.,  3.,  4.])
                >>> exp
                tensor([ 1.,  2.,  3.,  4.])
                >>> torch.pow(a, exp)
                tensor([   1.,    4.,   27.,  256.])
    
            .. function:: pow(self, exponent, *, out=None) -> Tensor
               :noindex:
    
            :attr:`self` is a scalar ``float`` value, and :attr:`exponent` is a tensor.
            The returned tensor :attr:`out` is of the same shape as :attr:`exponent`
    
            The operation applied is:
    
            .. math::
                \text{out}_i = \text{self} ^ {\text{exponent}_i}
    
            Args:
                self (float): the scalar base value for the power operation
                exponent (Tensor): the exponent tensor
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> exp = torch.arange(1., 5.)
                >>> base = 2
                >>> torch.pow(base, exp)
                tensor([  2.,   4.,   8.,  16.])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [exponent] if x is not None)
            obj = torch_super(input, 'pow')(*auto_generated_args, )
        ref_shape = size_mapping['pow'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "pow"
        return obj
    
    def fmod(self: 'Tensor', other: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.fmod'. The automatically generated codes are as follows:
            [ 1]: def fmod(self: 'Tensor', other: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['fmod'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, 'fmod')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "fmod"
            [44]:     return obj
            [45]:
            The document of the original function is:
    
            fmod(divisor) -> Tensor
    
            See :func:`torch.fmod`
    
            which is:
    
            fmod(input, other, *, out=None) -> Tensor
    
            Applies C++'s `std::fmod <https://en.cppreference.com/w/cpp/numeric/math/fmod>`_ entrywise.
            The result has the same sign as the dividend :attr:`input` and its absolute value
            is less than that of :attr:`other`.
    
            This function may be defined in terms of :func:`torch.div` as
    
            .. code:: python
    
                torch.fmod(a, b) == a - a.div(b, rounding_mode="trunc") * b
    
            Supports :ref:`broadcasting to a common shape <broadcasting-semantics>`,
            :ref:`type promotion <type-promotion-doc>`, and integer and float inputs.
    
            .. note::
    
                When the divisor is zero, returns ``NaN`` for floating point dtypes
                on both CPU and GPU; raises ``RuntimeError`` for integer division by
                zero on CPU; Integer division by zero on GPU may return any value.
    
            .. note::
    
               Complex inputs are not supported. In some cases, it is not mathematically
               possible to satisfy the definition of a modulo operation with complex numbers.
    
            .. seealso::
    
                :func:`torch.remainder` which implements Python's modulus operator.
                This one is defined using division rounding down the result.
    
            Args:
                input (Tensor): the dividend
                other (Tensor or Scalar): the divisor
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)
                tensor([-1., -0., -1.,  1.,  0.,  1.])
                >>> torch.fmod(torch.tensor([1, 2, 3, 4, 5]), -1.5)
                tensor([1.0000, 0.5000, 0.0000, 1.0000, 0.5000])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['fmod'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, 'fmod')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "fmod"
        return obj
    
    def log(input: 'Tensor', base=torch.e, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.log'. The automatically generated codes are as follows:
            [ 1]: def log(input: 'Tensor', base=torch.e, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     torch_returned = False
            [22]:     with torch._C.DisableTorchFunction():
            [23]:         obj = torch.log(input).as_subclass(torch.Tensor) / torch.log(torch.tensor(base))
            [24]:     ref_shape = size_mapping['log'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "log"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            log() -> Tensor
    
            See :func:`torch.log`
    
            which is:
    
            log(input, *, out=None) -> Tensor
    
            Returns a new tensor with the natural logarithm of the elements
            of :attr:`input`.
    
            .. math::
                y_{i} = \log_{e} (x_{i})
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.rand(5) * 5
                >>> a
                tensor([4.7767, 4.3234, 1.2156, 0.2411, 4.5739])
                >>> torch.log(a)
                tensor([ 1.5637,  1.4640,  0.1952, -1.4226,  1.5204])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        with torch._C.DisableTorchFunction():
            obj = torch.log(input).as_subclass(torch.Tensor) / torch.log(torch.tensor(base))
        ref_shape = size_mapping['log'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "log"
        return obj
    
    def ln(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.ln'. The automatically generated codes are as follows:
            [ 1]: def ln(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     torch_returned = False
            [22]:     with torch._C.DisableTorchFunction():
            [23]:         obj = torch.log(input).as_subclass(torch.Tensor)
            [24]:     ref_shape = size_mapping['ln'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "ln"
            [35]:     return obj
            [36]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        with torch._C.DisableTorchFunction():
            obj = torch.log(input).as_subclass(torch.Tensor)
        ref_shape = size_mapping['ln'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "ln"
        return obj
    
    def log2(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.log2'. The automatically generated codes are as follows:
            [ 1]: def log2(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'log2')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['log2'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "log2"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            log2() -> Tensor
    
            See :func:`torch.log2`
    
            which is:
    
            log2(input, *, out=None) -> Tensor
    
            Returns a new tensor with the logarithm to the base 2 of the elements
            of :attr:`input`.
    
            .. math::
                y_{i} = \log_{2} (x_{i})
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.rand(5)
                >>> a
                tensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])
    
                >>> torch.log2(a)
                tensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'log2')(*auto_generated_args, )
        ref_shape = size_mapping['log2'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "log2"
        return obj
    
    def log10(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.log10'. The automatically generated codes are as follows:
            [ 1]: def log10(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'log10')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['log10'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "log10"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            log10() -> Tensor
    
            See :func:`torch.log10`
    
            which is:
    
            log10(input, *, out=None) -> Tensor
    
            Returns a new tensor with the logarithm to the base 10 of the elements
            of :attr:`input`.
    
            .. math::
                y_{i} = \log_{10} (x_{i})
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.rand(5)
                >>> a
                tensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])
    
                >>> torch.log10(a)
                tensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'log10')(*auto_generated_args, )
        ref_shape = size_mapping['log10'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "log10"
        return obj
    
    def exp(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.exp'. The automatically generated codes are as follows:
            [ 1]: def exp(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'exp')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['exp'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "exp"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            exp() -> Tensor
    
            See :func:`torch.exp`
    
            which is:
    
            exp(input, *, out=None) -> Tensor
    
            Returns a new tensor with the exponential of the elements
            of the input tensor :attr:`input`.
    
            .. math::
                y_{i} = e^{x_{i}}
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> torch.exp(torch.tensor([0, math.log(2.)]))
                tensor([ 1.,  2.])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'exp')(*auto_generated_args, )
        ref_shape = size_mapping['exp'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "exp"
        return obj
    
    def square(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.square'. The automatically generated codes are as follows:
            [ 1]: def square(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'square')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['square'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "square"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            square() -> Tensor
    
            See :func:`torch.square`
    
            which is:
    
            square(input, *, out=None) -> Tensor
    
            Returns a new tensor with the square of the elements of :attr:`input`.
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.randn(4)
                >>> a
                tensor([-2.0755,  1.0226,  0.0831,  0.4806])
                >>> torch.square(a)
                tensor([ 4.3077,  1.0457,  0.0069,  0.2310])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'square')(*auto_generated_args, )
        ref_shape = size_mapping['square'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "square"
        return obj
    
    def sqrt(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.sqrt'. The automatically generated codes are as follows:
            [ 1]: def sqrt(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'sqrt')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['sqrt'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "sqrt"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            sqrt() -> Tensor
    
            See :func:`torch.sqrt`
    
            which is:
    
            sqrt(input, *, out=None) -> Tensor
    
            Returns a new tensor with the square-root of the elements of :attr:`input`.
    
            .. math::
                \text{out}_{i} = \sqrt{\text{input}_{i}}
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.randn(4)
                >>> a
                tensor([-2.0755,  1.0226,  0.0831,  0.4806])
                >>> torch.sqrt(a)
                tensor([    nan,  1.0112,  0.2883,  0.6933])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'sqrt')(*auto_generated_args, )
        ref_shape = size_mapping['sqrt'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "sqrt"
        return obj
    
    def abs(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.abs'. The automatically generated codes are as follows:
            [ 1]: def abs(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'abs')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['abs'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "abs"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            abs() -> Tensor
    
            See :func:`torch.abs`
    
            which is:
    
            abs(input, *, out=None) -> Tensor
    
            Computes the absolute value of each element in :attr:`input`.
    
            .. math::
                \text{out}_{i} = |\text{input}_{i}|
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> torch.abs(torch.tensor([-1, -2, 3]))
                tensor([ 1,  2,  3])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'abs')(*auto_generated_args, )
        ref_shape = size_mapping['abs'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "abs"
        return obj
    
    def sign(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.sign'. The automatically generated codes are as follows:
            [ 1]: def sign(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'sign')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['sign'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "sign"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            sign() -> Tensor
    
            See :func:`torch.sign`
    
            which is:
    
            sign(input, *, out=None) -> Tensor
    
            Returns a new tensor with the signs of the elements of :attr:`input`.
    
            .. math::
                \text{out}_{i} = \operatorname{sgn}(\text{input}_{i})
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.tensor([0.7, -1.2, 0., 2.3])
                >>> a
                tensor([ 0.7000, -1.2000,  0.0000,  2.3000])
                >>> torch.sign(a)
                tensor([ 1., -1.,  0.,  1.])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'sign')(*auto_generated_args, )
        ref_shape = size_mapping['sign'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "sign"
        return obj
    
    def sin(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.sin'. The automatically generated codes are as follows:
            [ 1]: def sin(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'sin')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['sin'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "sin"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            sin() -> Tensor
    
            See :func:`torch.sin`
    
            which is:
    
            sin(input, *, out=None) -> Tensor
    
            Returns a new tensor with the sine of the elements of :attr:`input`.
    
            .. math::
                \text{out}_{i} = \sin(\text{input}_{i})
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.randn(4)
                >>> a
                tensor([-0.5461,  0.1347, -2.7266, -0.2746])
                >>> torch.sin(a)
                tensor([-0.5194,  0.1343, -0.4032, -0.2711])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'sin')(*auto_generated_args, )
        ref_shape = size_mapping['sin'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "sin"
        return obj
    
    def cos(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.cos'. The automatically generated codes are as follows:
            [ 1]: def cos(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'cos')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['cos'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "cos"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            cos() -> Tensor
    
            See :func:`torch.cos`
    
            which is:
    
            cos(input, *, out=None) -> Tensor
    
            Returns a new tensor with the cosine  of the elements of :attr:`input`.
    
            .. math::
                \text{out}_{i} = \cos(\text{input}_{i})
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.randn(4)
                >>> a
                tensor([ 1.4309,  1.2706, -0.8562,  0.9796])
                >>> torch.cos(a)
                tensor([ 0.1395,  0.2957,  0.6553,  0.5574])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'cos')(*auto_generated_args, )
        ref_shape = size_mapping['cos'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "cos"
        return obj
    
    def tan(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.tan'. The automatically generated codes are as follows:
            [ 1]: def tan(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'tan')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['tan'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "tan"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            tan() -> Tensor
    
            See :func:`torch.tan`
    
            which is:
    
            tan(input, *, out=None) -> Tensor
    
            Returns a new tensor with the tangent of the elements of :attr:`input`.
    
            .. math::
                \text{out}_{i} = \tan(\text{input}_{i})
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.randn(4)
                >>> a
                tensor([-1.2027, -1.7687,  0.4412, -1.3856])
                >>> torch.tan(a)
                tensor([-2.5930,  4.9859,  0.4722, -5.3366])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'tan')(*auto_generated_args, )
        ref_shape = size_mapping['tan'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "tan"
        return obj
    
    def cot(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.cot'. The automatically generated codes are as follows:
            [ 1]: def cot(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     torch_returned = False
            [22]:     with torch._C.DisableTorchFunction():
            [23]:         obj = 1 / torch.tan(input)
            [24]:     ref_shape = size_mapping['cot'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "cot"
            [35]:     return obj
            [36]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        with torch._C.DisableTorchFunction():
            obj = 1 / torch.tan(input)
        ref_shape = size_mapping['cot'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "cot"
        return obj
    
    def sec(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.sec'. The automatically generated codes are as follows:
            [ 1]: def sec(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     torch_returned = False
            [22]:     with torch._C.DisableTorchFunction():
            [23]:         obj = 1 / torch.cos(input)
            [24]:     ref_shape = size_mapping['sec'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "sec"
            [35]:     return obj
            [36]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        with torch._C.DisableTorchFunction():
            obj = 1 / torch.cos(input)
        ref_shape = size_mapping['sec'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "sec"
        return obj
    
    def csc(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.csc'. The automatically generated codes are as follows:
            [ 1]: def csc(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     torch_returned = False
            [22]:     with torch._C.DisableTorchFunction():
            [23]:         obj = 1 / torch.sin(input)
            [24]:     ref_shape = size_mapping['csc'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "csc"
            [35]:     return obj
            [36]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        with torch._C.DisableTorchFunction():
            obj = 1 / torch.sin(input)
        ref_shape = size_mapping['csc'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "csc"
        return obj
    
    def asin(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.asin'. The automatically generated codes are as follows:
            [ 1]: def asin(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'asin')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['asin'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "asin"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            asin() -> Tensor
    
            See :func:`torch.asin`
    
            which is:
    
            asin(input, *, out=None) -> Tensor
    
            Returns a new tensor with the arcsine of the elements of :attr:`input`.
    
            .. math::
                \text{out}_{i} = \sin^{-1}(\text{input}_{i})
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.randn(4)
                >>> a
                tensor([-0.5962,  1.4985, -0.4396,  1.4525])
                >>> torch.asin(a)
                tensor([-0.6387,     nan, -0.4552,     nan])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'asin')(*auto_generated_args, )
        ref_shape = size_mapping['asin'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "asin"
        return obj
    
    def acos(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.acos'. The automatically generated codes are as follows:
            [ 1]: def acos(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'acos')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['acos'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "acos"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            acos() -> Tensor
    
            See :func:`torch.acos`
    
            which is:
    
            acos(input, *, out=None) -> Tensor
    
            Computes the inverse cosine of each element in :attr:`input`.
    
            .. math::
                \text{out}_{i} = \cos^{-1}(\text{input}_{i})
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.randn(4)
                >>> a
                tensor([ 0.3348, -0.5889,  0.2005, -0.1584])
                >>> torch.acos(a)
                tensor([ 1.2294,  2.2004,  1.3690,  1.7298])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'acos')(*auto_generated_args, )
        ref_shape = size_mapping['acos'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "acos"
        return obj
    
    def atan(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.atan'. The automatically generated codes are as follows:
            [ 1]: def atan(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'atan')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['atan'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "atan"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            atan() -> Tensor
    
            See :func:`torch.atan`
    
            which is:
    
            atan(input, *, out=None) -> Tensor
    
            Returns a new tensor with the arctangent of the elements of :attr:`input`.
    
            .. math::
                \text{out}_{i} = \tan^{-1}(\text{input}_{i})
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.randn(4)
                >>> a
                tensor([ 0.2341,  0.2539, -0.6256, -0.6448])
                >>> torch.atan(a)
                tensor([ 0.2299,  0.2487, -0.5591, -0.5727])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'atan')(*auto_generated_args, )
        ref_shape = size_mapping['atan'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "atan"
        return obj
    
    def arcsin(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.arcsin'. The automatically generated codes are as follows:
            [ 1]: def arcsin(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'arcsin')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['arcsin'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "arcsin"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            arcsin() -> Tensor
    
            See :func:`torch.arcsin`
    
            which is:
    
            arcsin(input, *, out=None) -> Tensor
    
            Alias for :func:`torch.asin`.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'arcsin')(*auto_generated_args, )
        ref_shape = size_mapping['arcsin'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "arcsin"
        return obj
    
    def arccos(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.arccos'. The automatically generated codes are as follows:
            [ 1]: def arccos(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'arccos')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['arccos'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "arccos"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            arccos() -> Tensor
    
            See :func:`torch.arccos`
    
            which is:
    
            arccos(input, *, out=None) -> Tensor
    
            Alias for :func:`torch.acos`.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'arccos')(*auto_generated_args, )
        ref_shape = size_mapping['arccos'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "arccos"
        return obj
    
    def arctan(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.arctan'. The automatically generated codes are as follows:
            [ 1]: def arctan(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'arctan')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['arctan'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "arctan"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            arctan() -> Tensor
    
            See :func:`torch.arctan`
    
            which is:
    
            arctan(input, *, out=None) -> Tensor
    
            Alias for :func:`torch.atan`.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'arctan')(*auto_generated_args, )
        ref_shape = size_mapping['arctan'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "arctan"
        return obj
    
    def mm(input: 'Tensor', other: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.mm'. The automatically generated codes are as follows:
            [ 1]: def mm(input: 'Tensor', other: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     input_shape=None if input is None else Size(input.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, input_shape, other_shape = size_mapping_op['mm'](input_shape, other_shape)
            [29]:     input = input.view(input_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(input, 'mm')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "mm"
            [44]:     return obj
            [45]:
            The document of the original function is:
    
            mm(mat2) -> Tensor
    
            See :func:`torch.mm`
    
            which is:
    
            mm(input, mat2, *, out=None) -> Tensor
    
            Performs a matrix multiplication of the matrices :attr:`input` and :attr:`mat2`.
    
            If :attr:`input` is a :math:`(n \times m)` tensor, :attr:`mat2` is a
            :math:`(m \times p)` tensor, :attr:`out` will be a :math:`(n \times p)` tensor.
    
            .. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.
                      For broadcasting matrix products, see :func:`torch.matmul`.
    
            Supports strided and sparse 2-D tensors as inputs, autograd with
            respect to strided inputs.
    
            This operation has support for arguments with :ref:`sparse layouts<sparse-docs>`.
            If :attr:`out` is provided it's layout will be used. Otherwise, the result
            layout will be deduced from that of :attr:`input`.
    
            .. warning::
                Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported,
                or may not have autograd support. If you notice missing functionality please
                open a feature request.
    
            This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
            On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.
    
            Args:
                input (Tensor): the first matrix to be matrix multiplied
                mat2 (Tensor): the second matrix to be matrix multiplied
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> mat1 = torch.randn(2, 3)
                >>> mat2 = torch.randn(3, 3)
                >>> torch.mm(mat1, mat2)
                tensor([[ 0.4851,  0.5037, -0.3633],
                        [-0.0760, -3.6705,  2.4784]])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, input_shape, other_shape = size_mapping_op['mm'](input_shape, other_shape)
        input = input.view(input_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(input, 'mm')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "mm"
        return obj
    
    def bmm(input: 'Tensor', other: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.bmm'. The automatically generated codes are as follows:
            [ 1]: def bmm(input: 'Tensor', other: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     input_shape=None if input is None else Size(input.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, input_shape, other_shape = size_mapping_op['bmm'](input_shape, other_shape)
            [29]:     input = input.view(input_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(input, 'bmm')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "bmm"
            [44]:     return obj
            [45]:
            The document of the original function is:
    
            bmm(batch2) -> Tensor
    
            See :func:`torch.bmm`
    
            which is:
    
            bmm(input, mat2, *, out=None) -> Tensor
    
            Performs a batch matrix-matrix product of matrices stored in :attr:`input`
            and :attr:`mat2`.
    
            :attr:`input` and :attr:`mat2` must be 3-D tensors each containing
            the same number of matrices.
    
            If :attr:`input` is a :math:`(b \times n \times m)` tensor, :attr:`mat2` is a
            :math:`(b \times m \times p)` tensor, :attr:`out` will be a
            :math:`(b \times n \times p)` tensor.
    
            .. math::
                \text{out}_i = \text{input}_i \mathbin{@} \text{mat2}_i
    
            This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
            On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.
    
            .. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.
                      For broadcasting matrix products, see :func:`torch.matmul`.
    
            Args:
                input (Tensor): the first batch of matrices to be multiplied
                mat2 (Tensor): the second batch of matrices to be multiplied
    
            Keyword Args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> input = torch.randn(10, 3, 4)
                >>> mat2 = torch.randn(10, 4, 5)
                >>> res = torch.bmm(input, mat2)
                >>> res.size()
                torch.Size([10, 3, 5])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, input_shape, other_shape = size_mapping_op['bmm'](input_shape, other_shape)
        input = input.view(input_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(input, 'bmm')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "bmm"
        return obj
    
    def smm(input: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.smm'. The automatically generated codes are as follows:
            [ 1]: def smm(input: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     input_shape=None if input is None else Size(input.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, input_shape, other_shape = size_mapping_op['smm'](input_shape, other_shape)
            [29]:     input = input.view(input_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(input, 'smm')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "smm"
            [44]:     return obj
            [45]:
            The document of the original function is:
    
            smm(mat) -> Tensor
    
            See :func:`torch.smm`
    
            which is:
    
            smm(input, mat) -> Tensor
    
            Performs a matrix multiplication of the sparse matrix :attr:`input`
            with the dense matrix :attr:`mat`.
    
            Args:
                input (Tensor): a sparse matrix to be matrix multiplied
                mat (Tensor): a dense matrix to be matrix multiplied
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, input_shape, other_shape = size_mapping_op['smm'](input_shape, other_shape)
        input = input.view(input_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(input, 'smm')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "smm"
        return obj
    
    def floor_divide(input: 'Tensor', other: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.floor_divide'. The automatically generated codes are as follows:
            [ 1]: def floor_divide(input: 'Tensor', other: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     input_shape=None if input is None else Size(input.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, input_shape, other_shape = size_mapping_op['floor_divide'](input_shape, other_shape)
            [29]:     input = input.view(input_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(input, 'floor_divide')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "floor_divide"
            [44]:     return obj
            [45]:
            The document of the original function is:
    
            floor_divide(value) -> Tensor
    
            See :func:`torch.floor_divide`
    
            which is:
    
            floor_divide(input, other, *, out=None) -> Tensor
    
            .. note::
    
                Before PyTorch 1.13 :func:`torch.floor_divide` incorrectly performed
                truncation division. To restore the previous behavior use
                :func:`torch.div` with ``rounding_mode='trunc'``.
    
            Computes :attr:`input` divided by :attr:`other`, elementwise, and floors
            the result.
    
            .. math::
                \text{{out}}_i = \text{floor} \left( \frac{{\text{{input}}_i}}{{\text{{other}}_i}} \right)
    
            Supports broadcasting to a common shape, type promotion, and integer and float inputs.
    
            Args:
                input (Tensor or Number): the dividend
                other (Tensor or Number): the divisor
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.tensor([4.0, 3.0])
                >>> b = torch.tensor([2.0, 2.0])
                >>> torch.floor_divide(a, b)
                tensor([2.0, 1.0])
                >>> torch.floor_divide(a, 1.4)
                tensor([2.0, 2.0])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, input_shape, other_shape = size_mapping_op['floor_divide'](input_shape, other_shape)
        input = input.view(input_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(input, 'floor_divide')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "floor_divide"
        return obj
    
    def true_divide(dividend: 'Tensor', divisor: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.true_divide'. The automatically generated codes are as follows:
            [ 1]: def true_divide(dividend: 'Tensor', divisor: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [dividend, divisor]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if dividend is None: ...
            [13]:     elif not isinstance(dividend, torch.Tensor): dividend = torch.tensor(dividend)
            [14]:     if not isinstance(dividend, subclass): dividend = dividend.as_subclass(subclass).special_from(dividend.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if dividend.device.type == 'cpu': dividend = dividend.to(pivot.device)
            [17]:
            [18]:     if divisor is None: ...
            [19]:     elif not isinstance(divisor, torch.Tensor): divisor = torch.tensor(divisor)
            [20]:     if not isinstance(divisor, subclass): divisor = divisor.as_subclass(subclass).special_from(divisor.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if divisor.device.type == 'cpu': divisor = divisor.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     dividend_shape=None if dividend is None else Size(dividend.shape)
            [26]:     divisor_shape=None if divisor is None else Size(divisor.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, dividend_shape, divisor_shape = size_mapping_op['true_divide'](dividend_shape, divisor_shape)
            [29]:     dividend = dividend.view(dividend_shape)
            [30]:     divisor = divisor.view(divisor_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [divisor] if x is not None)
            [33]:         obj = torch_super(dividend, 'true_divide')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "true_divide"
            [44]:     return obj
            [45]:
            The document of the original function is:
    
            true_divide(value) -> Tensor
    
            See :func:`torch.true_divide`
    
            which is:
    
            true_divide(dividend, divisor, *, out) -> Tensor
    
            Alias for :func:`torch.div` with ``rounding_mode=None``.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [dividend, divisor]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if dividend is None: ...
        elif not isinstance(dividend, torch.Tensor): dividend = torch.tensor(dividend)
        if not isinstance(dividend, subclass): dividend = dividend.as_subclass(subclass).special_from(dividend.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if dividend.device.type == 'cpu': dividend = dividend.to(pivot.device)
    
        if divisor is None: ...
        elif not isinstance(divisor, torch.Tensor): divisor = torch.tensor(divisor)
        if not isinstance(divisor, subclass): divisor = divisor.as_subclass(subclass).special_from(divisor.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if divisor.device.type == 'cpu': divisor = divisor.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        dividend_shape=None if dividend is None else Size(dividend.shape)
        divisor_shape=None if divisor is None else Size(divisor.shape)
        # Use the given inner codes if they are provided.
        ref_shape, dividend_shape, divisor_shape = size_mapping_op['true_divide'](dividend_shape, divisor_shape)
        dividend = dividend.view(dividend_shape)
        divisor = divisor.view(divisor_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [divisor] if x is not None)
            obj = torch_super(dividend, 'true_divide')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "true_divide"
        return obj
    
    def equal(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.equal'. The automatically generated codes are as follows:
            [ 1]: def equal(self: 'Tensor', other: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, self_shape, other_shape = size_mapping_op['equal'](self_shape, other_shape)
            [29]:     self = self.view(self_shape)
            [30]:     other = other.view(other_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [other] if x is not None)
            [33]:         obj = torch_super(self, 'equal')(*auto_generated_args, )
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "equal"
            [44]:     return obj
            [45]:
            The document of the original function is:
    
            equal(other) -> bool
    
            See :func:`torch.equal`
    
            which is:
    
            equal(input, other) -> bool
    
            ``True`` if two tensors have the same size and elements, ``False`` otherwise.
    
            Example::
    
                >>> torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2]))
                True
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        ref_shape, self_shape, other_shape = size_mapping_op['equal'](self_shape, other_shape)
        self = self.view(self_shape)
        other = other.view(other_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [other] if x is not None)
            obj = torch_super(self, 'equal')(*auto_generated_args, )
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "equal"
        return obj
    
    def addmm(input: 'Tensor', mat1: 'Tensor', mat2: 'Tensor', *, beta=1, alpha=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.addmm'. The automatically generated codes are as follows:
            [ 1]: def addmm(input: 'Tensor', mat1: 'Tensor', mat2: 'Tensor', *, beta=1, alpha=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input, mat1, mat2]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     if mat1 is None: ...
            [19]:     elif not isinstance(mat1, torch.Tensor): mat1 = torch.tensor(mat1)
            [20]:     if not isinstance(mat1, subclass): mat1 = mat1.as_subclass(subclass).special_from(mat1.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if mat1.device.type == 'cpu': mat1 = mat1.to(pivot.device)
            [23]:
            [24]:     if mat2 is None: ...
            [25]:     elif not isinstance(mat2, torch.Tensor): mat2 = torch.tensor(mat2)
            [26]:     if not isinstance(mat2, subclass): mat2 = mat2.as_subclass(subclass).special_from(mat2.shape)
            [27]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [28]:         if mat2.device.type == 'cpu': mat2 = mat2.to(pivot.device)
            [29]:
            [30]:     # Obtain available sized in arguments (which will be fed into size function).
            [31]:     input_shape=None if input is None else Size(input.shape)
            [32]:     mat1_shape=None if mat1 is None else Size(mat1.shape)
            [33]:     mat2_shape=None if mat2 is None else Size(mat2.shape)
            [34]:     # Use the given inner codes if they are provided.
            [35]:     ref_shape, input_shape, mat1_shape, mat2_shape = size_mapping_op['addmm'](input_shape, mat1_shape, mat2_shape)
            [36]:     input = input.view(input_shape)
            [37]:     mat1 = mat1.view(mat1_shape)
            [38]:     mat2 = mat2.view(mat2_shape)
            [39]:     with torch._C.DisableTorchFunction():
            [40]:         auto_generated_args = tuple(x for x in [mat1,mat2] if x is not None)
            [41]:         obj = torch_super(input, 'addmm')(*auto_generated_args, beta=beta,alpha=alpha)
            [42]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [43]:
            [44]:     def update_special(obj, updates):
            [45]:         if isinstance(obj, tuple):
            [46]:             for x in obj: update_special(x, updates)
            [47]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [48]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [49]:
            [50]:     if getattr(obj, 'grad_fn', None) is not None:
            [51]:         obj.grad_fn_name = "addmm"
            [52]:     return obj
            [53]:
            The document of the original function is:
    
            addmm(mat1, mat2, *, beta=1, alpha=1) -> Tensor
    
            See :func:`torch.addmm`
    
            which is:
    
            addmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) -> Tensor
    
            Performs a matrix multiplication of the matrices :attr:`mat1` and :attr:`mat2`.
            The matrix :attr:`input` is added to the final result.
    
            If :attr:`mat1` is a :math:`(n \times m)` tensor, :attr:`mat2` is a
            :math:`(m \times p)` tensor, then :attr:`input` must be
            :ref:`broadcastable <broadcasting-semantics>` with a :math:`(n \times p)` tensor
            and :attr:`out` will be a :math:`(n \times p)` tensor.
    
            :attr:`alpha` and :attr:`beta` are scaling factors on matrix-vector product between
            :attr:`mat1` and :attr:`mat2` and the added matrix :attr:`input` respectively.
    
            .. math::
                \text{out} = \beta\ \text{input} + \alpha\ (\text{mat1}_i \mathbin{@} \text{mat2}_i)
    
            If :attr:`beta` is 0, then :attr:`input` will be ignored, and `nan` and `inf` in
            it will not be propagated.
    
            For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and
            :attr:`alpha` must be real numbers, otherwise they should be integers.
    
            This operation has support for arguments with :ref:`sparse layouts<sparse-docs>`. If
            :attr:`input` is sparse the result will have the same layout and if :attr:`out`
            is provided it must have the same layout as :attr:`input`.
    
            .. warning::
                Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported,
                or may not have autograd support. If you notice missing functionality please
                open a feature request.
    
            This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
            On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.
    
            Args:
                input (Tensor): matrix to be added
                mat1 (Tensor): the first matrix to be matrix multiplied
                mat2 (Tensor): the second matrix to be matrix multiplied
    
            Keyword args:
                beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
                alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`)
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> M = torch.randn(2, 3)
                >>> mat1 = torch.randn(2, 3)
                >>> mat2 = torch.randn(3, 3)
                >>> torch.addmm(M, mat1, mat2)
                tensor([[-4.8716,  1.4671, -1.3746],
                        [ 0.7573, -3.9555, -2.8681]])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input, mat1, mat2]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        if mat1 is None: ...
        elif not isinstance(mat1, torch.Tensor): mat1 = torch.tensor(mat1)
        if not isinstance(mat1, subclass): mat1 = mat1.as_subclass(subclass).special_from(mat1.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if mat1.device.type == 'cpu': mat1 = mat1.to(pivot.device)
    
        if mat2 is None: ...
        elif not isinstance(mat2, torch.Tensor): mat2 = torch.tensor(mat2)
        if not isinstance(mat2, subclass): mat2 = mat2.as_subclass(subclass).special_from(mat2.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if mat2.device.type == 'cpu': mat2 = mat2.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        mat1_shape=None if mat1 is None else Size(mat1.shape)
        mat2_shape=None if mat2 is None else Size(mat2.shape)
        # Use the given inner codes if they are provided.
        ref_shape, input_shape, mat1_shape, mat2_shape = size_mapping_op['addmm'](input_shape, mat1_shape, mat2_shape)
        input = input.view(input_shape)
        mat1 = mat1.view(mat1_shape)
        mat2 = mat2.view(mat2_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [mat1,mat2] if x is not None)
            obj = torch_super(input, 'addmm')(*auto_generated_args, beta=beta,alpha=alpha)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "addmm"
        return obj
    
    def addbmm(input: 'Tensor', batch1: 'Tensor', batch2: 'Tensor', *, beta=1, alpha=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.addbmm'. The automatically generated codes are as follows:
            [ 1]: def addbmm(input: 'Tensor', batch1: 'Tensor', batch2: 'Tensor', *, beta=1, alpha=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input, batch1, batch2]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     if batch1 is None: ...
            [19]:     elif not isinstance(batch1, torch.Tensor): batch1 = torch.tensor(batch1)
            [20]:     if not isinstance(batch1, subclass): batch1 = batch1.as_subclass(subclass).special_from(batch1.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if batch1.device.type == 'cpu': batch1 = batch1.to(pivot.device)
            [23]:
            [24]:     if batch2 is None: ...
            [25]:     elif not isinstance(batch2, torch.Tensor): batch2 = torch.tensor(batch2)
            [26]:     if not isinstance(batch2, subclass): batch2 = batch2.as_subclass(subclass).special_from(batch2.shape)
            [27]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [28]:         if batch2.device.type == 'cpu': batch2 = batch2.to(pivot.device)
            [29]:
            [30]:     # Obtain available sized in arguments (which will be fed into size function).
            [31]:     input_shape=None if input is None else Size(input.shape)
            [32]:     batch1_shape=None if batch1 is None else Size(batch1.shape)
            [33]:     batch2_shape=None if batch2 is None else Size(batch2.shape)
            [34]:     # Use the given inner codes if they are provided.
            [35]:     torch_returned = False
            [36]:     ref_shape, input_shape, batch1_shape, batch2_shape = size_mapping_op['addbmm'](input_shape, batch1_shape, batch2_shape)
            [37]:     input = input.view(input_shape).squeeze({})
            [38]:     batch1 = batch1.view(batch1_shape)
            [39]:     batch2 = batch2.view(batch2_shape)
            [40]:     with torch._C.DisableTorchFunction():
            [41]:         obj = Tensor.inherit_from(torch.addbmm(input,batch1,batch2, beta=beta,alpha=alpha,out=out), input)
            [42]:
            [43]:     def update_special(obj, updates):
            [44]:         if isinstance(obj, tuple):
            [45]:             for x in obj: update_special(x, updates)
            [46]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [47]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [48]:
            [49]:     if getattr(obj, 'grad_fn', None) is not None:
            [50]:         obj.grad_fn_name = "addbmm"
            [51]:     return obj
            [52]:
            The document of the original function is:
    
            addbmm(batch1, batch2, *, beta=1, alpha=1) -> Tensor
    
            See :func:`torch.addbmm`
    
            which is:
    
            addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) -> Tensor
    
            Performs a batch matrix-matrix product of matrices stored
            in :attr:`batch1` and :attr:`batch2`,
            with a reduced add step (all matrix multiplications get accumulated
            along the first dimension).
            :attr:`input` is added to the final result.
    
            :attr:`batch1` and :attr:`batch2` must be 3-D tensors each containing the
            same number of matrices.
    
            If :attr:`batch1` is a :math:`(b \times n \times m)` tensor, :attr:`batch2` is a
            :math:`(b \times m \times p)` tensor, :attr:`input` must be
            :ref:`broadcastable <broadcasting-semantics>` with a :math:`(n \times p)` tensor
            and :attr:`out` will be a :math:`(n \times p)` tensor.
    
            .. math::
                out = \beta\ \text{input} + \alpha\ (\sum_{i=0}^{b-1} \text{batch1}_i \mathbin{@} \text{batch2}_i)
    
            If :attr:`beta` is 0, then :attr:`input` will be ignored, and `nan` and `inf` in
            it will not be propagated.
    
            For inputs of type `FloatTensor` or `DoubleTensor`, arguments :attr:`beta` and :attr:`alpha`
            must be real numbers, otherwise they should be integers.
    
            This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
            On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.
    
            Args:
                batch1 (Tensor): the first batch of matrices to be multiplied
                batch2 (Tensor): the second batch of matrices to be multiplied
    
            Keyword args:
                beta (Number, optional): multiplier for :attr:`input` (:math:`\beta`)
                input (Tensor): matrix to be added
                alpha (Number, optional): multiplier for `batch1 @ batch2` (:math:`\alpha`)
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> M = torch.randn(3, 5)
                >>> batch1 = torch.randn(10, 3, 4)
                >>> batch2 = torch.randn(10, 4, 5)
                >>> torch.addbmm(M, batch1, batch2)
                tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],
                        [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],
                        [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input, batch1, batch2]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        if batch1 is None: ...
        elif not isinstance(batch1, torch.Tensor): batch1 = torch.tensor(batch1)
        if not isinstance(batch1, subclass): batch1 = batch1.as_subclass(subclass).special_from(batch1.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if batch1.device.type == 'cpu': batch1 = batch1.to(pivot.device)
    
        if batch2 is None: ...
        elif not isinstance(batch2, torch.Tensor): batch2 = torch.tensor(batch2)
        if not isinstance(batch2, subclass): batch2 = batch2.as_subclass(subclass).special_from(batch2.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if batch2.device.type == 'cpu': batch2 = batch2.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        batch1_shape=None if batch1 is None else Size(batch1.shape)
        batch2_shape=None if batch2 is None else Size(batch2.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        ref_shape, input_shape, batch1_shape, batch2_shape = size_mapping_op['addbmm'](input_shape, batch1_shape, batch2_shape)
        input = input.view(input_shape).squeeze({})
        batch1 = batch1.view(batch1_shape)
        batch2 = batch2.view(batch2_shape)
        with torch._C.DisableTorchFunction():
            obj = Tensor.inherit_from(torch.addbmm(input,batch1,batch2, beta=beta,alpha=alpha,out=out), input)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "addbmm"
        return obj
    
    def saddmm(input: 'Tensor', mat1: 'Tensor', mat2: 'Tensor', *, beta=1, alpha=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.saddmm'. The automatically generated codes are as follows:
            [ 1]: def saddmm(input: 'Tensor', mat1: 'Tensor', mat2: 'Tensor', *, beta=1, alpha=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input, mat1, mat2]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     if mat1 is None: ...
            [19]:     elif not isinstance(mat1, torch.Tensor): mat1 = torch.tensor(mat1)
            [20]:     if not isinstance(mat1, subclass): mat1 = mat1.as_subclass(subclass).special_from(mat1.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if mat1.device.type == 'cpu': mat1 = mat1.to(pivot.device)
            [23]:
            [24]:     if mat2 is None: ...
            [25]:     elif not isinstance(mat2, torch.Tensor): mat2 = torch.tensor(mat2)
            [26]:     if not isinstance(mat2, subclass): mat2 = mat2.as_subclass(subclass).special_from(mat2.shape)
            [27]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [28]:         if mat2.device.type == 'cpu': mat2 = mat2.to(pivot.device)
            [29]:
            [30]:     # Obtain available sized in arguments (which will be fed into size function).
            [31]:     input_shape=None if input is None else Size(input.shape)
            [32]:     mat1_shape=None if mat1 is None else Size(mat1.shape)
            [33]:     mat2_shape=None if mat2 is None else Size(mat2.shape)
            [34]:     # Use the given inner codes if they are provided.
            [35]:     ref_shape, input_shape, mat1_shape, mat2_shape = size_mapping_op['saddmm'](input_shape, mat1_shape, mat2_shape)
            [36]:     input = input.view(input_shape)
            [37]:     mat1 = mat1.view(mat1_shape)
            [38]:     mat2 = mat2.view(mat2_shape)
            [39]:     with torch._C.DisableTorchFunction():
            [40]:         auto_generated_args = tuple(x for x in [mat1,mat2] if x is not None)
            [41]:         obj = torch_super(input, 'saddmm')(*auto_generated_args, beta=beta,alpha=alpha)
            [42]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [43]:
            [44]:     def update_special(obj, updates):
            [45]:         if isinstance(obj, tuple):
            [46]:             for x in obj: update_special(x, updates)
            [47]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [48]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [49]:
            [50]:     if getattr(obj, 'grad_fn', None) is not None:
            [51]:         obj.grad_fn_name = "saddmm"
            [52]:     return obj
            [53]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input, mat1, mat2]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        if mat1 is None: ...
        elif not isinstance(mat1, torch.Tensor): mat1 = torch.tensor(mat1)
        if not isinstance(mat1, subclass): mat1 = mat1.as_subclass(subclass).special_from(mat1.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if mat1.device.type == 'cpu': mat1 = mat1.to(pivot.device)
    
        if mat2 is None: ...
        elif not isinstance(mat2, torch.Tensor): mat2 = torch.tensor(mat2)
        if not isinstance(mat2, subclass): mat2 = mat2.as_subclass(subclass).special_from(mat2.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if mat2.device.type == 'cpu': mat2 = mat2.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        mat1_shape=None if mat1 is None else Size(mat1.shape)
        mat2_shape=None if mat2 is None else Size(mat2.shape)
        # Use the given inner codes if they are provided.
        ref_shape, input_shape, mat1_shape, mat2_shape = size_mapping_op['saddmm'](input_shape, mat1_shape, mat2_shape)
        input = input.view(input_shape)
        mat1 = mat1.view(mat1_shape)
        mat2 = mat2.view(mat2_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [mat1,mat2] if x is not None)
            obj = torch_super(input, 'saddmm')(*auto_generated_args, beta=beta,alpha=alpha)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "saddmm"
        return obj
    
    def addcmul(input: 'Tensor', tensor1: 'Tensor', tensor2: 'Tensor', *, value=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.addcmul'. The automatically generated codes are as follows:
            [ 1]: def addcmul(input: 'Tensor', tensor1: 'Tensor', tensor2: 'Tensor', *, value=1, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input, tensor1, tensor2]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     if tensor1 is None: ...
            [19]:     elif not isinstance(tensor1, torch.Tensor): tensor1 = torch.tensor(tensor1)
            [20]:     if not isinstance(tensor1, subclass): tensor1 = tensor1.as_subclass(subclass).special_from(tensor1.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if tensor1.device.type == 'cpu': tensor1 = tensor1.to(pivot.device)
            [23]:
            [24]:     if tensor2 is None: ...
            [25]:     elif not isinstance(tensor2, torch.Tensor): tensor2 = torch.tensor(tensor2)
            [26]:     if not isinstance(tensor2, subclass): tensor2 = tensor2.as_subclass(subclass).special_from(tensor2.shape)
            [27]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [28]:         if tensor2.device.type == 'cpu': tensor2 = tensor2.to(pivot.device)
            [29]:
            [30]:     # Obtain available sized in arguments (which will be fed into size function).
            [31]:     input_shape=None if input is None else Size(input.shape)
            [32]:     tensor1_shape=None if tensor1 is None else Size(tensor1.shape)
            [33]:     tensor2_shape=None if tensor2 is None else Size(tensor2.shape)
            [34]:     # Use the given inner codes if they are provided.
            [35]:     ref_shape, input_shape, tensor1_shape, tensor2_shape = size_mapping_op['addcmul'](input_shape, tensor1_shape, tensor2_shape)
            [36]:     input = input.view(input_shape)
            [37]:     tensor1 = tensor1.view(tensor1_shape)
            [38]:     tensor2 = tensor2.view(tensor2_shape)
            [39]:     with torch._C.DisableTorchFunction():
            [40]:         auto_generated_args = tuple(x for x in [tensor1,tensor2] if x is not None)
            [41]:         obj = torch_super(input, 'addcmul')(*auto_generated_args, value=value)
            [42]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [43]:
            [44]:     def update_special(obj, updates):
            [45]:         if isinstance(obj, tuple):
            [46]:             for x in obj: update_special(x, updates)
            [47]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [48]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [49]:
            [50]:     if getattr(obj, 'grad_fn', None) is not None:
            [51]:         obj.grad_fn_name = "addcmul"
            [52]:     return obj
            [53]:
            The document of the original function is:
    
            addcmul(tensor1, tensor2, *, value=1) -> Tensor
    
            See :func:`torch.addcmul`
    
            which is:
    
            addcmul(input, tensor1, tensor2, *, value=1, out=None) -> Tensor
    
            Performs the element-wise multiplication of :attr:`tensor1`
            by :attr:`tensor2`, multiplies the result by the scalar :attr:`value`
            and adds it to :attr:`input`.
    
            .. math::
                \text{out}_i = \text{input}_i + \text{value} \times \text{tensor1}_i \times \text{tensor2}_i
    
            The shapes of :attr:`tensor`, :attr:`tensor1`, and :attr:`tensor2` must be
            :ref:`broadcastable <broadcasting-semantics>`.
    
            For inputs of type `FloatTensor` or `DoubleTensor`, :attr:`value` must be
            a real number, otherwise an integer.
    
            Args:
                input (Tensor): the tensor to be added
                tensor1 (Tensor): the tensor to be multiplied
                tensor2 (Tensor): the tensor to be multiplied
    
            Keyword args:
                value (Number, optional): multiplier for :math:`tensor1 .* tensor2`
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> t = torch.randn(1, 3)
                >>> t1 = torch.randn(3, 1)
                >>> t2 = torch.randn(1, 3)
                >>> torch.addcmul(t, t1, t2, value=0.1)
                tensor([[-0.8635, -0.6391,  1.6174],
                        [-0.7617, -0.5879,  1.7388],
                        [-0.8353, -0.6249,  1.6511]])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input, tensor1, tensor2]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        if tensor1 is None: ...
        elif not isinstance(tensor1, torch.Tensor): tensor1 = torch.tensor(tensor1)
        if not isinstance(tensor1, subclass): tensor1 = tensor1.as_subclass(subclass).special_from(tensor1.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if tensor1.device.type == 'cpu': tensor1 = tensor1.to(pivot.device)
    
        if tensor2 is None: ...
        elif not isinstance(tensor2, torch.Tensor): tensor2 = torch.tensor(tensor2)
        if not isinstance(tensor2, subclass): tensor2 = tensor2.as_subclass(subclass).special_from(tensor2.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if tensor2.device.type == 'cpu': tensor2 = tensor2.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        tensor1_shape=None if tensor1 is None else Size(tensor1.shape)
        tensor2_shape=None if tensor2 is None else Size(tensor2.shape)
        # Use the given inner codes if they are provided.
        ref_shape, input_shape, tensor1_shape, tensor2_shape = size_mapping_op['addcmul'](input_shape, tensor1_shape, tensor2_shape)
        input = input.view(input_shape)
        tensor1 = tensor1.view(tensor1_shape)
        tensor2 = tensor2.view(tensor2_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [tensor1,tensor2] if x is not None)
            obj = torch_super(input, 'addcmul')(*auto_generated_args, value=value)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "addcmul"
        return obj
    
    def clamp(input: 'Tensor', min=None, max=None, *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.clamp'. The automatically generated codes are as follows:
            [ 1]: def clamp(input: 'Tensor', min=None, max=None, *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'clamp')(*auto_generated_args, min=min,max=max)
            [24]:     ref_shape = size_mapping['clamp'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "clamp"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            clamp(min=None, max=None) -> Tensor
    
            See :func:`torch.clamp`
    
            which is:
    
            clamp(input, min=None, max=None, *, out=None) -> Tensor
    
            Clamps all elements in :attr:`input` into the range `[` :attr:`min`, :attr:`max` `]`.
            Letting min_value and max_value be :attr:`min` and :attr:`max`, respectively, this returns:
    
            .. math::
                y_i = \min(\max(x_i, \text{min\_value}_i), \text{max\_value}_i)
    
            If :attr:`min` is ``None``, there is no lower bound.
            Or, if :attr:`max` is ``None`` there is no upper bound.
    
            .. note::
                If :attr:`min` is greater than :attr:`max` :func:`torch.clamp(..., min, max) <torch.clamp>`
                sets all elements in :attr:`input` to the value of :attr:`max`.
    
            Args:
                input (Tensor): the input tensor.
                min (Number or Tensor, optional): lower-bound of the range to be clamped to
                max (Number or Tensor, optional): upper-bound of the range to be clamped to
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.randn(4)
                >>> a
                tensor([-1.7120,  0.1734, -0.0478, -0.0922])
                >>> torch.clamp(a, min=-0.5, max=0.5)
                tensor([-0.5000,  0.1734, -0.0478, -0.0922])
    
                >>> min = torch.linspace(-1, 1, steps=4)
                >>> torch.clamp(a, min=min)
                tensor([-1.0000,  0.1734,  0.3333,  1.0000])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'clamp')(*auto_generated_args, min=min,max=max)
        ref_shape = size_mapping['clamp'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "clamp"
        return obj
    
    def floor(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.floor'. The automatically generated codes are as follows:
            [ 1]: def floor(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'floor')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['floor'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "floor"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            floor() -> Tensor
    
            See :func:`torch.floor`
    
            which is:
    
            floor(input, *, out=None) -> Tensor
    
            Returns a new tensor with the floor of the elements of :attr:`input`,
            the largest integer less than or equal to each element.
    
            For integer inputs, follows the array-api convention of returning a
            copy of the input tensor.
    
            .. math::
                \text{out}_{i} = \left\lfloor \text{input}_{i} \right\rfloor
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.randn(4)
                >>> a
                tensor([-0.8166,  1.5308, -0.2530, -0.2091])
                >>> torch.floor(a)
                tensor([-1.,  1., -1., -1.])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'floor')(*auto_generated_args, )
        ref_shape = size_mapping['floor'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "floor"
        return obj
    
    def ceil(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.ceil'. The automatically generated codes are as follows:
            [ 1]: def ceil(input: 'Tensor', *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'ceil')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['ceil'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "ceil"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            ceil() -> Tensor
    
            See :func:`torch.ceil`
    
            which is:
    
            ceil(input, *, out=None) -> Tensor
    
            Returns a new tensor with the ceil of the elements of :attr:`input`,
            the smallest integer greater than or equal to each element.
    
            For integer inputs, follows the array-api convention of returning a
            copy of the input tensor.
    
            .. math::
                \text{out}_{i} = \left\lceil \text{input}_{i} \right\rceil
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.randn(4)
                >>> a
                tensor([-0.6341, -1.4208, -1.0900,  0.5826])
                >>> torch.ceil(a)
                tensor([-0., -1., -1.,  1.])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'ceil')(*auto_generated_args, )
        ref_shape = size_mapping['ceil'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "ceil"
        return obj
    
    def round(input: 'Tensor', *, decimals=0, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.round'. The automatically generated codes are as follows:
            [ 1]: def round(input: 'Tensor', *, decimals=0, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'round')(*auto_generated_args, decimals=decimals)
            [24]:     ref_shape = size_mapping['round'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "round"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            round(decimals=0) -> Tensor
    
            See :func:`torch.round`
    
            which is:
    
            round(input, *, decimals=0, out=None) -> Tensor
    
            Rounds elements of :attr:`input` to the nearest integer.
    
            For integer inputs, follows the array-api convention of returning a
            copy of the input tensor.
    
            .. note::
                This function implements the "round half to even" to
                break ties when a number is equidistant from two
                integers (e.g. `round(2.5)` is 2).
    
                When the :attr:\`decimals\` argument is specified the
                algorithm used is similar to NumPy's `around`. This
                algorithm is fast but inexact and it can easily
                overflow for low precision dtypes.
                Eg. `round(tensor([10000], dtype=torch.float16), decimals=3)` is `inf`.
    
            .. seealso::
                :func:`torch.ceil`, which rounds up.
                :func:`torch.floor`, which rounds down.
                :func:`torch.trunc`, which rounds towards zero.
    
            Args:
                input (Tensor): the input tensor.
                decimals (int): Number of decimal places to round to (default: 0).
                    If decimals is negative, it specifies the number of positions
                    to the left of the decimal point.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> torch.round(torch.tensor((4.7, -2.3, 9.1, -7.7)))
                tensor([ 5.,  -2.,  9., -8.])
    
                >>> # Values equidistant from two integers are rounded towards the
                >>> #   the nearest even value (zero is treated as even)
                >>> torch.round(torch.tensor([-0.5, 0.5, 1.5, 2.5]))
                tensor([-0., 0., 2., 2.])
    
                >>> # A positive decimals argument rounds to the to that decimal place
                >>> torch.round(torch.tensor([0.1234567]), decimals=3)
                tensor([0.1230])
    
                >>> # A negative decimals argument rounds to the left of the decimal
                >>> torch.round(torch.tensor([1200.1234567]), decimals=-3)
                tensor([1000.])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'round')(*auto_generated_args, decimals=decimals)
        ref_shape = size_mapping['round'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "round"
        return obj
    
    def any(input: 'Tensor', *dims: del_dim[...], keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.any'. The automatically generated codes are as follows:
            [ 1]: def any(input: 'Tensor', *dims: del_dim[...], keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     dims = del_dim[...](input, *dims)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     input_shape=None if input is None else Size(input.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     with torch._C.DisableTorchFunction():
            [24]:         for dims_iter, in zip(dims[::-1]):
            [25]:             auto_generated_args = tuple(x for x in [] if x is not None)
            [26]:             input = torch_super(input, 'any')(*auto_generated_args, dims_iter,keepdim=keepdim)
            [27]:     obj = input
            [28]:     ref_shape = size_mapping['any'](input_shape, dims, **dict(keepdim=keepdim))
            [29]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [30]:
            [31]:     def update_special(obj, updates):
            [32]:         if isinstance(obj, tuple):
            [33]:             for x in obj: update_special(x, updates)
            [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [36]:
            [37]:     if getattr(obj, 'grad_fn', None) is not None:
            [38]:         obj.grad_fn_name = "any"
            [39]:     return obj
            [40]:
            The document of the original function is:
    
            any(dim=None, keepdim=False) -> Tensor
    
            See :func:`torch.any`
    
            which is:
    
            any(input) -> Tensor
    
            Tests if any element in :attr:`input` evaluates to `True`.
    
            .. note:: This function matches the behaviour of NumPy in returning
                      output of dtype `bool` for all supported dtypes except `uint8`.
                      For `uint8` the dtype of output is `uint8` itself.
    
            Example::
    
                >>> a = torch.rand(1, 2).bool()
                >>> a
                tensor([[False, True]], dtype=torch.bool)
                >>> torch.any(a)
                tensor(True, dtype=torch.bool)
                >>> a = torch.arange(0, 3)
                >>> a
                tensor([0, 1, 2])
                >>> torch.any(a)
                tensor(True)
    
            .. function:: any(input, dim, keepdim=False, *, out=None) -> Tensor
               :noindex:
    
            For each row of :attr:`input` in the given dimension :attr:`dim`,
            returns `True` if any element in the row evaluate to `True` and `False` otherwise.
    
            If :attr:`keepdim` is ``True``, the output tensor is of the same size
            as :attr:`input` except in the dimension :attr:`dim` where it is of size 1.
            Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in
            the output tensor having 1 fewer dimension than :attr:`input`.
    
            Args:
                input (Tensor): the input tensor.
                dim (int): the dimension to reduce.
                keepdim (bool): whether the output tensor has :attr:`dim` retained or not.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.randn(4, 2) < 0
                >>> a
                tensor([[ True,  True],
                        [False,  True],
                        [ True,  True],
                        [False, False]])
                >>> torch.any(a, 1)
                tensor([ True,  True,  True, False])
                >>> torch.any(a, 0)
                tensor([True, True])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dims = del_dim[...](input, *dims)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            for dims_iter, in zip(dims[::-1]):
                auto_generated_args = tuple(x for x in [] if x is not None)
                input = torch_super(input, 'any')(*auto_generated_args, dims_iter,keepdim=keepdim)
        obj = input
        ref_shape = size_mapping['any'](input_shape, dims, **dict(keepdim=keepdim))
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "any"
        return obj
    
    def all(input: 'Tensor', *dims: del_dim[...], keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.all'. The automatically generated codes are as follows:
            [ 1]: def all(input: 'Tensor', *dims: del_dim[...], keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     dims = del_dim[...](input, *dims)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     input_shape=None if input is None else Size(input.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     with torch._C.DisableTorchFunction():
            [24]:         for dims_iter, in zip(dims[::-1]):
            [25]:             auto_generated_args = tuple(x for x in [] if x is not None)
            [26]:             input = torch_super(input, 'all')(*auto_generated_args, dims_iter,keepdim=keepdim)
            [27]:     obj = input
            [28]:     ref_shape = size_mapping['all'](input_shape, dims, **dict(keepdim=keepdim))
            [29]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [30]:
            [31]:     def update_special(obj, updates):
            [32]:         if isinstance(obj, tuple):
            [33]:             for x in obj: update_special(x, updates)
            [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [36]:
            [37]:     if getattr(obj, 'grad_fn', None) is not None:
            [38]:         obj.grad_fn_name = "all"
            [39]:     return obj
            [40]:
            The document of the original function is:
    
            all(dim=None, keepdim=False) -> Tensor
    
            See :func:`torch.all`
    
            which is:
    
            all(input) -> Tensor
    
            Tests if all elements in :attr:`input` evaluate to `True`.
    
            .. note:: This function matches the behaviour of NumPy in returning
                      output of dtype `bool` for all supported dtypes except `uint8`.
                      For `uint8` the dtype of output is `uint8` itself.
    
            Example::
    
                >>> a = torch.rand(1, 2).bool()
                >>> a
                tensor([[False, True]], dtype=torch.bool)
                >>> torch.all(a)
                tensor(False, dtype=torch.bool)
                >>> a = torch.arange(0, 3)
                >>> a
                tensor([0, 1, 2])
                >>> torch.all(a)
                tensor(False)
    
            .. function:: all(input, dim, keepdim=False, *, out=None) -> Tensor
               :noindex:
    
            For each row of :attr:`input` in the given dimension :attr:`dim`,
            returns `True` if all elements in the row evaluate to `True` and `False` otherwise.
    
            If :attr:`keepdim` is ``True``, the output tensor is of the same size
            as :attr:`input` except in the dimension :attr:`dim` where it is of size 1.
            Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in
            the output tensor having 1 fewer dimension than :attr:`input`.
    
            Args:
                input (Tensor): the input tensor.
                dim (int): the dimension to reduce.
                keepdim (bool): whether the output tensor has :attr:`dim` retained or not.
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.rand(4, 2).bool()
                >>> a
                tensor([[True, True],
                        [True, False],
                        [True, True],
                        [True, True]], dtype=torch.bool)
                >>> torch.all(a, dim=1)
                tensor([ True, False,  True,  True], dtype=torch.bool)
                >>> torch.all(a, dim=0)
                tensor([ True, False], dtype=torch.bool)
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dims = del_dim[...](input, *dims)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            for dims_iter, in zip(dims[::-1]):
                auto_generated_args = tuple(x for x in [] if x is not None)
                input = torch_super(input, 'all')(*auto_generated_args, dims_iter,keepdim=keepdim)
        obj = input
        ref_shape = size_mapping['all'](input_shape, dims, **dict(keepdim=keepdim))
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "all"
        return obj
    
    def unique(input: 'Tensor', sorted=True, return_inverse=False, return_counts=False, dim=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.unique'. The automatically generated codes are as follows:
            [ 1]: def unique(input: 'Tensor', sorted=True, return_inverse=False, return_counts=False, dim=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     torch_returned = False
            [22]:     with torch._C.DisableTorchFunction():
            [23]:         ret = torch.unique(input, sorted=sorted,return_inverse=return_inverse,return_counts=return_counts,dim=dim)
            [24]:
            [25]:     if isinstance(ret, tuple):
            [26]:         if len(ret) >= 1 and ret[0].ndim == 1:  ret[0] = Tensor.inherit_from(ret[0], input, shape=...)
            [27]:         elif len(ret) >= 1:  ret[0] = Tensor.inherit_from(ret[0], input, shape=input_shape)
            [28]:         if len(ret) >= 2 and return_inverse:  ret[1] = Tensor.inherit_from(ret[1], input, shape=input_shape)
            [29]:         elif len(ret) >= 2:  ret[1] = Tensor.inherit_from(ret[1], input, shape=...)
            [30]:         if len(ret) >= 3:  ret[2] = Tensor.inherit_from(ret[2], input, shape=...)
            [31]:         obj = ret
            [32]:     elif ret.ndim == 1:
            [33]:         obj = Tensor.inherit_from(ret, input, shape=[])
            [34]:     else: torch_returned = True; obj = Tensor.inherit_from(ret, input, shape=input_shape)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "unique"
            [44]:     return obj
            [45]:
            The document of the original function is:
            Returns the unique elements of the input tensor.
    
                    See :func:`torch.unique`
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        with torch._C.DisableTorchFunction():
            ret = torch.unique(input, sorted=sorted,return_inverse=return_inverse,return_counts=return_counts,dim=dim)
    
        if isinstance(ret, tuple):
            if len(ret) >= 1 and ret[0].ndim == 1:  ret[0] = Tensor.inherit_from(ret[0], input, shape=...)
            elif len(ret) >= 1:  ret[0] = Tensor.inherit_from(ret[0], input, shape=input_shape)
            if len(ret) >= 2 and return_inverse:  ret[1] = Tensor.inherit_from(ret[1], input, shape=input_shape)
            elif len(ret) >= 2:  ret[1] = Tensor.inherit_from(ret[1], input, shape=...)
            if len(ret) >= 3:  ret[2] = Tensor.inherit_from(ret[2], input, shape=...)
            obj = ret
        elif ret.ndim == 1:
            obj = Tensor.inherit_from(ret, input, shape=[])
        else: torch_returned = True; obj = Tensor.inherit_from(ret, input, shape=input_shape)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "unique"
        return obj
    
    def isin(elements: 'Tensor', test_elements: 'Tensor', *, assume_unique=False, invert=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.isin'. The automatically generated codes are as follows:
            [ 1]: def isin(elements: 'Tensor', test_elements: 'Tensor', *, assume_unique=False, invert=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [elements, test_elements]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if elements is None: ...
            [13]:     elif not isinstance(elements, torch.Tensor): elements = torch.tensor(elements)
            [14]:     if not isinstance(elements, subclass): elements = elements.as_subclass(subclass).special_from(elements.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if elements.device.type == 'cpu': elements = elements.to(pivot.device)
            [17]:
            [18]:     if test_elements is None: ...
            [19]:     elif not isinstance(test_elements, torch.Tensor): test_elements = torch.tensor(test_elements)
            [20]:     if not isinstance(test_elements, subclass): test_elements = test_elements.as_subclass(subclass).special_from(test_elements.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if test_elements.device.type == 'cpu': test_elements = test_elements.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     elements_shape=None if elements is None else Size(elements.shape)
            [26]:     test_elements_shape=None if test_elements is None else Size(test_elements.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     ref_shape, elements_shape, test_elements_shape = size_mapping_op['isin'](elements_shape, test_elements_shape)
            [29]:     elements = elements.view(elements_shape)
            [30]:     test_elements = test_elements.view(test_elements_shape)
            [31]:     with torch._C.DisableTorchFunction():
            [32]:         auto_generated_args = tuple(x for x in [test_elements] if x is not None)
            [33]:         obj = torch_super(elements, 'isin')(*auto_generated_args, assume_unique=assume_unique,invert=invert)
            [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "isin"
            [44]:     return obj
            [45]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [elements, test_elements]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if elements is None: ...
        elif not isinstance(elements, torch.Tensor): elements = torch.tensor(elements)
        if not isinstance(elements, subclass): elements = elements.as_subclass(subclass).special_from(elements.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if elements.device.type == 'cpu': elements = elements.to(pivot.device)
    
        if test_elements is None: ...
        elif not isinstance(test_elements, torch.Tensor): test_elements = torch.tensor(test_elements)
        if not isinstance(test_elements, subclass): test_elements = test_elements.as_subclass(subclass).special_from(test_elements.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if test_elements.device.type == 'cpu': test_elements = test_elements.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        elements_shape=None if elements is None else Size(elements.shape)
        test_elements_shape=None if test_elements is None else Size(test_elements.shape)
        # Use the given inner codes if they are provided.
        ref_shape, elements_shape, test_elements_shape = size_mapping_op['isin'](elements_shape, test_elements_shape)
        elements = elements.view(elements_shape)
        test_elements = test_elements.view(test_elements_shape)
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [test_elements] if x is not None)
            obj = torch_super(elements, 'isin')(*auto_generated_args, assume_unique=assume_unique,invert=invert)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "isin"
        return obj
    
    def isnan(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.isnan'. The automatically generated codes are as follows:
            [ 1]: def isnan(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'isnan')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['isnan'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "isnan"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            isnan() -> Tensor
    
            See :func:`torch.isnan`
    
            which is:
    
            isnan(input) -> Tensor
    
            Returns a new tensor with boolean elements representing if each element of :attr:`input`
            is NaN or not. Complex values are considered NaN when either their real
            and/or imaginary part is NaN.
    
            Arguments:
                input (Tensor): the input tensor.
    
            Returns:
                A boolean tensor that is True where :attr:`input` is NaN and False elsewhere
    
            Example::
    
                >>> torch.isnan(torch.tensor([1, float('nan'), 2]))
                tensor([False, True, False])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'isnan')(*auto_generated_args, )
        ref_shape = size_mapping['isnan'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "isnan"
        return obj
    
    def isinf(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.isinf'. The automatically generated codes are as follows:
            [ 1]: def isinf(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'isinf')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['isinf'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "isinf"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            isinf() -> Tensor
    
            See :func:`torch.isinf`
    
            which is:
    
            isinf(input) -> Tensor
    
            Tests if each element of :attr:`input` is infinite
            (positive or negative infinity) or not.
    
            .. note::
                Complex values are infinite when their real or imaginary part is
                infinite.
    
            Args:
                input (Tensor): the input tensor.
    
            Returns:
                A boolean tensor that is True where :attr:`input` is infinite and False elsewhere
    
            Example::
    
                >>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))
                tensor([False,  True,  False,  True,  False])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'isinf')(*auto_generated_args, )
        ref_shape = size_mapping['isinf'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "isinf"
        return obj
    
    def isposinf(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.isposinf'. The automatically generated codes are as follows:
            [ 1]: def isposinf(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'isposinf')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['isposinf'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "isposinf"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            isposinf() -> Tensor
    
            See :func:`torch.isposinf`
    
            which is:
    
            isposinf(input, *, out=None) -> Tensor
            Tests if each element of :attr:`input` is positive infinity or not.
    
            Args:
              input (Tensor): the input tensor.
    
            Keyword args:
              out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.tensor([-float('inf'), float('inf'), 1.2])
                >>> torch.isposinf(a)
                tensor([False,  True, False])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'isposinf')(*auto_generated_args, )
        ref_shape = size_mapping['isposinf'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "isposinf"
        return obj
    
    def isneginf(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.isneginf'. The automatically generated codes are as follows:
            [ 1]: def isneginf(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'isneginf')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['isneginf'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "isneginf"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            isneginf() -> Tensor
    
            See :func:`torch.isneginf`
    
            which is:
    
            isneginf(input, *, out=None) -> Tensor
            Tests if each element of :attr:`input` is negative infinity or not.
    
            Args:
              input (Tensor): the input tensor.
    
            Keyword args:
              out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.tensor([-float('inf'), float('inf'), 1.2])
                >>> torch.isneginf(a)
                tensor([ True, False, False])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'isneginf')(*auto_generated_args, )
        ref_shape = size_mapping['isneginf'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "isneginf"
        return obj
    
    def isfinite(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.isfinite'. The automatically generated codes are as follows:
            [ 1]: def isfinite(input: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(input, 'isfinite')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['isfinite'](input_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "isfinite"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
            isfinite() -> Tensor
    
            See :func:`torch.isfinite`
    
            which is:
    
            isfinite(input) -> Tensor
    
            Returns a new tensor with boolean elements representing if each element is `finite` or not.
    
            Real values are finite when they are not NaN, negative infinity, or infinity.
            Complex values are finite when both their real and imaginary parts are finite.
    
            Args:
                input (Tensor): the input tensor.
    
            Returns:
                A boolean tensor that is True where :attr:`input` is finite and False elsewhere
    
            Example::
    
                >>> torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))
                tensor([True,  False,  True,  False,  False])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'isfinite')(*auto_generated_args, )
        ref_shape = size_mapping['isfinite'](input_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "isfinite"
        return obj
    
    def unsqueeze(self: 'Tensor', *dims: new_dim[...], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.unsqueeze'. The automatically generated codes are as follows:
            [ 1]: def unsqueeze(self: 'Tensor', *dims: new_dim[...], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     dims = new_dim[...](self, *dims)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     self_shape=None if self is None else Size(self.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     with torch._C.DisableTorchFunction():
            [24]:         for dims_iter, in zip(dims):
            [25]:             auto_generated_args = tuple(x for x in [] if x is not None)
            [26]:             self = torch_super(self, 'unsqueeze')(*auto_generated_args, dims_iter)
            [27]:     obj = self
            [28]:     ref_shape = size_mapping['unsqueeze'](self_shape, dims)
            [29]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [30]:
            [31]:     def update_special(obj, updates):
            [32]:         if isinstance(obj, tuple):
            [33]:             for x in obj: update_special(x, updates)
            [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [36]:
            [37]:     if getattr(obj, 'grad_fn', None) is not None:
            [38]:         obj.grad_fn_name = "unsqueeze"
            [39]:     return obj
            [40]:
            The document of the original function is:
    
            unsqueeze(dim) -> Tensor
    
            See :func:`torch.unsqueeze`
    
            which is:
    
            unsqueeze(input, dim) -> Tensor
    
            Returns a new tensor with a dimension of size one inserted at the
            specified position.
    
            The returned tensor shares the same underlying data with this tensor.
    
            A :attr:`dim` value within the range ``[-input.dim() - 1, input.dim() + 1)``
            can be used. Negative :attr:`dim` will correspond to :meth:`unsqueeze`
            applied at :attr:`dim` = ``dim + input.dim() + 1``.
    
            Args:
                input (Tensor): the input tensor.
                dim (int): the index at which to insert the singleton dimension
    
            Example::
    
                >>> x = torch.tensor([1, 2, 3, 4])
                >>> torch.unsqueeze(x, 0)
                tensor([[ 1,  2,  3,  4]])
                >>> torch.unsqueeze(x, 1)
                tensor([[ 1],
                        [ 2],
                        [ 3],
                        [ 4]])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        dims = new_dim[...](self, *dims)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            for dims_iter, in zip(dims):
                auto_generated_args = tuple(x for x in [] if x is not None)
                self = torch_super(self, 'unsqueeze')(*auto_generated_args, dims_iter)
        obj = self
        ref_shape = size_mapping['unsqueeze'](self_shape, dims)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "unsqueeze"
        return obj
    
    def squeeze(self: 'Tensor', *dims: del_dim[...], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.squeeze'. The automatically generated codes are as follows:
            [ 1]: def squeeze(self: 'Tensor', *dims: del_dim[...], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     dims = del_dim[...](self, *dims)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     self_shape=None if self is None else Size(self.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     torch_returned = False
            [24]:     valid_dims = []
            [25]:     with torch._C.DisableTorchFunction():
            [26]:         for d in dims[::-1]:
            [27]:             if self.size(d) == 1:
            [28]:                 valid_dims.append(d)
            [29]:                 self = torch_super(self, 'squeeze')(d)
            [30]:     dims = tuple(valid_dims)
            [31]:     obj = self
            [32]:     ref_shape = size_mapping['squeeze'](self_shape, dims)
            [33]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [34]:
            [35]:     def update_special(obj, updates):
            [36]:         if isinstance(obj, tuple):
            [37]:             for x in obj: update_special(x, updates)
            [38]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [39]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [40]:
            [41]:     if getattr(obj, 'grad_fn', None) is not None:
            [42]:         obj.grad_fn_name = "squeeze"
            [43]:     return obj
            [44]:
            The document of the original function is:
    
            squeeze(dim=None) -> Tensor
    
            See :func:`torch.squeeze`
    
            which is:
    
            squeeze(input, dim=None) -> Tensor
    
            Returns a tensor with all specified dimensions of :attr:`input` of size `1` removed.
    
            For example, if `input` is of shape:
            :math:`(A \times 1 \times B \times C \times 1 \times D)` then the `input.squeeze()`
            will be of shape: :math:`(A \times B \times C \times D)`.
    
            When :attr:`dim` is given, a squeeze operation is done only in the given
            dimension(s). If `input` is of shape: :math:`(A \times 1 \times B)`,
            ``squeeze(input, 0)`` leaves the tensor unchanged, but ``squeeze(input, 1)``
            will squeeze the tensor to the shape :math:`(A \times B)`.
    
            .. note:: The returned tensor shares the storage with the input tensor,
                      so changing the contents of one will change the contents of the other.
    
            .. warning:: If the tensor has a batch dimension of size 1, then `squeeze(input)`
                      will also remove the batch dimension, which can lead to unexpected
                      errors. Consider specifying only the dims you wish to be squeezed.
    
            Args:
                input (Tensor): the input tensor.
                dim (int or tuple of ints, optional): if given, the input will be squeezed
                       only in the specified dimensions.
    
                    .. versionchanged:: 2.0
                       :attr:`dim` now accepts tuples of dimensions.
    
            Example::
    
                >>> x = torch.zeros(2, 1, 2, 1, 2)
                >>> x.size()
                torch.Size([2, 1, 2, 1, 2])
                >>> y = torch.squeeze(x)
                >>> y.size()
                torch.Size([2, 2, 2])
                >>> y = torch.squeeze(x, 0)
                >>> y.size()
                torch.Size([2, 1, 2, 1, 2])
                >>> y = torch.squeeze(x, 1)
                >>> y.size()
                torch.Size([2, 2, 1, 2])
                >>> y = torch.squeeze(x, (1, 2, 3))
                torch.Size([2, 2, 2])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        dims = del_dim[...](self, *dims)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        valid_dims = []
        with torch._C.DisableTorchFunction():
            for d in dims[::-1]:
                if self.size(d) == 1:
                    valid_dims.append(d)
                    self = torch_super(self, 'squeeze')(d)
        dims = tuple(valid_dims)
        obj = self
        ref_shape = size_mapping['squeeze'](self_shape, dims)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "squeeze"
        return obj
    
    def flatten(self: 'Tensor', *dims: exist_dim, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.flatten'. The automatically generated codes are as follows:
            [ 1]: def flatten(self: 'Tensor', *dims: exist_dim, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     dims = exist_dim(self, *dims)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     self_shape=None if self is None else Size(self.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     torch_returned = False
            [24]:     if len(dims) > 1:  flat_range = min_(dims), max_(dims) + 1
            [25]:     elif len(dims) == 1:  flat_range = dims[0], self.n_dim
            [26]:     else:  flat_range = 0, self.n_dim
            [27]:     ref_shape = self.shape[:flat_range[0] + 1] + self.shape[flat_range[1]:]
            [28]:     if len(ref_shape) == 0:  ref_shape = bt.Size(FuncDim(1))
            [29]:     with torch._C.DisableTorchFunction():
            [30]:         obj = Tensor.inherit_from(torch_super(self, 'flatten')(flat_range[0], flat_range[1]-1), self, shape=ref_shape)
            [31]:
            [32]:     def update_special(obj, updates):
            [33]:         if isinstance(obj, tuple):
            [34]:             for x in obj: update_special(x, updates)
            [35]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [36]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [37]:
            [38]:     if getattr(obj, 'grad_fn', None) is not None:
            [39]:         obj.grad_fn_name = "flatten"
            [40]:     return obj
            [41]:
            The document of the original function is:
    
            flatten(start_dim=0, end_dim=-1) -> Tensor
    
            See :func:`torch.flatten`
    
            which is:
    
            flatten(input, start_dim=0, end_dim=-1) -> Tensor
    
            Flattens :attr:`input` by reshaping it into a one-dimensional tensor. If :attr:`start_dim` or :attr:`end_dim`
            are passed, only dimensions starting with :attr:`start_dim` and ending with :attr:`end_dim` are flattened.
            The order of elements in :attr:`input` is unchanged.
    
            Unlike NumPy's flatten, which always copies input's data, this function may return the original object, a view,
            or copy. If no dimensions are flattened, then the original object :attr:`input` is returned. Otherwise, if input can
            be viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the
            flattened shape is input's data copied. See :meth:`torch.Tensor.view` for details on when a view will be returned.
    
            .. note::
                Flattening a zero-dimensional tensor will return a one-dimensional view.
    
            Args:
                input (Tensor): the input tensor.
                start_dim (int): the first dim to flatten
                end_dim (int): the last dim to flatten
    
            Example::
    
                >>> t = torch.tensor([[[1, 2],
                ...                    [3, 4]],
                ...                   [[5, 6],
                ...                    [7, 8]]])
                >>> torch.flatten(t)
                tensor([1, 2, 3, 4, 5, 6, 7, 8])
                >>> torch.flatten(t, start_dim=1)
                tensor([[1, 2, 3, 4],
                        [5, 6, 7, 8]])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        dims = exist_dim(self, *dims)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        if len(dims) > 1:  flat_range = min_(dims), max_(dims) + 1
        elif len(dims) == 1:  flat_range = dims[0], self.n_dim
        else:  flat_range = 0, self.n_dim
        ref_shape = self.shape[:flat_range[0] + 1] + self.shape[flat_range[1]:]
        if len(ref_shape) == 0:  ref_shape = bt.Size(FuncDim(1))
        with torch._C.DisableTorchFunction():
            obj = Tensor.inherit_from(torch_super(self, 'flatten')(flat_range[0], flat_range[1]-1), self, shape=ref_shape)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "flatten"
        return obj
    
    def transpose(self: 'Tensor', dim0: exist_dim[1], dim1: exist_dim[1], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.transpose'. The automatically generated codes are as follows:
            [ 1]: def transpose(self: 'Tensor', dim0: exist_dim[1], dim1: exist_dim[1], function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     dim0 = exist_dim[1](self, dim0)
            [19]:     dim1 = exist_dim[1](self, dim1)
            [20]:
            [21]:     # Obtain available sized in arguments (which will be fed into size function).
            [22]:     self_shape=None if self is None else Size(self.shape)
            [23]:     # Use the given inner codes if they are provided.
            [24]:     with torch._C.DisableTorchFunction():
            [25]:         for dim0_iter, dim1_iter in zip(dim0, dim1):
            [26]:             auto_generated_args = tuple(x for x in [dim0_iter,dim1_iter] if x is not None)
            [27]:             self = torch_super(self, 'transpose')(*auto_generated_args, )
            [28]:     obj = self
            [29]:     ref_shape = size_mapping['transpose'](self_shape, dim0, dim1)
            [30]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [31]:
            [32]:     def update_special(obj, updates):
            [33]:         if isinstance(obj, tuple):
            [34]:             for x in obj: update_special(x, updates)
            [35]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [36]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [37]:
            [38]:     if getattr(obj, 'grad_fn', None) is not None:
            [39]:         obj.grad_fn_name = "transpose"
            [40]:     return obj
            [41]:
            The document of the original function is:
    
            transpose(dim0, dim1) -> Tensor
    
            See :func:`torch.transpose`
    
            which is:
    
            transpose(input, dim0, dim1) -> Tensor
    
            Returns a tensor that is a transposed version of :attr:`input`.
            The given dimensions :attr:`dim0` and :attr:`dim1` are swapped.
    
            If :attr:`input` is a strided tensor then the resulting :attr:`out`
            tensor shares its underlying storage with the :attr:`input` tensor, so
            changing the content of one would change the content of the other.
    
            If :attr:`input` is a :ref:`sparse tensor <sparse-docs>` then the
            resulting :attr:`out` tensor *does not* share the underlying storage
            with the :attr:`input` tensor.
    
            If :attr:`input` is a :ref:`sparse tensor <sparse-docs>` with compressed
            layout (SparseCSR, SparseBSR, SparseCSC or SparseBSC) the arguments
            :attr:`dim0` and :attr:`dim1` must be both batch dimensions, or must
            both be sparse dimensions. The batch dimensions of a sparse tensor are the
            dimensions preceding the sparse dimensions.
    
            .. note::
                Transpositions which interchange the sparse dimensions of a `SparseCSR`
                or `SparseCSC` layout tensor will result in the layout changing between
                the two options. Transposition of the sparse dimensions of a ` SparseBSR`
                or `SparseBSC` layout tensor will likewise generate a result with the
                opposite layout.
    
            Args:
                input (Tensor): the input tensor.
                dim0 (int): the first dimension to be transposed
                dim1 (int): the second dimension to be transposed
    
            Example::
    
                >>> x = torch.randn(2, 3)
                >>> x
                tensor([[ 1.0028, -0.9893,  0.5809],
                        [-0.1669,  0.7299,  0.4942]])
                >>> torch.transpose(x, 0, 1)
                tensor([[ 1.0028, -0.1669],
                        [-0.9893,  0.7299],
                        [ 0.5809,  0.4942]])
    
            See also :func:`torch.t`.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        dim0 = exist_dim[1](self, dim0)
        dim1 = exist_dim[1](self, dim1)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            for dim0_iter, dim1_iter in zip(dim0, dim1):
                auto_generated_args = tuple(x for x in [dim0_iter,dim1_iter] if x is not None)
                self = torch_super(self, 'transpose')(*auto_generated_args, )
        obj = self
        ref_shape = size_mapping['transpose'](self_shape, dim0, dim1)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "transpose"
        return obj
    
    def t(self, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.t'. The automatically generated codes are as follows:
            [ 1]: def t(self, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in []:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     # Obtain available sized in arguments (which will be fed into size function).
            [13]:
            [14]:     # Use the given inner codes if they are provided.
            [15]:     with torch._C.DisableTorchFunction():
            [16]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [17]:         obj = torch_super(self, 't')(*auto_generated_args, )
            [18]:     ref_shape = size_mapping['t']()
            [19]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [20]:
            [21]:     def update_special(obj, updates):
            [22]:         if isinstance(obj, tuple):
            [23]:             for x in obj: update_special(x, updates)
            [24]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [25]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [26]:
            [27]:     if getattr(obj, 'grad_fn', None) is not None:
            [28]:         obj.grad_fn_name = "t"
            [29]:     return obj
            [30]:
            The document of the original function is:
    
            t() -> Tensor
    
            See :func:`torch.t`
    
            which is:
    
            t(input) -> Tensor
    
            Expects :attr:`input` to be <= 2-D tensor and transposes dimensions 0
            and 1.
    
            0-D and 1-D tensors are returned as is. When input is a 2-D tensor this
            is equivalent to ``transpose(input, 0, 1)``.
    
            Args:
                input (Tensor): the input tensor.
    
            Example::
    
                >>> x = torch.randn(())
                >>> x
                tensor(0.1995)
                >>> torch.t(x)
                tensor(0.1995)
                >>> x = torch.randn(3)
                >>> x
                tensor([ 2.4320, -0.4608,  0.7702])
                >>> torch.t(x)
                tensor([ 2.4320, -0.4608,  0.7702])
                >>> x = torch.randn(2, 3)
                >>> x
                tensor([[ 0.4875,  0.9158, -0.5872],
                        [ 0.3938, -0.6929,  0.6932]])
                >>> torch.t(x)
                tensor([[ 0.4875,  0.3938],
                        [ 0.9158, -0.6929],
                        [-0.5872,  0.6932]])
    
            See also :func:`torch.transpose`.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in []:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        # Obtain available sized in arguments (which will be fed into size function).
    
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(self, 't')(*auto_generated_args, )
        ref_shape = size_mapping['t']()
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "t"
        return obj
    
    def permute(self: 'Tensor', *dims, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.permute'. The automatically generated codes are as follows:
            [ 1]: def permute(self: 'Tensor', *dims, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     self_shape=None if self is None else Size(self.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     torch_returned = False
            [22]:     ref_shape = Size(*dims)
            [23]:     dims = exist_dim(self, *dims)
            [24]:     with torch._C.DisableTorchFunction():
            [25]:         obj = torch.permute(self, dims)
            [26]:     obj = Tensor.inherit_from(obj, self, shape=size_mapping['permute'](self_shape, dims))
            [27]:     if ref_shape.has_special:  obj.special_from(ref_shape)
            [28]:     obj = obj
            [29]:
            [30]:     def update_special(obj, updates):
            [31]:         if isinstance(obj, tuple):
            [32]:             for x in obj: update_special(x, updates)
            [33]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [34]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [35]:
            [36]:     if getattr(obj, 'grad_fn', None) is not None:
            [37]:         obj.grad_fn_name = "permute"
            [38]:     return obj
            [39]:
            The document of the original function is:
    
            permute(*dims) -> Tensor
    
            See :func:`torch.permute`
    
            which is:
    
            permute(input, dims) -> Tensor
    
            Returns a view of the original tensor :attr:`input` with its dimensions permuted.
    
            Args:
                input (Tensor): the input tensor.
                dims (tuple of int): The desired ordering of dimensions
    
            Example:
                >>> x = torch.randn(2, 3, 5)
                >>> x.size()
                torch.Size([2, 3, 5])
                >>> torch.permute(x, (2, 0, 1)).size()
                torch.Size([5, 2, 3])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        ref_shape = Size(*dims)
        dims = exist_dim(self, *dims)
        with torch._C.DisableTorchFunction():
            obj = torch.permute(self, dims)
        obj = Tensor.inherit_from(obj, self, shape=size_mapping['permute'](self_shape, dims))
        if ref_shape.has_special:  obj.special_from(ref_shape)
        obj = obj
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "permute"
        return obj
    
    def standard_shape(self, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.standard_shape'. The automatically generated codes are as follows:
            [ 1]: def standard_shape(self, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in []:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     # Obtain available sized in arguments (which will be fed into size function).
            [13]:
            [14]:     # Use the given inner codes if they are provided.
            [15]:     torch_returned = False
            [16]:     left_dims = []
            [17]:     right_dims = []
            [18]:     for dim in MajorSpecialDimensions + [SpaceDim]:
            [19]:         start, stop = getattr(self, f"{dim.name}_range")
            [20]:         if start is not None:
            [21]:             if dim.last:  right_dims = list(range(start, stop)) + right_dims
            [22]:             else:  left_dims.extend(list(range(start, stop)))
            [23]:     permutation = left_dims + right_dims
            [24]:     obj = permute(self, *permutation) # suppress:  special_from
            [25]:
            [26]:     def update_special(obj, updates):
            [27]:         if isinstance(obj, tuple):
            [28]:             for x in obj: update_special(x, updates)
            [29]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [30]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [31]:
            [32]:     if getattr(obj, 'grad_fn', None) is not None:
            [33]:         obj.grad_fn_name = "standard_shape"
            [34]:     return obj
            [35]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in []:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        # Obtain available sized in arguments (which will be fed into size function).
    
        # Use the given inner codes if they are provided.
        torch_returned = False
        left_dims = []
        right_dims = []
        for dim in MajorSpecialDimensions + [SpaceDim]:
            start, stop = getattr(self, f"{dim.name}_range")
            if start is not None:
                if dim.last:  right_dims = list(range(start, stop)) + right_dims
                else:  left_dims.extend(list(range(start, stop)))
        permutation = left_dims + right_dims
        obj = permute(self, *permutation) # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "standard_shape"
        return obj
    
    def duplicate(self: 'Tensor', num=2, dim: new_dim[1]={}, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
        data.duplicate(num, 0): data(n_1, n_2) => (num, n_1, n_2)
        Duplicate a tensor by `num` times and stack them as a new tensor.
    
        Args:
            num (int, optional): The number of duplications. Defaults to 2.
            dim (int/new_dim, optional): The dimension to stack along. Defaults to batch.
    
        Automatically inheritted method from 'torch.Tensor.duplicate'. The automatically generated codes are as follows:
            [ 1]: def duplicate(self: 'Tensor', num=2, dim: new_dim[1]={}, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:     data.duplicate(num, 0): data(n_1, n_2) => (num, n_1, n_2)
            [ 4]:     Duplicate a tensor by `num` times and stack them as a new tensor.
            [ 5]:
            [ 6]:     Args:
            [ 7]:         num (int, optional): The number of duplications. Defaults to 2.
            [ 8]:         dim (int/new_dim, optional): The dimension to stack along. Defaults to batch.
            [ 9]:     '''
            [10]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [11]:
            [12]:     pivot = None
            [13]:     for t in [self]:
            [14]:         if isinstance(t, torch.Tensor): pivot = t; break
            [15]:     subclass = Tensor.get_tensor_subclass(pivot)
            [16]:
            [17]:     if self is None: ...
            [18]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [19]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [20]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [21]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [22]:
            [23]:     dim = new_dim[1](self, dim)
            [24]:
            [25]:     # Obtain available sized in arguments (which will be fed into size function).
            [26]:     self_shape=None if self is None else Size(self.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     torch_returned = False
            [29]:     obj = self.unsqueeze(dim).repeat((1,) * dim[0] + (num,) + (1,) * (self.ndim - dim[0])).special_from(dim)
            [30]:
            [31]:     def update_special(obj, updates):
            [32]:         if isinstance(obj, tuple):
            [33]:             for x in obj: update_special(x, updates)
            [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [36]:
            [37]:     if getattr(obj, 'grad_fn', None) is not None:
            [38]:         obj.grad_fn_name = "duplicate"
            [39]:     return obj
            [40]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        dim = new_dim[1](self, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        obj = self.unsqueeze(dim).repeat((1,) * dim[0] + (num,) + (1,) * (self.ndim - dim[0])).special_from(dim)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "duplicate"
        return obj
    
    def amplify(self: 'Tensor', num=2, dim: exist_dim[1]={}, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
        data.amplify(num, 0): data(n_1, n_2) => (n_1 * num, n_2)
        Amplify a dimension of a tensor by enlarging with duplications: amplifying [a, b, c] with number 2 results in [a, a, b, b, c, c].
        Note that one should use 'repeated' (for one dimension) or 'repeat' (for all dimensions) to duplicate the whole tensor and
            concatenate the duplications together ([a, b, c] => [a, b, c, a, b, c]).
    
        Args:
            num (int, optional): The number of duplications. Defaults to 2.
            dim (int/new_dim, optional): The dimension to stack along. Defaults to batch.
    
        Automatically inheritted method from 'torch.Tensor.amplify'. The automatically generated codes are as follows:
            [ 1]: def amplify(self: 'Tensor', num=2, dim: exist_dim[1]={}, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:     data.amplify(num, 0): data(n_1, n_2) => (n_1 * num, n_2)
            [ 4]:     Amplify a dimension of a tensor by enlarging with duplications: amplifying [a, b, c] with number 2 results in [a, a, b, b, c, c].
            [ 5]:     Note that one should use 'repeated' (for one dimension) or 'repeat' (for all dimensions) to duplicate the whole tensor and
            [ 6]:         concatenate the duplications together ([a, b, c] => [a, b, c, a, b, c]).
            [ 7]:
            [ 8]:     Args:
            [ 9]:         num (int, optional): The number of duplications. Defaults to 2.
            [10]:         dim (int/new_dim, optional): The dimension to stack along. Defaults to batch.
            [11]:     '''
            [12]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [13]:
            [14]:     pivot = None
            [15]:     for t in [self]:
            [16]:         if isinstance(t, torch.Tensor): pivot = t; break
            [17]:     subclass = Tensor.get_tensor_subclass(pivot)
            [18]:
            [19]:     if self is None: ...
            [20]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [21]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [22]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [23]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [24]:
            [25]:     dim = exist_dim[1](self, dim)
            [26]:
            [27]:     # Obtain available sized in arguments (which will be fed into size function).
            [28]:     self_shape=None if self is None else Size(self.shape)
            [29]:     # Use the given inner codes if they are provided.
            [30]:     torch_returned = False
            [31]:     dim = dim[0]
            [32]:     with self.hide_special():
            [33]:         output = self.duplicate(num, dim+1).flatten(dim, dim + 1)
            [34]:     obj = output.special_from(self)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "amplify"
            [44]:     return obj
            [45]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        dim = exist_dim[1](self, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        dim = dim[0]
        with self.hide_special():
            output = self.duplicate(num, dim+1).flatten(dim, dim + 1)
        obj = output.special_from(self)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "amplify"
        return obj
    
    def repeated(self: 'Tensor', num=2, dim: exist_dim[1]={}, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
        data.repeated(num, 0): data(n_1, n_2) => (num * n_1, n_2)
        Repeat a tensor by `num` times along one dimension `dim` (use 'repeat' for multiple dimensions) and concatenate them as a new tensor.
        Note that repeating [a, b, c] with number 2 results in [a, b, c, a, b, c].
            One should use 'amplify' to amplify to [a, a, b, b, c, c].
    
        Args:
            num (int, optional): The number of duplications. Defaults to 2.
            dim (int/new_dim, optional): The dimension to stack along. Defaults to batch.
    
        Automatically inheritted method from 'torch.Tensor.repeated'. The automatically generated codes are as follows:
            [ 1]: def repeated(self: 'Tensor', num=2, dim: exist_dim[1]={}, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:     data.repeated(num, 0): data(n_1, n_2) => (num * n_1, n_2)
            [ 4]:     Repeat a tensor by `num` times along one dimension `dim` (use 'repeat' for multiple dimensions) and concatenate them as a new tensor.
            [ 5]:     Note that repeating [a, b, c] with number 2 results in [a, b, c, a, b, c].
            [ 6]:         One should use 'amplify' to amplify to [a, a, b, b, c, c].
            [ 7]:
            [ 8]:     Args:
            [ 9]:         num (int, optional): The number of duplications. Defaults to 2.
            [10]:         dim (int/new_dim, optional): The dimension to stack along. Defaults to batch.
            [11]:     '''
            [12]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [13]:
            [14]:     pivot = None
            [15]:     for t in [self]:
            [16]:         if isinstance(t, torch.Tensor): pivot = t; break
            [17]:     subclass = Tensor.get_tensor_subclass(pivot)
            [18]:
            [19]:     if self is None: ...
            [20]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [21]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [22]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [23]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [24]:
            [25]:     dim = exist_dim[1](self, dim)
            [26]:
            [27]:     # Obtain available sized in arguments (which will be fed into size function).
            [28]:     self_shape=None if self is None else Size(self.shape)
            [29]:     # Use the given inner codes if they are provided.
            [30]:     torch_returned = False
            [31]:     dim = dim[0]
            [32]:     with self.hide_special():
            [33]:         output = self.duplicate(num, dim).flatten(dim, dim + 1)
            [34]:     obj = output.special_from(self)
            [35]:
            [36]:     def update_special(obj, updates):
            [37]:         if isinstance(obj, tuple):
            [38]:             for x in obj: update_special(x, updates)
            [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [41]:
            [42]:     if getattr(obj, 'grad_fn', None) is not None:
            [43]:         obj.grad_fn_name = "repeated"
            [44]:     return obj
            [45]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        dim = exist_dim[1](self, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        dim = dim[0]
        with self.hide_special():
            output = self.duplicate(num, dim).flatten(dim, dim + 1)
        obj = output.special_from(self)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "repeated"
        return obj
    
    def repeat(self: 'Tensor', *size: 'Size', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.repeat'. The automatically generated codes are as follows:
            [ 1]: def repeat(self: 'Tensor', *size: 'Size', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     self_shape=None if self is None else Size(self.shape)
            [20]:     size=Size(*size)
            [21]:     # Use the given inner codes if they are provided.
            [22]:     torch_returned = False
            [23]:     with torch._C.DisableTorchFunction():
            [24]:         obj = Tensor.inherit_from(torch_super(self, 'repeat')(*size), self).update_special_from(size)
            [25]:
            [26]:     def update_special(obj, updates):
            [27]:         if isinstance(obj, tuple):
            [28]:             for x in obj: update_special(x, updates)
            [29]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [30]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [31]:
            [32]:     if getattr(obj, 'grad_fn', None) is not None:
            [33]:         obj.grad_fn_name = "repeat"
            [34]:     return obj
            [35]:
            The document of the original function is:
    
            repeat(*sizes) -> Tensor
    
            Repeats this tensor along the specified dimensions.
    
            Unlike :meth:`~Tensor.expand`, this function copies the tensor's data.
    
            .. warning::
    
                :meth:`~Tensor.repeat` behaves differently from
                `numpy.repeat <https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html>`_,
                but is more similar to
                `numpy.tile <https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html>`_.
                For the operator similar to `numpy.repeat`, see :func:`torch.repeat_interleave`.
    
            Args:
                sizes (torch.Size or int...): The number of times to repeat this tensor along each
                    dimension
    
            Example::
    
                >>> x = torch.tensor([1, 2, 3])
                >>> x.repeat(4, 2)
                tensor([[ 1,  2,  3,  1,  2,  3],
                        [ 1,  2,  3,  1,  2,  3],
                        [ 1,  2,  3,  1,  2,  3],
                        [ 1,  2,  3,  1,  2,  3]])
                >>> x.repeat(4, 2, 1).size()
                torch.Size([4, 2, 3])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        size=Size(*size)
        # Use the given inner codes if they are provided.
        torch_returned = False
        with torch._C.DisableTorchFunction():
            obj = Tensor.inherit_from(torch_super(self, 'repeat')(*size), self).update_special_from(size)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "repeat"
        return obj
    
    def gather(self: 'Tensor', dim: exist_dim[1], index, *, sparse_grad=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.gather'. The automatically generated codes are as follows:
            [ 1]: def gather(self: 'Tensor', dim: exist_dim[1], index, *, sparse_grad=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     dim = exist_dim[1](self, dim)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     self_shape=None if self is None else Size(self.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     with torch._C.DisableTorchFunction():
            [24]:         for dim_iter, in zip(dim):
            [25]:             auto_generated_args = tuple(x for x in [dim_iter,index] if x is not None)
            [26]:             self = torch_super(self, 'gather')(*auto_generated_args, sparse_grad=sparse_grad)
            [27]:     obj = self
            [28]:     ref_shape = size_mapping['gather'](self_shape, dim)
            [29]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [30]:
            [31]:     def update_special(obj, updates):
            [32]:         if isinstance(obj, tuple):
            [33]:             for x in obj: update_special(x, updates)
            [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [36]:
            [37]:     if getattr(obj, 'grad_fn', None) is not None:
            [38]:         obj.grad_fn_name = "gather"
            [39]:     return obj
            [40]:
            The document of the original function is:
    
            gather(dim, index) -> Tensor
    
            See :func:`torch.gather`
    
            which is:
    
            gather(input, dim, index, *, sparse_grad=False, out=None) -> Tensor
    
            Gathers values along an axis specified by `dim`.
    
            For a 3-D tensor the output is specified by::
    
                out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
                out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
                out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2
    
            :attr:`input` and :attr:`index` must have the same number of dimensions.
            It is also required that ``index.size(d) <= input.size(d)`` for all
            dimensions ``d != dim``.  :attr:`out` will have the same shape as :attr:`index`.
            Note that ``input`` and ``index`` do not broadcast against each other.
    
            Args:
                input (Tensor): the source tensor
                dim (int): the axis along which to index
                index (LongTensor): the indices of elements to gather
    
            Keyword arguments:
                sparse_grad (bool, optional): If ``True``, gradient w.r.t. :attr:`input` will be a sparse tensor.
                out (Tensor, optional): the destination tensor
    
            Example::
    
                >>> t = torch.tensor([[1, 2], [3, 4]])
                >>> torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))
                tensor([[ 1,  1],
                        [ 4,  3]])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        dim = exist_dim[1](self, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            for dim_iter, in zip(dim):
                auto_generated_args = tuple(x for x in [dim_iter,index] if x is not None)
                self = torch_super(self, 'gather')(*auto_generated_args, sparse_grad=sparse_grad)
        obj = self
        ref_shape = size_mapping['gather'](self_shape, dim)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "gather"
        return obj
    
    def flip(self: 'Tensor', *dims: exist_dim, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.flip'. The automatically generated codes are as follows:
            [ 1]: def flip(self: 'Tensor', *dims: exist_dim, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     dims = exist_dim(self, *dims)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     self_shape=None if self is None else Size(self.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     with torch._C.DisableTorchFunction():
            [24]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [25]:         obj = torch_super(self, 'flip')(*auto_generated_args, *dims)
            [26]:     ref_shape = size_mapping['flip'](self_shape, dims)
            [27]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [28]:
            [29]:     def update_special(obj, updates):
            [30]:         if isinstance(obj, tuple):
            [31]:             for x in obj: update_special(x, updates)
            [32]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [33]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [34]:
            [35]:     if getattr(obj, 'grad_fn', None) is not None:
            [36]:         obj.grad_fn_name = "flip"
            [37]:     return obj
            [38]:
            The document of the original function is:
    
            flip(dims) -> Tensor
    
            See :func:`torch.flip`
    
            which is:
    
            flip(input, dims) -> Tensor
    
            Reverse the order of an n-D tensor along given axis in dims.
    
            .. note::
                `torch.flip` makes a copy of :attr:`input`'s data. This is different from NumPy's `np.flip`,
                which returns a view in constant time. Since copying a tensor's data is more work than viewing that data,
                `torch.flip` is expected to be slower than `np.flip`.
    
            Args:
                input (Tensor): the input tensor.
                dims (a list or tuple): axis to flip on
    
            Example::
    
                >>> x = torch.arange(8).view(2, 2, 2)
                >>> x
                tensor([[[ 0,  1],
                         [ 2,  3]],
    
                        [[ 4,  5],
                         [ 6,  7]]])
                >>> torch.flip(x, [0, 1])
                tensor([[[ 6,  7],
                         [ 4,  5]],
    
                        [[ 2,  3],
                         [ 0,  1]]])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        dims = exist_dim(self, *dims)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(self, 'flip')(*auto_generated_args, *dims)
        ref_shape = size_mapping['flip'](self_shape, dims)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "flip"
        return obj
    
    def detach(self: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.detach'. The automatically generated codes are as follows:
            [ 1]: def detach(self: 'Tensor', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     self_shape=None if self is None else Size(self.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     with torch._C.DisableTorchFunction():
            [22]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [23]:         obj = torch_super(self, 'detach')(*auto_generated_args, )
            [24]:     ref_shape = size_mapping['detach'](self_shape)
            [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [26]:
            [27]:     def update_special(obj, updates):
            [28]:         if isinstance(obj, tuple):
            [29]:             for x in obj: update_special(x, updates)
            [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [32]:
            [33]:     if getattr(obj, 'grad_fn', None) is not None:
            [34]:         obj.grad_fn_name = "detach"
            [35]:     return obj
            [36]:
            The document of the original function is:
    
                Returns a new Tensor, detached from the current graph.
    
                The result will never require gradient.
    
                This method also affects forward mode AD gradients and the result will never
                have forward mode AD gradients.
    
                .. note::
    
                  Returned Tensor shares the same storage with the original one.
                  In-place modifications on either of them will be seen, and may trigger
                  errors in correctness checks.
                  IMPORTANT NOTE: Previously, in-place size / stride / storage changes
                  (such as `resize_` / `resize_as_` / `set_` / `transpose_`) to the returned tensor
                  also update the original tensor. Now, these in-place changes will not update the
                  original tensor anymore, and will instead trigger an error.
                  For sparse tensors:
                  In-place indices / values changes (such as `zero_` / `copy_` / `add_`) to the
                  returned tensor will not update the original tensor anymore, and will instead
                  trigger an error.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(self, 'detach')(*auto_generated_args, )
        ref_shape = size_mapping['detach'](self_shape)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "detach"
        return obj
    
    def quantile(self: 'Tensor', q: 'Tensor', dim=None, keepdim=False, *, interpolation='linear', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.quantile'. The automatically generated codes are as follows:
            [ 1]: def quantile(self: 'Tensor', q: 'Tensor', dim=None, keepdim=False, *, interpolation='linear', function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self, q]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     if q is None: ...
            [19]:     elif not isinstance(q, torch.Tensor): q = torch.tensor(q)
            [20]:     if not isinstance(q, subclass): q = q.as_subclass(subclass).special_from(q.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if q.device.type == 'cpu': q = q.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     self_shape=None if self is None else Size(self.shape)
            [26]:     q_shape=None if q is None else Size(q.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     torch_returned = False
            [29]:     n_dim_count = None
            [30]:     if dim is not None:
            [31]:         dim = exist_dim(self, dim)
            [32]:         if len(dim) > 1:  self = self.flatten(dim); n_dim_count = len(dim)
            [33]:         ref_shape, _, _ = size_mapping_op['quantile'](self.shape, q_shape, dim[:1], keepdim=keepdim)
            [34]:     with torch._C.DisableTorchFunction():
            [35]:         if dim is None:  res = Tensor.inherit_from(torch.quantile(self, q, keepdim=keepdim,interpolation=interpolation), self, shape=q_shape); dim = [int_(q.n_dim > 0)]
            [36]:         else:  res = Tensor.inherit_from(torch.quantile(self, q, dim[0], keepdim=keepdim,interpolation=interpolation), self, shape=ref_shape)
            [37]:     if keepdim:
            [38]:         d = dim[0] + int_(q.n_dim > 0)
            [39]:         obj = res.split_dim(d, res.shape[d:d+1] * n_dim_count)
            [40]:     else: torch_returned = True; obj = res
            [41]:
            [42]:     def update_special(obj, updates):
            [43]:         if isinstance(obj, tuple):
            [44]:             for x in obj: update_special(x, updates)
            [45]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [46]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [47]:
            [48]:     if getattr(obj, 'grad_fn', None) is not None:
            [49]:         obj.grad_fn_name = "quantile"
            [50]:     return obj
            [51]:
            The document of the original function is:
    
            quantile(q, dim=None, keepdim=False, *, interpolation='linear') -> Tensor
    
            See :func:`torch.quantile`
    
            which is:
    
            quantile(input, q, dim=None, keepdim=False, *, interpolation='linear', out=None) -> Tensor
    
            Computes the q-th quantiles of each row of the :attr:`input` tensor along the dimension :attr:`dim`.
    
            To compute the quantile, we map q in [0, 1] to the range of indices [0, n] to find the location
            of the quantile in the sorted input. If the quantile lies between two data points ``a < b`` with
            indices ``i`` and ``j`` in the sorted order, result is computed according to the given
            :attr:`interpolation` method as follows:
    
            - ``linear``: ``a + (b - a) * fraction``, where ``fraction`` is the fractional part of the computed quantile index.
            - ``lower``: ``a``.
            - ``higher``: ``b``.
            - ``nearest``: ``a`` or ``b``, whichever's index is closer to the computed quantile index (rounding down for .5 fractions).
            - ``midpoint``: ``(a + b) / 2``.
    
            If :attr:`q` is a 1D tensor, the first dimension of the output represents the quantiles and has size
            equal to the size of :attr:`q`, the remaining dimensions are what remains from the reduction.
    
            .. note::
                By default :attr:`dim` is ``None`` resulting in the :attr:`input` tensor being flattened before computation.
    
            Args:
                input (Tensor): the input tensor.
                q (float or Tensor): a scalar or 1D tensor of values in the range [0, 1].
                dim (int): the dimension to reduce.
                keepdim (bool): whether the output tensor has :attr:`dim` retained or not.
    
            Keyword arguments:
                interpolation (str): interpolation method to use when the desired quantile lies between two data points.
                                        Can be ``linear``, ``lower``, ``higher``, ``midpoint`` and ``nearest``.
                                        Default is ``linear``.
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.randn(2, 3)
                >>> a
                tensor([[ 0.0795, -1.2117,  0.9765],
                        [ 1.1707,  0.6706,  0.4884]])
                >>> q = torch.tensor([0.25, 0.5, 0.75])
                >>> torch.quantile(a, q, dim=1, keepdim=True)
                tensor([[[-0.5661],
                        [ 0.5795]],
    
                        [[ 0.0795],
                        [ 0.6706]],
    
                        [[ 0.5280],
                        [ 0.9206]]])
                >>> torch.quantile(a, q, dim=1, keepdim=True).shape
                torch.Size([3, 2, 1])
                >>> a = torch.arange(4.)
                >>> a
                tensor([0., 1., 2., 3.])
                >>> torch.quantile(a, 0.6, interpolation='linear')
                tensor(1.8000)
                >>> torch.quantile(a, 0.6, interpolation='lower')
                tensor(1.)
                >>> torch.quantile(a, 0.6, interpolation='higher')
                tensor(2.)
                >>> torch.quantile(a, 0.6, interpolation='midpoint')
                tensor(1.5000)
                >>> torch.quantile(a, 0.6, interpolation='nearest')
                tensor(2.)
                >>> torch.quantile(a, 0.4, interpolation='nearest')
                tensor(1.)
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self, q]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        if q is None: ...
        elif not isinstance(q, torch.Tensor): q = torch.tensor(q)
        if not isinstance(q, subclass): q = q.as_subclass(subclass).special_from(q.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if q.device.type == 'cpu': q = q.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        q_shape=None if q is None else Size(q.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        n_dim_count = None
        if dim is not None:
            dim = exist_dim(self, dim)
            if len(dim) > 1:  self = self.flatten(dim); n_dim_count = len(dim)
            ref_shape, _, _ = size_mapping_op['quantile'](self.shape, q_shape, dim[:1], keepdim=keepdim)
        with torch._C.DisableTorchFunction():
            if dim is None:  res = Tensor.inherit_from(torch.quantile(self, q, keepdim=keepdim,interpolation=interpolation), self, shape=q_shape); dim = [int_(q.n_dim > 0)]
            else:  res = Tensor.inherit_from(torch.quantile(self, q, dim[0], keepdim=keepdim,interpolation=interpolation), self, shape=ref_shape)
        if keepdim:
            d = dim[0] + int_(q.n_dim > 0)
            obj = res.split_dim(d, res.shape[d:d+1] * n_dim_count)
        else: torch_returned = True; obj = res
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "quantile"
        return obj
    
    def val_range(self, dim: exist_dim=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
        Compute the range in dimensions `dim`, resulting in a squeeze of these dimensions
            to a sole functional dimension of 2, i.e. the minimal and maximal values.
    
        Args: dim (int/exist_dim, optional): The dimensions to find range. Defaults to None.
        Output: ((2), ...[size without the specified dimnsions])
    
        Automatically inheritted method from 'torch.Tensor.val_range'. The automatically generated codes are as follows:
            [ 1]: def val_range(self, dim: exist_dim=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:     Compute the range in dimensions `dim`, resulting in a squeeze of these dimensions
            [ 4]:         to a sole functional dimension of 2, i.e. the minimal and maximal values.
            [ 5]:
            [ 6]:     Args: dim (int/exist_dim, optional): The dimensions to find range. Defaults to None.
            [ 7]:     Output: ((2), ...[size without the specified dimnsions])
            [ 8]:     '''
            [ 9]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [10]:
            [11]:     pivot = None
            [12]:     for t in []:
            [13]:         if isinstance(t, torch.Tensor): pivot = t; break
            [14]:     subclass = Tensor.get_tensor_subclass(pivot)
            [15]:
            [16]:     dim = exist_dim(self, dim)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:
            [20]:     # Use the given inner codes if they are provided.
            [21]:     torch_returned = False
            [22]:     obj = stack(self.min(dim), self.max(dim), FuncDim) # suppress:  special_from
            [23]:
            [24]:     def update_special(obj, updates):
            [25]:         if isinstance(obj, tuple):
            [26]:             for x in obj: update_special(x, updates)
            [27]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [28]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [29]:
            [30]:     if getattr(obj, 'grad_fn', None) is not None:
            [31]:         obj.grad_fn_name = "val_range"
            [32]:     return obj
            [33]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in []:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        dim = exist_dim(self, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
    
        # Use the given inner codes if they are provided.
        torch_returned = False
        obj = stack(self.min(dim), self.max(dim), FuncDim) # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "val_range"
        return obj
    
    def sum(input: 'Tensor', *dim: del_dim[:], keepdim=False, dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.sum'. The automatically generated codes are as follows:
            [ 1]: def sum(input: 'Tensor', *dim: del_dim[:], keepdim=False, dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     dim = del_dim[:](input, *dim)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     input_shape=None if input is None else Size(input.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     with torch._C.DisableTorchFunction():
            [24]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [25]:         obj = torch_super(input, 'sum')(*auto_generated_args, dim,keepdim=keepdim,dtype=dtype)
            [26]:     ref_shape = size_mapping['sum'](input_shape, dim, **dict(keepdim=keepdim, dtype=dtype))
            [27]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [28]:
            [29]:     def update_special(obj, updates):
            [30]:         if isinstance(obj, tuple):
            [31]:             for x in obj: update_special(x, updates)
            [32]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [33]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [34]:
            [35]:     if getattr(obj, 'grad_fn', None) is not None:
            [36]:         obj.grad_fn_name = "sum"
            [37]:     return obj
            [38]:
            The document of the original function is:
    
            sum(dim=None, keepdim=False, dtype=None) -> Tensor
    
            See :func:`torch.sum`
    
            which is:
    
            sum(input, *, dtype=None) -> Tensor
    
            Returns the sum of all elements in the :attr:`input` tensor.
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                    If specified, the input tensor is casted to :attr:`dtype` before the operation
                    is performed. This is useful for preventing data type overflows. Default: None.
    
            Example::
    
                >>> a = torch.randn(1, 3)
                >>> a
                tensor([[ 0.1133, -0.9567,  0.2958]])
                >>> torch.sum(a)
                tensor(-0.5475)
    
            .. function:: sum(input, dim, keepdim=False, *, dtype=None) -> Tensor
               :noindex:
    
            Returns the sum of each row of the :attr:`input` tensor in the given
            dimension :attr:`dim`. If :attr:`dim` is a list of dimensions,
            reduce over all of them.
    
            If :attr:`keepdim` is ``True``, the output tensor is of the same size
            as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
            Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
            output tensor having 1 (or ``len(dim)``) fewer dimension(s).
    
            Args:
                input (Tensor): the input tensor.
    
                dim (int or tuple of ints, optional): the dimension or dimensions to reduce.
                    If ``None``, all dimensions are reduced.
    
                keepdim (bool): whether the output tensor has :attr:`dim` retained or not.
    
            Keyword args:
                dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                    If specified, the input tensor is casted to :attr:`dtype` before the operation
                    is performed. This is useful for preventing data type overflows. Default: None.
    
            Example::
    
                >>> a = torch.randn(4, 4)
                >>> a
                tensor([[ 0.0569, -0.2475,  0.0737, -0.3429],
                        [-0.2993,  0.9138,  0.9337, -1.6864],
                        [ 0.1132,  0.7892, -0.1003,  0.5688],
                        [ 0.3637, -0.9906, -0.4752, -1.5197]])
                >>> torch.sum(a, 1)
                tensor([-0.4598, -0.1381,  1.3708, -2.6217])
                >>> b = torch.arange(4 * 5 * 6).view(4, 5, 6)
                >>> torch.sum(b, (2, 1))
                tensor([  435.,  1335.,  2235.,  3135.])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = del_dim[:](input, *dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'sum')(*auto_generated_args, dim,keepdim=keepdim,dtype=dtype)
        ref_shape = size_mapping['sum'](input_shape, dim, **dict(keepdim=keepdim, dtype=dtype))
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "sum"
        return obj
    
    def prod(input: 'Tensor', dim: del_dim[...]=None, keepdim=False, dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.prod'. The automatically generated codes are as follows:
            [ 1]: def prod(input: 'Tensor', dim: del_dim[...]=None, keepdim=False, dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     dim = del_dim[...](input, dim)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     input_shape=None if input is None else Size(input.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     with torch._C.DisableTorchFunction():
            [24]:         for dim_iter, in zip(dim[::-1]):
            [25]:             auto_generated_args = tuple(x for x in [] if x is not None)
            [26]:             input = torch_super(input, 'prod')(*auto_generated_args, dim=dim_iter,keepdim=keepdim,dtype=dtype)
            [27]:     obj = input
            [28]:     ref_shape = size_mapping['prod'](input_shape, dim, **dict(dim=dim, keepdim=keepdim, dtype=dtype))
            [29]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [30]:
            [31]:     def update_special(obj, updates):
            [32]:         if isinstance(obj, tuple):
            [33]:             for x in obj: update_special(x, updates)
            [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [36]:
            [37]:     if getattr(obj, 'grad_fn', None) is not None:
            [38]:         obj.grad_fn_name = "prod"
            [39]:     return obj
            [40]:
            The document of the original function is:
    
            prod(dim=None, keepdim=False, dtype=None) -> Tensor
    
            See :func:`torch.prod`
    
            which is:
    
            prod(input, *, dtype=None) -> Tensor
    
            Returns the product of all elements in the :attr:`input` tensor.
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                    If specified, the input tensor is casted to :attr:`dtype` before the operation
                    is performed. This is useful for preventing data type overflows. Default: None.
    
            Example::
    
                >>> a = torch.randn(1, 3)
                >>> a
                tensor([[-0.8020,  0.5428, -1.5854]])
                >>> torch.prod(a)
                tensor(0.6902)
    
            .. function:: prod(input, dim, keepdim=False, *, dtype=None) -> Tensor
               :noindex:
    
            Returns the product of each row of the :attr:`input` tensor in the given
            dimension :attr:`dim`.
    
            If :attr:`keepdim` is ``True``, the output tensor is of the same size
            as :attr:`input` except in the dimension :attr:`dim` where it is of size 1.
            Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in
            the output tensor having 1 fewer dimension than :attr:`input`.
    
            Args:
                input (Tensor): the input tensor.
                dim (int): the dimension to reduce.
                keepdim (bool): whether the output tensor has :attr:`dim` retained or not.
    
            Keyword args:
                dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                    If specified, the input tensor is casted to :attr:`dtype` before the operation
                    is performed. This is useful for preventing data type overflows. Default: None.
    
            Example::
    
                >>> a = torch.randn(4, 2)
                >>> a
                tensor([[ 0.5261, -0.3837],
                        [ 1.1857, -0.2498],
                        [-1.1646,  0.0705],
                        [ 1.1131, -1.0629]])
                >>> torch.prod(a, 1)
                tensor([-0.2018, -0.2962, -0.0821, -1.1831])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = del_dim[...](input, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            for dim_iter, in zip(dim[::-1]):
                auto_generated_args = tuple(x for x in [] if x is not None)
                input = torch_super(input, 'prod')(*auto_generated_args, dim=dim_iter,keepdim=keepdim,dtype=dtype)
        obj = input
        ref_shape = size_mapping['prod'](input_shape, dim, **dict(dim=dim, keepdim=keepdim, dtype=dtype))
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "prod"
        return obj
    
    def mean(input: 'Tensor', *dim: del_dim[:], keepdim=False, dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.mean'. The automatically generated codes are as follows:
            [ 1]: def mean(input: 'Tensor', *dim: del_dim[:], keepdim=False, dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     dim = del_dim[:](input, *dim)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     input_shape=None if input is None else Size(input.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     with torch._C.DisableTorchFunction():
            [24]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [25]:         obj = torch_super(input, 'mean')(*auto_generated_args, dim,keepdim=keepdim,dtype=dtype)
            [26]:     ref_shape = size_mapping['mean'](input_shape, dim, **dict(keepdim=keepdim, dtype=dtype))
            [27]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [28]:
            [29]:     def update_special(obj, updates):
            [30]:         if isinstance(obj, tuple):
            [31]:             for x in obj: update_special(x, updates)
            [32]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [33]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [34]:
            [35]:     if getattr(obj, 'grad_fn', None) is not None:
            [36]:         obj.grad_fn_name = "mean"
            [37]:     return obj
            [38]:
            The document of the original function is:
    
            mean(dim=None, keepdim=False, *, dtype=None) -> Tensor
    
            See :func:`torch.mean`
    
            which is:
    
            mean(input, *, dtype=None) -> Tensor
    
            Returns the mean value of all elements in the :attr:`input` tensor.
    
            Args:
                input (Tensor): the input tensor.
    
            Keyword args:
                dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                    If specified, the input tensor is casted to :attr:`dtype` before the operation
                    is performed. This is useful for preventing data type overflows. Default: None.
    
            Example::
    
                >>> a = torch.randn(1, 3)
                >>> a
                tensor([[ 0.2294, -0.5481,  1.3288]])
                >>> torch.mean(a)
                tensor(0.3367)
    
            .. function:: mean(input, dim, keepdim=False, *, dtype=None, out=None) -> Tensor
               :noindex:
    
            Returns the mean value of each row of the :attr:`input` tensor in the given
            dimension :attr:`dim`. If :attr:`dim` is a list of dimensions,
            reduce over all of them.
    
            If :attr:`keepdim` is ``True``, the output tensor is of the same size
            as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
            Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
            output tensor having 1 (or ``len(dim)``) fewer dimension(s).
    
            Args:
                input (Tensor): the input tensor.
                dim (int or tuple of ints): the dimension or dimensions to reduce.
                keepdim (bool): whether the output tensor has :attr:`dim` retained or not.
    
            Keyword args:
                dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                    If specified, the input tensor is casted to :attr:`dtype` before the operation
                    is performed. This is useful for preventing data type overflows. Default: None.
                out (Tensor, optional): the output tensor.
    
            .. seealso::
    
                :func:`torch.nanmean` computes the mean value of `non-NaN` elements.
    
            Example::
    
                >>> a = torch.randn(4, 4)
                >>> a
                tensor([[-0.3841,  0.6320,  0.4254, -0.7384],
                        [-0.9644,  1.0131, -0.6549, -1.4279],
                        [-0.2951, -1.3350, -0.7694,  0.5600],
                        [ 1.0842, -0.9580,  0.3623,  0.2343]])
                >>> torch.mean(a, 1)
                tensor([-0.0163, -0.5085, -0.4599,  0.1807])
                >>> torch.mean(a, 1, True)
                tensor([[-0.0163],
                        [-0.5085],
                        [-0.4599],
                        [ 0.1807]])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = del_dim[:](input, *dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'mean')(*auto_generated_args, dim,keepdim=keepdim,dtype=dtype)
        ref_shape = size_mapping['mean'](input_shape, dim, **dict(keepdim=keepdim, dtype=dtype))
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "mean"
        return obj
    
    def std(input: 'Tensor', *dim: del_dim[:], correction=1, keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.std'. The automatically generated codes are as follows:
            [ 1]: def std(input: 'Tensor', *dim: del_dim[:], correction=1, keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     dim = del_dim[:](input, *dim)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     input_shape=None if input is None else Size(input.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     with torch._C.DisableTorchFunction():
            [24]:         auto_generated_args = tuple(x for x in [] if x is not None)
            [25]:         obj = torch_super(input, 'std')(*auto_generated_args, dim,correction=correction,keepdim=keepdim)
            [26]:     ref_shape = size_mapping['std'](input_shape, dim, **dict(correction=correction, keepdim=keepdim))
            [27]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [28]:
            [29]:     def update_special(obj, updates):
            [30]:         if isinstance(obj, tuple):
            [31]:             for x in obj: update_special(x, updates)
            [32]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [33]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [34]:
            [35]:     if getattr(obj, 'grad_fn', None) is not None:
            [36]:         obj.grad_fn_name = "std"
            [37]:     return obj
            [38]:
            The document of the original function is:
    
            std(dim=None, *, correction=1, keepdim=False) -> Tensor
    
            See :func:`torch.std`
    
            which is:
    
            std(input, dim=None, *, correction=1, keepdim=False, out=None) -> Tensor
    
            Calculates the standard deviation over the dimensions specified by :attr:`dim`.
            :attr:`dim` can be a single dimension, list of dimensions, or ``None`` to
            reduce over all dimensions.
    
            The standard deviation (:math:`\sigma`) is calculated as
    
            .. math:: \sigma = \sqrt{\frac{1}{N - \delta N}\sum_{i=0}^{N-1}(x_i-\bar{x})^2}
    
            where :math:`x` is the sample set of elements, :math:`\bar{x}` is the
            sample mean, :math:`N` is the number of samples and :math:`\delta N` is
            the :attr:`correction`.
    
            If :attr:`keepdim` is ``True``, the output tensor is of the same size
            as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
            Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
            output tensor having 1 (or ``len(dim)``) fewer dimension(s).
    
            Args:
                input (Tensor): the input tensor.
                dim (int or tuple of ints): the dimension or dimensions to reduce.
    
            Keyword args:
                correction (int): difference between the sample size and sample degrees of freedom.
                    Defaults to `Bessel's correction`_, ``correction=1``.
    
                    .. versionchanged:: 2.0
                        Previously this argument was called ``unbiased`` and was a boolean
                        with ``True`` corresponding to ``correction=1`` and ``False`` being
                        ``correction=0``.
                keepdim (bool): whether the output tensor has :attr:`dim` retained or not.
                out (Tensor, optional): the output tensor.
    
            Example:
    
                >>> a = torch.tensor(
                ...     [[ 0.2035,  1.2959,  1.8101, -0.4644],
                ...      [ 1.5027, -0.3270,  0.5905,  0.6538],
                ...      [-1.5745,  1.3330, -0.5596, -0.6548],
                ...      [ 0.1264, -0.5080,  1.6420,  0.1992]])
                >>> torch.std(a, dim=1, keepdim=True)
                tensor([[1.0311],
                        [0.7477],
                        [1.2204],
                        [0.9087]])
    
            .. _Bessel's correction: https://en.wikipedia.org/wiki/Bessel%27s_correction
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = del_dim[:](input, *dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            auto_generated_args = tuple(x for x in [] if x is not None)
            obj = torch_super(input, 'std')(*auto_generated_args, dim,correction=correction,keepdim=keepdim)
        ref_shape = size_mapping['std'](input_shape, dim, **dict(correction=correction, keepdim=keepdim))
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "std"
        return obj
    
    def norm(input: 'Tensor', p='fro', dim: del_dim[...]=None, keepdim=False, dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.norm'. The automatically generated codes are as follows:
            [ 1]: def norm(input: 'Tensor', p='fro', dim: del_dim[...]=None, keepdim=False, dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     dim = del_dim[...](input, dim)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     input_shape=None if input is None else Size(input.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     with torch._C.DisableTorchFunction():
            [24]:         for dim_iter, in zip(dim[::-1]):
            [25]:             auto_generated_args = tuple(x for x in [] if x is not None)
            [26]:             input = torch_super(input, 'norm')(*auto_generated_args, p=p,dim=dim_iter,keepdim=keepdim,dtype=dtype)
            [27]:     obj = input
            [28]:     ref_shape = size_mapping['norm'](input_shape, dim, **dict(p=p, dim=dim, keepdim=keepdim, dtype=dtype))
            [29]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [30]:
            [31]:     def update_special(obj, updates):
            [32]:         if isinstance(obj, tuple):
            [33]:             for x in obj: update_special(x, updates)
            [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [36]:
            [37]:     if getattr(obj, 'grad_fn', None) is not None:
            [38]:         obj.grad_fn_name = "norm"
            [39]:     return obj
            [40]:
            The document of the original function is:
            See :func:`torch.norm`
            which is:
            Returns the matrix norm or vector norm of a given tensor.
    
                .. warning::
    
                    torch.norm is deprecated and may be removed in a future PyTorch release.
                    Its documentation and behavior may be incorrect, and it is no longer
                    actively maintained.
    
                    Use :func:`torch.linalg.vector_norm` when computing vector norms and
                    :func:`torch.linalg.matrix_norm` when computing matrix norms.
                    For a function with a similar behavior as this one see :func:`torch.linalg.norm`.
                    Note, however, the signature for these functions is slightly different than the
                    signature for ``torch.norm``.
    
                Args:
                    input (Tensor): The input tensor. Its data type must be either a floating
                        point or complex type. For complex inputs, the norm is calculated using the
                        absolute value of each element. If the input is complex and neither
                        :attr:`dtype` nor :attr:`out` is specified, the result's data type will
                        be the corresponding floating point type (e.g. float if :attr:`input` is
                        complexfloat).
    
                    p (int, float, inf, -inf, 'fro', 'nuc', optional): the order of norm. Default: ``'fro'``
                        The following norms can be calculated:
    
                        ======  ==============  ==========================
                        ord     matrix norm     vector norm
                        ======  ==============  ==========================
                        'fro'   Frobenius norm  --
                        'nuc'   nuclear norm    --
                        Number  --              sum(abs(x)**ord)**(1./ord)
                        ======  ==============  ==========================
    
                        The vector norm can be calculated across any number of dimensions.
                        The corresponding dimensions of :attr:`input` are flattened into
                        one dimension, and the norm is calculated on the flattened
                        dimension.
    
                        Frobenius norm produces the same result as ``p=2`` in all cases
                        except when :attr:`dim` is a list of three or more dims, in which
                        case Frobenius norm throws an error.
    
                        Nuclear norm can only be calculated across exactly two dimensions.
    
                    dim (int, tuple of ints, list of ints, optional):
                        Specifies which dimension or dimensions of :attr:`input` to
                        calculate the norm across. If :attr:`dim` is ``None``, the norm will
                        be calculated across all dimensions of :attr:`input`. If the norm
                        type indicated by :attr:`p` does not support the specified number of
                        dimensions, an error will occur.
                    keepdim (bool, optional): whether the output tensors have :attr:`dim`
                        retained or not. Ignored if :attr:`dim` = ``None`` and
                        :attr:`out` = ``None``. Default: ``False``
                    out (Tensor, optional): the output tensor. Ignored if
                        :attr:`dim` = ``None`` and :attr:`out` = ``None``.
                    dtype (:class:`torch.dtype`, optional): the desired data type of
                        returned tensor. If specified, the input tensor is casted to
                        :attr:`dtype` while performing the operation. Default: None.
    
                .. note::
                    Even though ``p='fro'`` supports any number of dimensions, the true
                    mathematical definition of Frobenius norm only applies to tensors with
                    exactly two dimensions. :func:`torch.linalg.matrix_norm` with ``ord='fro'``
                    aligns with the mathematical definition, since it can only be applied across
                    exactly two dimensions.
    
                Example::
    
                    >>> import torch
                    >>> a = torch.arange(9, dtype= torch.float) - 4
                    >>> b = a.reshape((3, 3))
                    >>> torch.norm(a)
                    tensor(7.7460)
                    >>> torch.norm(b)
                    tensor(7.7460)
                    >>> torch.norm(a, float('inf'))
                    tensor(4.)
                    >>> torch.norm(b, float('inf'))
                    tensor(4.)
                    >>> c = torch.tensor([[ 1, 2, 3], [-1, 1, 4]] , dtype=torch.float)
                    >>> torch.norm(c, dim=0)
                    tensor([1.4142, 2.2361, 5.0000])
                    >>> torch.norm(c, dim=1)
                    tensor([3.7417, 4.2426])
                    >>> torch.norm(c, p=1, dim=1)
                    tensor([6., 6.])
                    >>> d = torch.arange(8, dtype=torch.float).reshape(2, 2, 2)
                    >>> torch.norm(d, dim=(1, 2))
                    tensor([ 3.7417, 11.2250])
                    >>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])
                    (tensor(3.7417), tensor(11.2250))
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = del_dim[...](input, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            for dim_iter, in zip(dim[::-1]):
                auto_generated_args = tuple(x for x in [] if x is not None)
                input = torch_super(input, 'norm')(*auto_generated_args, p=p,dim=dim_iter,keepdim=keepdim,dtype=dtype)
        obj = input
        ref_shape = size_mapping['norm'](input_shape, dim, **dict(p=p, dim=dim, keepdim=keepdim, dtype=dtype))
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "norm"
        return obj
    
    def cumsum(input: 'Tensor', dim: del_dim[1], dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.cumsum'. The automatically generated codes are as follows:
            [ 1]: def cumsum(input: 'Tensor', dim: del_dim[1], dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     dim = del_dim[1](input, dim)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     input_shape=None if input is None else Size(input.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     with torch._C.DisableTorchFunction():
            [24]:         for dim_iter, in zip(dim[::-1]):
            [25]:             auto_generated_args = tuple(x for x in [dim_iter] if x is not None)
            [26]:             input = torch_super(input, 'cumsum')(*auto_generated_args, dtype=dtype)
            [27]:     obj = input
            [28]:     ref_shape = size_mapping['cumsum'](input_shape, dim)
            [29]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [30]:
            [31]:     def update_special(obj, updates):
            [32]:         if isinstance(obj, tuple):
            [33]:             for x in obj: update_special(x, updates)
            [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [36]:
            [37]:     if getattr(obj, 'grad_fn', None) is not None:
            [38]:         obj.grad_fn_name = "cumsum"
            [39]:     return obj
            [40]:
            The document of the original function is:
    
            cumsum(dim, dtype=None) -> Tensor
    
            See :func:`torch.cumsum`
    
            which is:
    
            cumsum(input, dim, *, dtype=None, out=None) -> Tensor
    
            Returns the cumulative sum of elements of :attr:`input` in the dimension
            :attr:`dim`.
    
            For example, if :attr:`input` is a vector of size N, the result will also be
            a vector of size N, with elements.
    
            .. math::
                y_i = x_1 + x_2 + x_3 + \dots + x_i
    
            Args:
                input (Tensor): the input tensor.
                dim  (int): the dimension to do the operation over
    
            Keyword args:
                dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                    If specified, the input tensor is casted to :attr:`dtype` before the operation
                    is performed. This is useful for preventing data type overflows. Default: None.
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.randn(10)
                >>> a
                tensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,
                         0.1850, -1.1571, -0.4243])
                >>> torch.cumsum(a, dim=0)
                tensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,
                        -1.8209, -2.9780, -3.4022])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = del_dim[1](input, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            for dim_iter, in zip(dim[::-1]):
                auto_generated_args = tuple(x for x in [dim_iter] if x is not None)
                input = torch_super(input, 'cumsum')(*auto_generated_args, dtype=dtype)
        obj = input
        ref_shape = size_mapping['cumsum'](input_shape, dim)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "cumsum"
        return obj
    
    def cumprod(input: 'Tensor', dim: del_dim[1], dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.cumprod'. The automatically generated codes are as follows:
            [ 1]: def cumprod(input: 'Tensor', dim: del_dim[1], dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     dim = del_dim[1](input, dim)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     input_shape=None if input is None else Size(input.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     with torch._C.DisableTorchFunction():
            [24]:         for dim_iter, in zip(dim[::-1]):
            [25]:             auto_generated_args = tuple(x for x in [dim_iter] if x is not None)
            [26]:             input = torch_super(input, 'cumprod')(*auto_generated_args, dtype=dtype)
            [27]:     obj = input
            [28]:     ref_shape = size_mapping['cumprod'](input_shape, dim)
            [29]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
            [30]:
            [31]:     def update_special(obj, updates):
            [32]:         if isinstance(obj, tuple):
            [33]:             for x in obj: update_special(x, updates)
            [34]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [35]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [36]:
            [37]:     if getattr(obj, 'grad_fn', None) is not None:
            [38]:         obj.grad_fn_name = "cumprod"
            [39]:     return obj
            [40]:
            The document of the original function is:
    
            cumprod(dim, dtype=None) -> Tensor
    
            See :func:`torch.cumprod`
    
            which is:
    
            cumprod(input, dim, *, dtype=None, out=None) -> Tensor
    
            Returns the cumulative product of elements of :attr:`input` in the dimension
            :attr:`dim`.
    
            For example, if :attr:`input` is a vector of size N, the result will also be
            a vector of size N, with elements.
    
            .. math::
                y_i = x_1 \times x_2\times x_3\times \dots \times x_i
    
            Args:
                input (Tensor): the input tensor.
                dim  (int): the dimension to do the operation over
    
            Keyword args:
                dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                    If specified, the input tensor is casted to :attr:`dtype` before the operation
                    is performed. This is useful for preventing data type overflows. Default: None.
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> a = torch.randn(10)
                >>> a
                tensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,
                        -0.2129, -0.4206,  0.1968])
                >>> torch.cumprod(a, dim=0)
                tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,
                         0.0014, -0.0006, -0.0001])
    
                >>> a[5] = 0.0
                >>> torch.cumprod(a, dim=0)
                tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,
                         0.0000, -0.0000, -0.0000])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = del_dim[1](input, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        with torch._C.DisableTorchFunction():
            for dim_iter, in zip(dim[::-1]):
                auto_generated_args = tuple(x for x in [dim_iter] if x is not None)
                input = torch_super(input, 'cumprod')(*auto_generated_args, dtype=dtype)
        obj = input
        ref_shape = size_mapping['cumprod'](input_shape, dim)
        obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "cumprod"
        return obj
    
    def __indexed_reduce__(func_name, self: 'Tensor', *dim, keepdim=None, ret_index_only=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.__indexed_reduce__'. The automatically generated codes are as follows:
            [ 1]: def __indexed_reduce__(func_name, self: 'Tensor', *dim, keepdim=None, ret_index_only=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [self]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if self is None: ...
            [13]:     elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
            [14]:     if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if self.device.type == 'cpu': self = self.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     self_shape=None if self is None else Size(self.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     torch_returned = False
            [22]:     if len(dim) == 1 and isinstance(dim[0], torch.Tensor):
            [23]:         other = dim[0]
            [24]:         subclass = Tensor.get_tensor_subclass(self)
            [25]:         other = other.as_subclass(subclass).special_from(other.shape) if not isinstance(other, subclass) else other
            [26]:         self_shape = Size(self.shape); other_shape = Size(other.shape)
            [27]:         ref_shape, self_shape, other_shape = size_mapping_op[func_name](self_shape, other_shape)
            [28]:         with torch._C.DisableTorchFunction():
            [29]:             obj = Tensor.inherit_from(torch_super(self, func_name)(other, **(dict(out=out) if locals().get('out', None) is not None else {})), self, shape=ref_shape)
            [30]:     else:
            [31]:         dim = del_dim(self, *dim)
            [32]:         indices = None
            [33]:         num_dim = 0
            [34]:         init_shape = self.shape
            [35]:         with torch._C.DisableTorchFunction():
            [36]:             for d in dim[::-1]:
            [37]:                 result = torch_super(self, func_name)(d, **dict(keepdim=keepdim) if keepdim is not None else {})
            [38]:                 self = result.values
            [39]:                 res_indices = Tensor.inherit_from(result.indices, self, shape=[])
            [40]:                 if indices is None:  indices = res_indices.unsqueeze(FuncDim(0))
            [41]:                 elif keepdim or keepdim is None:  indices = cat(res_indices.unsqueeze(FuncDim(0)), indices.gather(d+1, res_indices.duplicate(num_dim, FuncDim(0))), 0)
            [42]:                 else:  indices = cat(res_indices.unsqueeze(FuncDim(0)), indices.gather(d+1, res_indices.unsqueeze(d).duplicate(num_dim, FuncDim(0))).squeeze_(d+1), 0)
            [43]:                 num_dim += 1
            [44]:         if keepdim is False:  cur_shape = remove_dim(init_shape, dim)
            [45]:         else:  cur_shape = init_shape
            [46]:         if ret_index_only:
            [47]:             if indices is None:  return
            [48]:             init_shape_tensor = tensor_to(init_shape, self)
            [49]:             dim_tensor = tensor(dim)
            [50]:             dim_size = cat(init_shape_tensor[dim_tensor][1:].flip().cumprod(0).flip(), tensor_like(ones(1), init_shape_tensor)).with_func_dim(True)
            [51]:             flatten_indices = (dim_size * indices).sum(FuncDim)
            [52]:             indices = indices.special_from(Size(FuncDim(1)) + cur_shape)
            [53]:             flatten_indices = flatten_indices.special_from(cur_shape)
            [54]:             if len(dim) == 1:  indices.squeeze_(0)
            [55]:             flatten_indices.indices = indices
            [56]:             flatten_indices.values = Tensor.inherit_from(self, self, shape=cur_shape)
            [57]:             if out is not None:
            [58]:                 out.zero_().add_(flatten_indices)
            [59]:             obj = flatten_indices
            [60]:         else:
            [61]:             self = Tensor.inherit_from(self, self, shape=cur_shape)
            [62]:             self.indices = indices.special_from(Size(FuncDim(1)) + cur_shape) if indices is not None else None
            [63]:             if len(dim) == 1:  self.indices.squeeze_(0)
            [64]:             # indices_tuple = indices.special_from(cur_shape + (1,)).split(dim=-1, squeezedim=True)
            [65]:             # self.indices = indices_tuple if len(dim) > 1 else indices_tuple[0]
            [66]:             self.values = self
            [67]:             if out is not None:
            [68]:                 if isinstance(out, tuple):
            [69]:                     out[0].zero_().add_(self.values)
            [70]:                     if len(out) > 1:  out[1].zero_().add_(self.indices)
            [71]:                 else:  out.zero_().add_(self.values)
            [72]:             obj = self
            [73]:
            [74]:     def update_special(obj, updates):
            [75]:         if isinstance(obj, tuple):
            [76]:             for x in obj: update_special(x, updates)
            [77]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [78]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [79]:
            [80]:     if getattr(obj, 'grad_fn', None) is not None:
            [81]:         obj.grad_fn_name = "__indexed_reduce__"
            [82]:     return obj
            [83]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [self]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if self is None: ...
        elif not isinstance(self, torch.Tensor): self = torch.tensor(self)
        if not isinstance(self, subclass): self = self.as_subclass(subclass).special_from(self.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if self.device.type == 'cpu': self = self.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        self_shape=None if self is None else Size(self.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        if len(dim) == 1 and isinstance(dim[0], torch.Tensor):
            other = dim[0]
            subclass = Tensor.get_tensor_subclass(self)
            other = other.as_subclass(subclass).special_from(other.shape) if not isinstance(other, subclass) else other
            self_shape = Size(self.shape); other_shape = Size(other.shape)
            ref_shape, self_shape, other_shape = size_mapping_op[func_name](self_shape, other_shape)
            with torch._C.DisableTorchFunction():
                obj = Tensor.inherit_from(torch_super(self, func_name)(other, **(dict(out=out) if locals().get('out', None) is not None else {})), self, shape=ref_shape)
        else:
            dim = del_dim(self, *dim)
            indices = None
            num_dim = 0
            init_shape = self.shape
            with torch._C.DisableTorchFunction():
                for d in dim[::-1]:
                    result = torch_super(self, func_name)(d, **dict(keepdim=keepdim) if keepdim is not None else {})
                    self = result.values
                    res_indices = Tensor.inherit_from(result.indices, self, shape=[])
                    if indices is None:  indices = res_indices.unsqueeze(FuncDim(0))
                    elif keepdim or keepdim is None:  indices = cat(res_indices.unsqueeze(FuncDim(0)), indices.gather(d+1, res_indices.duplicate(num_dim, FuncDim(0))), 0)
                    else:  indices = cat(res_indices.unsqueeze(FuncDim(0)), indices.gather(d+1, res_indices.unsqueeze(d).duplicate(num_dim, FuncDim(0))).squeeze_(d+1), 0)
                    num_dim += 1
            if keepdim is False:  cur_shape = remove_dim(init_shape, dim)
            else:  cur_shape = init_shape
            if ret_index_only:
                if indices is None:  return
                init_shape_tensor = tensor_to(init_shape, self)
                dim_tensor = tensor(dim)
                dim_size = cat(init_shape_tensor[dim_tensor][1:].flip().cumprod(0).flip(), tensor_like(ones(1), init_shape_tensor)).with_func_dim(True)
                flatten_indices = (dim_size * indices).sum(FuncDim)
                indices = indices.special_from(Size(FuncDim(1)) + cur_shape)
                flatten_indices = flatten_indices.special_from(cur_shape)
                if len(dim) == 1:  indices.squeeze_(0)
                flatten_indices.indices = indices
                flatten_indices.values = Tensor.inherit_from(self, self, shape=cur_shape)
                if out is not None:
                    out.zero_().add_(flatten_indices)
                obj = flatten_indices
            else:
                self = Tensor.inherit_from(self, self, shape=cur_shape)
                self.indices = indices.special_from(Size(FuncDim(1)) + cur_shape) if indices is not None else None
                if len(dim) == 1:  self.indices.squeeze_(0)
                # indices_tuple = indices.special_from(cur_shape + (1,)).split(dim=-1, squeezedim=True)
                # self.indices = indices_tuple if len(dim) > 1 else indices_tuple[0]
                self.values = self
                if out is not None:
                    if isinstance(out, tuple):
                        out[0].zero_().add_(self.values)
                        if len(out) > 1:  out[1].zero_().add_(self.indices)
                    else:  out.zero_().add_(self.values)
                obj = self
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "__indexed_reduce__"
        return obj
    
    def min(input: 'Tensor', *dim, keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.min'. The automatically generated codes are as follows:
            [ 1]: def min(input: 'Tensor', *dim, keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     torch_returned = False
            [22]:     obj = __indexed_reduce__('min', input, *dim, keepdim=keepdim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from
            [23]:
            [24]:     def update_special(obj, updates):
            [25]:         if isinstance(obj, tuple):
            [26]:             for x in obj: update_special(x, updates)
            [27]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [28]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [29]:
            [30]:     if getattr(obj, 'grad_fn', None) is not None:
            [31]:         obj.grad_fn_name = "min"
            [32]:     return obj
            [33]:
            The document of the original function is:
    
            min(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)
    
            See :func:`torch.min`
    
            which is:
    
            min(input) -> Tensor
    
            Returns the minimum value of all elements in the :attr:`input` tensor.
    
            .. warning::
                This function produces deterministic (sub)gradients unlike ``min(dim=0)``
    
            Args:
                input (Tensor): the input tensor.
    
            Example::
    
                >>> a = torch.randn(1, 3)
                >>> a
                tensor([[ 0.6750,  1.0857,  1.7197]])
                >>> torch.min(a)
                tensor(0.6750)
    
            .. function:: min(input, dim, keepdim=False, *, out=None) -> (Tensor, LongTensor)
               :noindex:
    
            Returns a namedtuple ``(values, indices)`` where ``values`` is the minimum
            value of each row of the :attr:`input` tensor in the given dimension
            :attr:`dim`. And ``indices`` is the index location of each minimum value found
            (argmin).
    
            If :attr:`keepdim` is ``True``, the output tensors are of the same size as
            :attr:`input` except in the dimension :attr:`dim` where they are of size 1.
            Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in
            the output tensors having 1 fewer dimension than :attr:`input`.
    
            .. note:: If there are multiple minimal values in a reduced row then
                      the indices of the first minimal value are returned.
    
            Args:
                input (Tensor): the input tensor.
                dim (int): the dimension to reduce.
                keepdim (bool): whether the output tensor has :attr:`dim` retained or not.
    
            Keyword args:
                out (tuple, optional): the tuple of two output tensors (min, min_indices)
    
            Example::
    
                >>> a = torch.randn(4, 4)
                >>> a
                tensor([[-0.6248,  1.1334, -1.1899, -0.2803],
                        [-1.4644, -0.2635, -0.3651,  0.6134],
                        [ 0.2457,  0.0384,  1.0128,  0.7015],
                        [-0.1153,  2.9849,  2.1458,  0.5788]])
                >>> torch.min(a, 1)
                torch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))
    
            .. function:: min(input, other, *, out=None) -> Tensor
               :noindex:
    
            See :func:`torch.minimum`.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        obj = __indexed_reduce__('min', input, *dim, keepdim=keepdim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "min"
        return obj
    
    def max(input: 'Tensor', *dim, keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.max'. The automatically generated codes are as follows:
            [ 1]: def max(input: 'Tensor', *dim, keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     torch_returned = False
            [22]:     obj = __indexed_reduce__('max', input, *dim, keepdim=keepdim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from
            [23]:
            [24]:     def update_special(obj, updates):
            [25]:         if isinstance(obj, tuple):
            [26]:             for x in obj: update_special(x, updates)
            [27]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [28]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [29]:
            [30]:     if getattr(obj, 'grad_fn', None) is not None:
            [31]:         obj.grad_fn_name = "max"
            [32]:     return obj
            [33]:
            The document of the original function is:
    
            max(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)
    
            See :func:`torch.max`
    
            which is:
    
            max(input) -> Tensor
    
            Returns the maximum value of all elements in the ``input`` tensor.
    
            .. warning::
                This function produces deterministic (sub)gradients unlike ``max(dim=0)``
    
            Args:
                input (Tensor): the input tensor.
    
            Example::
    
                >>> a = torch.randn(1, 3)
                >>> a
                tensor([[ 0.6763,  0.7445, -2.2369]])
                >>> torch.max(a)
                tensor(0.7445)
    
            .. function:: max(input, dim, keepdim=False, *, out=None) -> (Tensor, LongTensor)
               :noindex:
    
            Returns a namedtuple ``(values, indices)`` where ``values`` is the maximum
            value of each row of the :attr:`input` tensor in the given dimension
            :attr:`dim`. And ``indices`` is the index location of each maximum value found
            (argmax).
    
            If ``keepdim`` is ``True``, the output tensors are of the same size
            as ``input`` except in the dimension ``dim`` where they are of size 1.
            Otherwise, ``dim`` is squeezed (see :func:`torch.squeeze`), resulting
            in the output tensors having 1 fewer dimension than ``input``.
    
            .. note:: If there are multiple maximal values in a reduced row then
                      the indices of the first maximal value are returned.
    
            Args:
                input (Tensor): the input tensor.
                dim (int): the dimension to reduce.
                keepdim (bool): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.
    
            Keyword args:
                out (tuple, optional): the result tuple of two output tensors (max, max_indices)
    
            Example::
    
                >>> a = torch.randn(4, 4)
                >>> a
                tensor([[-1.2360, -0.2942, -0.1222,  0.8475],
                        [ 1.1949, -1.1127, -2.2379, -0.6702],
                        [ 1.5717, -0.9207,  0.1297, -1.8768],
                        [-0.6172,  1.0036, -0.6060, -0.2432]])
                >>> torch.max(a, 1)
                torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))
    
            .. function:: max(input, other, *, out=None) -> Tensor
               :noindex:
    
            See :func:`torch.maximum`.
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        obj = __indexed_reduce__('max', input, *dim, keepdim=keepdim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "max"
        return obj
    
    def median(input: 'Tensor', *dim, keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.median'. The automatically generated codes are as follows:
            [ 1]: def median(input: 'Tensor', *dim, keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     torch_returned = False
            [22]:     obj = __indexed_reduce__('median', input, *dim, keepdim=keepdim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from
            [23]:
            [24]:     def update_special(obj, updates):
            [25]:         if isinstance(obj, tuple):
            [26]:             for x in obj: update_special(x, updates)
            [27]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [28]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [29]:
            [30]:     if getattr(obj, 'grad_fn', None) is not None:
            [31]:         obj.grad_fn_name = "median"
            [32]:     return obj
            [33]:
            The document of the original function is:
    
            median(dim=None, keepdim=False) -> (Tensor, LongTensor)
    
            See :func:`torch.median`
    
            which is:
    
            median(input) -> Tensor
    
            Returns the median of the values in :attr:`input`.
    
            .. note::
                The median is not unique for :attr:`input` tensors with an even number
                of elements. In this case the lower of the two medians is returned. To
                compute the mean of both medians, use :func:`torch.quantile` with ``q=0.5`` instead.
    
            .. warning::
                This function produces deterministic (sub)gradients unlike ``median(dim=0)``
    
            Args:
                input (Tensor): the input tensor.
    
            Example::
    
                >>> a = torch.randn(1, 3)
                >>> a
                tensor([[ 1.5219, -1.5212,  0.2202]])
                >>> torch.median(a)
                tensor(0.2202)
    
            .. function:: median(input, dim=-1, keepdim=False, *, out=None) -> (Tensor, LongTensor)
               :noindex:
    
            Returns a namedtuple ``(values, indices)`` where ``values`` contains the median of each row of :attr:`input`
            in the dimension :attr:`dim`, and ``indices`` contains the index of the median values found in the dimension :attr:`dim`.
    
            By default, :attr:`dim` is the last dimension of the :attr:`input` tensor.
    
            If :attr:`keepdim` is ``True``, the output tensors are of the same size
            as :attr:`input` except in the dimension :attr:`dim` where they are of size 1.
            Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in
            the outputs tensor having 1 fewer dimension than :attr:`input`.
    
            .. note::
                The median is not unique for :attr:`input` tensors with an even number
                of elements in the dimension :attr:`dim`. In this case the lower of the
                two medians is returned. To compute the mean of both medians in
                :attr:`input`, use :func:`torch.quantile` with ``q=0.5`` instead.
    
            .. warning::
                ``indices`` does not necessarily contain the first occurrence of each
                median value found, unless it is unique.
                The exact implementation details are device-specific.
                Do not expect the same result when run on CPU and GPU in general.
                For the same reason do not expect the gradients to be deterministic.
    
            Args:
                input (Tensor): the input tensor.
                dim (int): the dimension to reduce.
                keepdim (bool): whether the output tensor has :attr:`dim` retained or not.
    
            Keyword args:
                out ((Tensor, Tensor), optional): The first tensor will be populated with the median values and the second
                                                  tensor, which must have dtype long, with their indices in the dimension
                                                  :attr:`dim` of :attr:`input`.
    
            Example::
    
                >>> a = torch.randn(4, 5)
                >>> a
                tensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],
                        [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],
                        [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],
                        [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])
                >>> torch.median(a, 1)
                torch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        obj = __indexed_reduce__('median', input, *dim, keepdim=keepdim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "median"
        return obj
    
    def cummin(input: 'Tensor', dim: exist_dim[1]=None, *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.cummin'. The automatically generated codes are as follows:
            [ 1]: def cummin(input: 'Tensor', dim: exist_dim[1]=None, *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     dim = exist_dim[1](input, dim)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     input_shape=None if input is None else Size(input.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     torch_returned = False
            [24]:     obj = __indexed_reduce__('cummin', input, *dim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from
            [25]:
            [26]:     def update_special(obj, updates):
            [27]:         if isinstance(obj, tuple):
            [28]:             for x in obj: update_special(x, updates)
            [29]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [30]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [31]:
            [32]:     if getattr(obj, 'grad_fn', None) is not None:
            [33]:         obj.grad_fn_name = "cummin"
            [34]:     return obj
            [35]:
            The document of the original function is:
    
            cummin(dim) -> (Tensor, Tensor)
    
            See :func:`torch.cummin`
    
            which is:
    
            cummin(input, dim, *, out=None) -> (Tensor, LongTensor)
            Returns a namedtuple ``(values, indices)`` where ``values`` is the cumulative minimum of
            elements of :attr:`input` in the dimension :attr:`dim`. And ``indices`` is the index
            location of each maximum value found in the dimension :attr:`dim`.
    
            .. math::
                y_i = min(x_1, x_2, x_3, \dots, x_i)
    
            Args:
                input (Tensor): the input tensor.
                dim  (int): the dimension to do the operation over
    
            Keyword args:
                out (tuple, optional): the result tuple of two output tensors (values, indices)
    
            Example::
    
                >>> a = torch.randn(10)
                >>> a
                tensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220, -0.3885,  1.1762,
                     0.9165,  1.6684])
                >>> torch.cummin(a, dim=0)
                torch.return_types.cummin(
                    values=tensor([-0.2284, -0.6628, -0.6628, -0.6628, -1.3298, -1.3298, -1.3298, -1.3298,
                    -1.3298, -1.3298]),
                    indices=tensor([0, 1, 1, 1, 4, 4, 4, 4, 4, 4]))
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = exist_dim[1](input, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        obj = __indexed_reduce__('cummin', input, *dim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "cummin"
        return obj
    
    def cummax(input: 'Tensor', dim: exist_dim[1]=None, *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.cummax'. The automatically generated codes are as follows:
            [ 1]: def cummax(input: 'Tensor', dim: exist_dim[1]=None, *, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     dim = exist_dim[1](input, dim)
            [19]:
            [20]:     # Obtain available sized in arguments (which will be fed into size function).
            [21]:     input_shape=None if input is None else Size(input.shape)
            [22]:     # Use the given inner codes if they are provided.
            [23]:     torch_returned = False
            [24]:     obj = __indexed_reduce__('cummax', input, *dim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from
            [25]:
            [26]:     def update_special(obj, updates):
            [27]:         if isinstance(obj, tuple):
            [28]:             for x in obj: update_special(x, updates)
            [29]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [30]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [31]:
            [32]:     if getattr(obj, 'grad_fn', None) is not None:
            [33]:         obj.grad_fn_name = "cummax"
            [34]:     return obj
            [35]:
            The document of the original function is:
    
            cummax(dim) -> (Tensor, Tensor)
    
            See :func:`torch.cummax`
    
            which is:
    
            cummax(input, dim, *, out=None) -> (Tensor, LongTensor)
            Returns a namedtuple ``(values, indices)`` where ``values`` is the cumulative maximum of
            elements of :attr:`input` in the dimension :attr:`dim`. And ``indices`` is the index
            location of each maximum value found in the dimension :attr:`dim`.
    
            .. math::
                y_i = max(x_1, x_2, x_3, \dots, x_i)
    
            Args:
                input (Tensor): the input tensor.
                dim  (int): the dimension to do the operation over
    
            Keyword args:
                out (tuple, optional): the result tuple of two output tensors (values, indices)
    
            Example::
    
                >>> a = torch.randn(10)
                >>> a
                tensor([-0.3449, -1.5447,  0.0685, -1.5104, -1.1706,  0.2259,  1.4696, -1.3284,
                     1.9946, -0.8209])
                >>> torch.cummax(a, dim=0)
                torch.return_types.cummax(
                    values=tensor([-0.3449, -0.3449,  0.0685,  0.0685,  0.0685,  0.2259,  1.4696,  1.4696,
                     1.9946,  1.9946]),
                    indices=tensor([0, 0, 2, 2, 2, 5, 6, 6, 8, 8]))
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = exist_dim[1](input, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        obj = __indexed_reduce__('cummax', input, *dim, **dict(out=out) if locals().get('out', None) is not None else {}) # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "cummax"
        return obj
    
    def argmin(input: 'Tensor', *dim, keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.argmin'. The automatically generated codes are as follows:
            [ 1]: def argmin(input: 'Tensor', *dim, keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     torch_returned = False
            [22]:     obj = __indexed_reduce__('min', input, *dim, keepdim=keepdim, ret_index_only=True) # suppress:  special_from
            [23]:
            [24]:     def update_special(obj, updates):
            [25]:         if isinstance(obj, tuple):
            [26]:             for x in obj: update_special(x, updates)
            [27]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [28]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [29]:
            [30]:     if getattr(obj, 'grad_fn', None) is not None:
            [31]:         obj.grad_fn_name = "argmin"
            [32]:     return obj
            [33]:
            The document of the original function is:
    
            argmin(dim=None, keepdim=False) -> LongTensor
    
            See :func:`torch.argmin`
    
            which is:
    
            argmin(input, dim=None, keepdim=False) -> LongTensor
    
            Returns the indices of the minimum value(s) of the flattened tensor or along a dimension
    
            This is the second value returned by :meth:`torch.min`. See its
            documentation for the exact semantics of this method.
    
            .. note:: If there are multiple minimal values then the indices of the first minimal value are returned.
    
            Args:
                input (Tensor): the input tensor.
                dim (int): the dimension to reduce. If ``None``, the argmin of the flattened input is returned.
                keepdim (bool): whether the output tensor has :attr:`dim` retained or not..
    
            Example::
    
                >>> a = torch.randn(4, 4)
                >>> a
                tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],
                        [ 1.0100, -1.1975, -0.0102, -0.4732],
                        [-0.9240,  0.1207, -0.7506, -1.0213],
                        [ 1.7809, -1.2960,  0.9384,  0.1438]])
                >>> torch.argmin(a)
                tensor(13)
                >>> torch.argmin(a, dim=1)
                tensor([ 2,  1,  3,  1])
                >>> torch.argmin(a, dim=1, keepdim=True)
                tensor([[2],
                        [1],
                        [3],
                        [1]])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        obj = __indexed_reduce__('min', input, *dim, keepdim=keepdim, ret_index_only=True) # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "argmin"
        return obj
    
    def argmax(input: 'Tensor', *dim, keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.argmax'. The automatically generated codes are as follows:
            [ 1]: def argmax(input: 'Tensor', *dim, keepdim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:         ***
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     # Obtain available sized in arguments (which will be fed into size function).
            [19]:     input_shape=None if input is None else Size(input.shape)
            [20]:     # Use the given inner codes if they are provided.
            [21]:     torch_returned = False
            [22]:     obj = __indexed_reduce__('max', input, *dim, keepdim=keepdim, ret_index_only=True) # suppress:  special_from
            [23]:
            [24]:     def update_special(obj, updates):
            [25]:         if isinstance(obj, tuple):
            [26]:             for x in obj: update_special(x, updates)
            [27]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [28]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [29]:
            [30]:     if getattr(obj, 'grad_fn', None) is not None:
            [31]:         obj.grad_fn_name = "argmax"
            [32]:     return obj
            [33]:
            The document of the original function is:
    
            argmax(dim=None, keepdim=False) -> LongTensor
    
            See :func:`torch.argmax`
    
            which is:
    
            argmax(input) -> LongTensor
    
            Returns the indices of the maximum value of all elements in the :attr:`input` tensor.
    
            This is the second value returned by :meth:`torch.max`. See its
            documentation for the exact semantics of this method.
    
            .. note:: If there are multiple maximal values then the indices of the first maximal value are returned.
    
            Args:
                input (Tensor): the input tensor.
    
            Example::
    
                >>> a = torch.randn(4, 4)
                >>> a
                tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
                        [-0.7401, -0.8805, -0.3402, -1.1936],
                        [ 0.4907, -1.3948, -1.0691, -0.3132],
                        [-1.6092,  0.5419, -0.2993,  0.3195]])
                >>> torch.argmax(a)
                tensor(0)
    
            .. function:: argmax(input, dim, keepdim=False) -> LongTensor
               :noindex:
    
            Returns the indices of the maximum values of a tensor across a dimension.
    
            This is the second value returned by :meth:`torch.max`. See its
            documentation for the exact semantics of this method.
    
            Args:
                input (Tensor): the input tensor.
                dim (int): the dimension to reduce. If ``None``, the argmax of the flattened input is returned.
                keepdim (bool): whether the output tensor has :attr:`dim` retained or not. Ignored if ``dim=None``.
    
            Example::
    
                >>> a = torch.randn(4, 4)
                >>> a
                tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],
                        [-0.7401, -0.8805, -0.3402, -1.1936],
                        [ 0.4907, -1.3948, -1.0691, -0.3132],
                        [-1.6092,  0.5419, -0.2993,  0.3195]])
                >>> torch.argmax(a, dim=1)
                tensor([ 0,  2,  0,  1])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        obj = __indexed_reduce__('max', input, *dim, keepdim=keepdim, ret_index_only=True) # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "argmax"
        return obj
    
    def split(self, split_size: int=1, dim: exist_dim[1] = {}, squeezedim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
        split(self, split_size: int=1, dim: exist_dim = {}) -> Tensor
        Split a tensor into a tuple of tensors, along `dim`, with split_size for each tensor.
    
        Args:
            split_size (int or list, optional): The split size for each tensor, using a list of integers adding up to size to split the dimension accordingly. Defaults to 1.
            dim (int/exist_dim, optional): The dimension to split along. Defaults to batch.
    
        Automatically inheritted method from 'torch.Tensor.split'. The automatically generated codes are as follows:
            [ 1]: def split(self, split_size: int=1, dim: exist_dim[1] = {}, squeezedim=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:     split(self, split_size: int=1, dim: exist_dim = {}) -> Tensor
            [ 4]:     Split a tensor into a tuple of tensors, along `dim`, with split_size for each tensor.
            [ 5]:
            [ 6]:     Args:
            [ 7]:         split_size (int or list, optional): The split size for each tensor, using a list of integers adding up to size to split the dimension accordingly. Defaults to 1.
            [ 8]:         dim (int/exist_dim, optional): The dimension to split along. Defaults to batch.
            [ 9]:     '''
            [10]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [11]:
            [12]:     pivot = None
            [13]:     for t in []:
            [14]:         if isinstance(t, torch.Tensor): pivot = t; break
            [15]:     subclass = Tensor.get_tensor_subclass(pivot)
            [16]:
            [17]:     dim = exist_dim[1](self, dim)
            [18]:
            [19]:     # Obtain available sized in arguments (which will be fed into size function).
            [20]:
            [21]:     # Use the given inner codes if they are provided.
            [22]:     torch_returned = False
            [23]:     dim = dim[0]
            [24]:     with torch._C.DisableTorchFunction():
            [25]:         if squeezedim:
            [26]:             avouch(split_size == 1 or all_(s == 1 for s in split_size), TypeError("Keyword argument 'squeezedim' is only active for 'split_size=1' in bt.Tensor.split. "))
            [27]:             obj = tuple(Tensor.inherit_from(x, self).squeeze_(dim) for x in torch_super(self, 'split')(split_size, dim))
            [28]:         else: torch_returned = True; obj = tuple(Tensor.inherit_from(x, self) for x in torch_super(self, 'split')(split_size, dim))
            [29]:
            [30]:     def update_special(obj, updates):
            [31]:         if isinstance(obj, tuple):
            [32]:             for x in obj: update_special(x, updates)
            [33]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [34]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [35]:
            [36]:     if getattr(obj, 'grad_fn', None) is not None:
            [37]:         obj.grad_fn_name = "split"
            [38]:     return obj
            [39]:
            The document of the original function is:
            See :func:`torch.split`
            which is:
            Splits the tensor into chunks. Each chunk is a view of the original tensor.
    
                If :attr:`split_size_or_sections` is an integer type, then :attr:`tensor` will
                be split into equally sized chunks (if possible). Last chunk will be smaller if
                the tensor size along the given dimension :attr:`dim` is not divisible by
                :attr:`split_size`.
    
                If :attr:`split_size_or_sections` is a list, then :attr:`tensor` will be split
                into ``len(split_size_or_sections)`` chunks with sizes in :attr:`dim` according
                to :attr:`split_size_or_sections`.
    
                Args:
                    tensor (Tensor): tensor to split.
                    split_size_or_sections (int) or (list(int)): size of a single chunk or
                        list of sizes for each chunk
                    dim (int): dimension along which to split the tensor.
    
                Example::
    
                    >>> a = torch.arange(10).reshape(5, 2)
                    >>> a
                    tensor([[0, 1],
                            [2, 3],
                            [4, 5],
                            [6, 7],
                            [8, 9]])
                    >>> torch.split(a, 2)
                    (tensor([[0, 1],
                             [2, 3]]),
                     tensor([[4, 5],
                             [6, 7]]),
                     tensor([[8, 9]]))
                    >>> torch.split(a, [1, 4])
                    (tensor([[0, 1]]),
                     tensor([[2, 3],
                             [4, 5],
                             [6, 7],
                             [8, 9]]))
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in []:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        dim = exist_dim[1](self, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
    
        # Use the given inner codes if they are provided.
        torch_returned = False
        dim = dim[0]
        with torch._C.DisableTorchFunction():
            if squeezedim:
                avouch(split_size == 1 or all_(s == 1 for s in split_size), TypeError("Keyword argument 'squeezedim' is only active for 'split_size=1' in bt.Tensor.split. "))
                obj = tuple(Tensor.inherit_from(x, self).squeeze_(dim) for x in torch_super(self, 'split')(split_size, dim))
            else: torch_returned = True; obj = tuple(Tensor.inherit_from(x, self) for x in torch_super(self, 'split')(split_size, dim))
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "split"
        return obj
    
    def sample(self, number: int = 1, dim: exist_dim = {}, random: bool = True, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
        sample(self, numbder: int = 1, dim: int = self.batch_dimension, random: bool = True) -> Tensor
    
        Sample `number` of slices from a given dimension.
    
        Args:
            number (int, optional): the number of slices. Defaults to `1`.
            dim (int/exist_dim, keyword argument): the dimension to slice or select. Defaults to batch dimension.
            random (bool, optional): whether to randomly pick the slices or not. Defaults to True.
    
        Note:
            Using `sample(num, dim)` for data of size (n_1, n_2, ..., n_r) would result in
                a tensor of size (n_1, n_2, ..., n_{dim-1}, num, n_{dim+1}, ..., n_r)
    
        Examples::
            >>> data.shape
            batorch.Size([4, 3], 5, 6)
            >>> data.sample(1, 2, random=False).shape
            batorch.Size([4, 3], 1, 6)
            >>> # The above is equivalant to data[:, :, :1, ...].shape.
            >>> data.sample(7, [], random=False).shape
            batorch.Size(7, 5, 6)
            >>> # The above is equivalant to data.flatten(0, 1)[:7].shape.
    
        Automatically inheritted method from 'torch.Tensor.sample'. The automatically generated codes are as follows:
            [ 1]: def sample(self, number: int = 1, dim: exist_dim = {}, random: bool = True, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:     sample(self, numbder: int = 1, dim: int = self.batch_dimension, random: bool = True) -> Tensor
            [ 4]:
            [ 5]:     Sample `number` of slices from a given dimension.
            [ 6]:
            [ 7]:     Args:
            [ 8]:         number (int, optional): the number of slices. Defaults to `1`.
            [ 9]:         dim (int/exist_dim, keyword argument): the dimension to slice or select. Defaults to batch dimension.
            [10]:         random (bool, optional): whether to randomly pick the slices or not. Defaults to True.
            [11]:
            [12]:     Note:
            [13]:         Using `sample(num, dim)` for data of size (n_1, n_2, ..., n_r) would result in
            [14]:             a tensor of size (n_1, n_2, ..., n_{dim-1}, num, n_{dim+1}, ..., n_r)
            [15]:
            [16]:     Examples::
            [17]:         >>> data.shape
            [18]:         batorch.Size([4, 3], 5, 6)
            [19]:         >>> data.sample(1, 2, random=False).shape
            [20]:         batorch.Size([4, 3], 1, 6)
            [21]:         >>> # The above is equivalant to data[:, :, :1, ...].shape.
            [22]:         >>> data.sample(7, [], random=False).shape
            [23]:         batorch.Size(7, 5, 6)
            [24]:         >>> # The above is equivalant to data.flatten(0, 1)[:7].shape.
            [25]:     '''
            [26]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [27]:
            [28]:     pivot = None
            [29]:     for t in []:
            [30]:         if isinstance(t, torch.Tensor): pivot = t; break
            [31]:     subclass = Tensor.get_tensor_subclass(pivot)
            [32]:
            [33]:     dim = exist_dim(self, dim)
            [34]:
            [35]:     # Obtain available sized in arguments (which will be fed into size function).
            [36]:
            [37]:     # Use the given inner codes if they are provided.
            [38]:     torch_returned = False
            [39]:     if len(dim) > 1:  self = self.merge_dims(*dim, target=dim[0])
            [40]:     sample_indices = [slice(None)] * self.n_dim
            [41]:     dim = dim[0]
            [42]:     if random:
            [43]:         import random
            [44]:         n_total = self.size(dim)
            [45]:         n_round = number // n_total
            [46]:         n_remain = number % n_total
            [47]:         samples = cat(randperm({n_round}, n_total, device=self.device).flatten().view(-1), tensor(random.sample(range_(n_total), k = n_remain), device=self.device), 0)
            [48]:     else:
            [49]:         avouch(number <= self.size(dim), TypeError(f"Too many elements needed to be sampled from dimension {dim}"))
            [50]:         samples = tensor(range_(number))
            [51]:     sample_indices[dim] = samples.special_from(self.shape[dim:dim+1])
            [52]:     obj = self[tuple(sample_indices)] # suppress:  special_from
            [53]:
            [54]:     def update_special(obj, updates):
            [55]:         if isinstance(obj, tuple):
            [56]:             for x in obj: update_special(x, updates)
            [57]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [58]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [59]:
            [60]:     if getattr(obj, 'grad_fn', None) is not None:
            [61]:         obj.grad_fn_name = "sample"
            [62]:     return obj
            [63]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in []:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        dim = exist_dim(self, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
    
        # Use the given inner codes if they are provided.
        torch_returned = False
        if len(dim) > 1:  self = self.merge_dims(*dim, target=dim[0])
        sample_indices = [slice(None)] * self.n_dim
        dim = dim[0]
        if random:
            import random
            n_total = self.size(dim)
            n_round = number // n_total
            n_remain = number % n_total
            samples = cat(randperm({n_round}, n_total, device=self.device).flatten().view(-1), tensor(random.sample(range_(n_total), k = n_remain), device=self.device), 0)
        else:
            avouch(number <= self.size(dim), TypeError(f"Too many elements needed to be sampled from dimension {dim}"))
            samples = tensor(range_(number))
        sample_indices[dim] = samples.special_from(self.shape[dim:dim+1])
        obj = self[tuple(sample_indices)] # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "sample"
        return obj
    
    def pick(self, index: int = None, dim: exist_dim = {}, random: bool = False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
        pick(self, index: int = 0, dim: int = self.batch_dimension, random: bool = False) -> Tensor
    
        Pick one slice on dimension `dim` for big tensors.
    
        Args:
            index (int, optional): the slice index to pick. Defaults to `None`.
            dim (int/exist_dim, keyword argument): the dimension to slice or select. Defaults to batch dimension.
            random (bool, optional): whether to randomly pick the slice or not. Defaults to False.
    
        Note:
            Using `pick(index, dim)` for data of size (n_1, n_2, ..., n_r) would result in
                a tensor of size (n_1, n_2, ..., n_{dim-1}, n_{dim+1}, ..., n_r)
    
        Examples::
            >>> data.shape
            batorch.Size(4, 3, 5, 6)
            >>> data.pick(-1, 2, random=False).shape
            batorch.Size(4, 3, 6)
            >>> # The above is equivalant to data[:, :, 4, ...].shape.
    
        Automatically inheritted method from 'torch.Tensor.pick'. The automatically generated codes are as follows:
            [ 1]: def pick(self, index: int = None, dim: exist_dim = {}, random: bool = False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:     pick(self, index: int = 0, dim: int = self.batch_dimension, random: bool = False) -> Tensor
            [ 4]:
            [ 5]:     Pick one slice on dimension `dim` for big tensors.
            [ 6]:
            [ 7]:     Args:
            [ 8]:         index (int, optional): the slice index to pick. Defaults to `None`.
            [ 9]:         dim (int/exist_dim, keyword argument): the dimension to slice or select. Defaults to batch dimension.
            [10]:         random (bool, optional): whether to randomly pick the slice or not. Defaults to False.
            [11]:
            [12]:     Note:
            [13]:         Using `pick(index, dim)` for data of size (n_1, n_2, ..., n_r) would result in
            [14]:             a tensor of size (n_1, n_2, ..., n_{dim-1}, n_{dim+1}, ..., n_r)
            [15]:
            [16]:     Examples::
            [17]:         >>> data.shape
            [18]:         batorch.Size(4, 3, 5, 6)
            [19]:         >>> data.pick(-1, 2, random=False).shape
            [20]:         batorch.Size(4, 3, 6)
            [21]:         >>> # The above is equivalant to data[:, :, 4, ...].shape.
            [22]:     '''
            [23]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [24]:
            [25]:     pivot = None
            [26]:     for t in []:
            [27]:         if isinstance(t, torch.Tensor): pivot = t; break
            [28]:     subclass = Tensor.get_tensor_subclass(pivot)
            [29]:
            [30]:     dim = exist_dim(self, dim)
            [31]:
            [32]:     # Obtain available sized in arguments (which will be fed into size function).
            [33]:
            [34]:     # Use the given inner codes if they are provided.
            [35]:     torch_returned = False
            [36]:     if len(dim) > 1:  self = self.merge_dims(*dim, target=dim[0])
            [37]:     dim = dim[0]
            [38]:     if random:
            [39]:         avouch(index is None, "'index' should be None if random pick is enabled. Use keyword argument 'dim=xx' to identify the dimension.")
            [40]:         import random
            [41]:         index = random.randint(0, self.size(dim)-1)
            [42]:     else:
            [43]:         avouch(isinstance(index, int_) and -self.size(dim) <= index < self.size(dim) or isinstance(index, (slice, Tensor)),
            [44]:                TypeError(f"Invalid index for picking from dimension {dim} of size {self.size(dim)}: {index} ({index.__class__}). "))
            [45]:     obj = self[(slice(None),) * dim + (index,)] # suppress:  special_from
            [46]:
            [47]:     def update_special(obj, updates):
            [48]:         if isinstance(obj, tuple):
            [49]:             for x in obj: update_special(x, updates)
            [50]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [51]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [52]:
            [53]:     if getattr(obj, 'grad_fn', None) is not None:
            [54]:         obj.grad_fn_name = "pick"
            [55]:     return obj
            [56]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in []:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        dim = exist_dim(self, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
    
        # Use the given inner codes if they are provided.
        torch_returned = False
        if len(dim) > 1:  self = self.merge_dims(*dim, target=dim[0])
        dim = dim[0]
        if random:
            avouch(index is None, "'index' should be None if random pick is enabled. Use keyword argument 'dim=xx' to identify the dimension.")
            import random
            index = random.randint(0, self.size(dim)-1)
        else:
            avouch(isinstance(index, int_) and -self.size(dim) <= index < self.size(dim) or isinstance(index, (slice, Tensor)),
                   TypeError(f"Invalid index for picking from dimension {dim} of size {self.size(dim)}: {index} ({index.__class__}). "))
        obj = self[(slice(None),) * dim + (index,)] # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "pick"
        return obj
    
    def eig(input: 'Tensor', dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
        Find eigen values and vectors for matrix `input`: (for the first available condition):
        (1) in feature dimensions if more than 2D is available;
        (2) in space dimensions if more than 2D is available;
        (3) in sequence dimensions if more than 2D is available.
    
        Automatically inheritted method from 'torch.Tensor.eig'. The automatically generated codes are as follows:
            [ 1]: def eig(input: 'Tensor', dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]:     Find eigen values and vectors for matrix `input`: (for the first available condition):
            [ 4]:     (1) in feature dimensions if more than 2D is available;
            [ 5]:     (2) in space dimensions if more than 2D is available;
            [ 6]:     (3) in sequence dimensions if more than 2D is available.
            [ 7]:     '''
            [ 8]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 9]:
            [10]:     pivot = None
            [11]:     for t in [input]:
            [12]:         if isinstance(t, torch.Tensor): pivot = t; break
            [13]:     subclass = Tensor.get_tensor_subclass(pivot)
            [14]:
            [15]:     if input is None: ...
            [16]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [17]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [18]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [19]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [20]:
            [21]:     dim = linalg_dim[2](input, dim)
            [22]:
            [23]:     # Obtain available sized in arguments (which will be fed into size function).
            [24]:     input_shape=None if input is None else Size(input.shape)
            [25]:     # Use the given inner codes if they are provided.
            [26]:     torch_returned = False
            [27]:     has_batch = False
            [28]:     with input.hide_special(), torch._C.DisableTorchFunction():
            [29]:         A = input.move_dim(dim, -1)
            [30]:         if A.n_dim > 2:  A = A.flatten(0, -3); has_batch = True
            [31]:         if torch.__version__ >= Version('1.10'):
            [32]:             L, V = torch.linalg.eig(A)
            [33]:         else:
            [34]:             K, P = torch.eig(A, eigenvectors=True)
            [35]:             L = torch.complex(K[:, 0], K[:, 1])
            [36]:             Vr = torch.where((K[:, 1] < 0).reshape((1, -1)), torch.cat((P[:, :1], P[:, :-1]), 1), P)
            [37]:             Vi = (K[:, 1] > 0).reshape((1, -1)) * torch.cat((P[:, 1:], P[:, -1:]), 1) - (K[:, 1] < 0).reshape((1, -1)) * P
            [38]:             V = torch.complex(Vr, Vi)
            [39]:         L = Tensor.inherit_from(L, input, shape=[])
            [40]:         V = Tensor.inherit_from(V, input, shape=[])
            [41]:     dim_type = input.shape[dim[0]:dim[0]+1]
            [42]:     if has_batch:
            [43]:         L = L.split_dim(0, remove_dim(input.shape, dim))
            [44]:         V = V.split_dim(0, remove_dim(input.shape, dim))
            [45]:     L = L.move_dim(-1, dim[0]).add_special_dim(dim[0], dim_type)
            [46]:     V = V.move_dim([-2, -1], dim).add_special_dim(dim[0], dim_type).add_special_dim(dim[0]+1, dim_type)
            [47]:     obj = L, V # suppress:  special_from
            [48]:
            [49]:     def update_special(obj, updates):
            [50]:         if isinstance(obj, tuple):
            [51]:             for x in obj: update_special(x, updates)
            [52]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [53]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [54]:
            [55]:     if getattr(obj, 'grad_fn', None) is not None:
            [56]:         obj.grad_fn_name = "eig"
            [57]:     return obj
            [58]:
            The document of the original function is:
            None
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = linalg_dim[2](input, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        has_batch = False
        with input.hide_special(), torch._C.DisableTorchFunction():
            A = input.move_dim(dim, -1)
            if A.n_dim > 2:  A = A.flatten(0, -3); has_batch = True
            if torch.__version__ >= Version('1.10'):
                L, V = torch.linalg.eig(A)
            else:
                K, P = torch.eig(A, eigenvectors=True)
                L = torch.complex(K[:, 0], K[:, 1])
                Vr = torch.where((K[:, 1] < 0).reshape((1, -1)), torch.cat((P[:, :1], P[:, :-1]), 1), P)
                Vi = (K[:, 1] > 0).reshape((1, -1)) * torch.cat((P[:, 1:], P[:, -1:]), 1) - (K[:, 1] < 0).reshape((1, -1)) * P
                V = torch.complex(Vr, Vi)
            L = Tensor.inherit_from(L, input, shape=[])
            V = Tensor.inherit_from(V, input, shape=[])
        dim_type = input.shape[dim[0]:dim[0]+1]
        if has_batch:
            L = L.split_dim(0, remove_dim(input.shape, dim))
            V = V.split_dim(0, remove_dim(input.shape, dim))
        L = L.move_dim(-1, dim[0]).add_special_dim(dim[0], dim_type)
        V = V.move_dim([-2, -1], dim).add_special_dim(dim[0], dim_type).add_special_dim(dim[0]+1, dim_type)
        obj = L, V # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "eig"
        return obj
    
    def matmul(input: 'Tensor', other: 'Tensor', *, dim1=None, dim2=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
        perform matmul for the best linear dimensions, justlike other mat** functions do.
    
        Automatically inheritted method from 'torch.Tensor.matmul'. The automatically generated codes are as follows:
            [ 1]: def matmul(input: 'Tensor', other: 'Tensor', *, dim1=None, dim2=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 2]:     '''
            [ 3]: perform matmul for the best linear dimensions, justlike other mat** functions do.
            [ 4]:     '''
            [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 6]:
            [ 7]:     pivot = None
            [ 8]:     for t in [input, other]:
            [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
            [10]:     subclass = Tensor.get_tensor_subclass(pivot)
            [11]:
            [12]:     if input is None: ...
            [13]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [14]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [16]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [17]:
            [18]:     if other is None: ...
            [19]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
            [20]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
            [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [22]:         if other.device.type == 'cpu': other = other.to(pivot.device)
            [23]:
            [24]:     # Obtain available sized in arguments (which will be fed into size function).
            [25]:     input_shape=None if input is None else Size(input.shape)
            [26]:     other_shape=None if other is None else Size(other.shape)
            [27]:     # Use the given inner codes if they are provided.
            [28]:     torch_returned = False
            [29]:     if input.has_feature and other.has_feature:
            [30]:         dim1 = exist_dim(input, [])
            [31]:         dim2 = exist_dim(other, [])
            [32]:     elif input.has_space and other.has_space:
            [33]:         dim1 = exist_dim(input, ...)
            [34]:         dim2 = exist_dim(other, ...)
            [35]:     elif input.has_sequence and other.has_sequence:
            [36]:         dim1 = exist_dim(input, '')
            [37]:         dim2 = exist_dim(other, '')
            [38]:     else:  raise TypeError(f"Cannot perform matmul alignment for shapes {input_shape} and {other_shape}. ")
            [39]:     dim1 = dim1[-2:]
            [40]:     dim2 = dim2[-2:]
            [41]:     size = 2 if len(dim1) == len(dim2) else 1
            [42]:     input = input.move_dim(dim1, -1)
            [43]:     other = other.move_dim(dim2, -1)
            [44]:     obj = (input @ other).movedim(list(range_(-size, 0)), dim2[0]) # suppress:  special_from
            [45]:
            [46]:     def update_special(obj, updates):
            [47]:         if isinstance(obj, tuple):
            [48]:             for x in obj: update_special(x, updates)
            [49]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [50]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [51]:
            [52]:     if getattr(obj, 'grad_fn', None) is not None:
            [53]:         obj.grad_fn_name = "matmul"
            [54]:     return obj
            [55]:
            The document of the original function is:
    
            matmul(tensor2) -> Tensor
    
            See :func:`torch.matmul`
    
            which is:
    
            matmul(input, other, *, out=None) -> Tensor
    
            Matrix product of two tensors.
    
            The behavior depends on the dimensionality of the tensors as follows:
    
            - If both tensors are 1-dimensional, the dot product (scalar) is returned.
            - If both arguments are 2-dimensional, the matrix-matrix product is returned.
            - If the first argument is 1-dimensional and the second argument is 2-dimensional,
              a 1 is prepended to its dimension for the purpose of the matrix multiply.
              After the matrix multiply, the prepended dimension is removed.
            - If the first argument is 2-dimensional and the second argument is 1-dimensional,
              the matrix-vector product is returned.
            - If both arguments are at least 1-dimensional and at least one argument is
              N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first
              argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the
              batched matrix multiply and removed after.  If the second argument is 1-dimensional, a
              1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.
              The non-matrix (i.e. batch) dimensions are :ref:`broadcasted <broadcasting-semantics>` (and thus
              must be broadcastable).  For example, if :attr:`input` is a
              :math:`(j \times 1 \times n \times n)` tensor and :attr:`other` is a :math:`(k \times n \times n)`
              tensor, :attr:`out` will be a :math:`(j \times k \times n \times n)` tensor.
    
              Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs
              are broadcastable, and not the matrix dimensions. For example, if :attr:`input` is a
              :math:`(j \times 1 \times n \times m)` tensor and :attr:`other` is a :math:`(k \times m \times p)`
              tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the
              matrix dimensions) are different. :attr:`out` will be a :math:`(j \times k \times n \times p)` tensor.
    
            This operation has support for arguments with :ref:`sparse layouts<sparse-docs>`. In particular the
            matrix-matrix (both arguments 2-dimensional) supports sparse arguments with the same restrictions
            as :func:`torch.mm`
    
            .. warning::
                Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported,
                or may not have autograd support. If you notice missing functionality please
                open a feature request.
    
            This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
            On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.
    
            .. note::
    
                The 1-dimensional dot product version of this function does not support an :attr:`out` parameter.
    
            Arguments:
                input (Tensor): the first tensor to be multiplied
                other (Tensor): the second tensor to be multiplied
    
            Keyword args:
                out (Tensor, optional): the output tensor.
    
            Example::
    
                >>> # vector x vector
                >>> tensor1 = torch.randn(3)
                >>> tensor2 = torch.randn(3)
                >>> torch.matmul(tensor1, tensor2).size()
                torch.Size([])
                >>> # matrix x vector
                >>> tensor1 = torch.randn(3, 4)
                >>> tensor2 = torch.randn(4)
                >>> torch.matmul(tensor1, tensor2).size()
                torch.Size([3])
                >>> # batched matrix x broadcasted vector
                >>> tensor1 = torch.randn(10, 3, 4)
                >>> tensor2 = torch.randn(4)
                >>> torch.matmul(tensor1, tensor2).size()
                torch.Size([10, 3])
                >>> # batched matrix x batched matrix
                >>> tensor1 = torch.randn(10, 3, 4)
                >>> tensor2 = torch.randn(10, 4, 5)
                >>> torch.matmul(tensor1, tensor2).size()
                torch.Size([10, 3, 5])
                >>> # batched matrix x broadcasted matrix
                >>> tensor1 = torch.randn(10, 3, 4)
                >>> tensor2 = torch.randn(4, 5)
                >>> torch.matmul(tensor1, tensor2).size()
                torch.Size([10, 3, 5])
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input, other]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        if other is None: ...
        elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if other.device.type == 'cpu': other = other.to(pivot.device)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        other_shape=None if other is None else Size(other.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        if input.has_feature and other.has_feature:
            dim1 = exist_dim(input, [])
            dim2 = exist_dim(other, [])
        elif input.has_space and other.has_space:
            dim1 = exist_dim(input, ...)
            dim2 = exist_dim(other, ...)
        elif input.has_sequence and other.has_sequence:
            dim1 = exist_dim(input, '')
            dim2 = exist_dim(other, '')
        else:  raise TypeError(f"Cannot perform matmul alignment for shapes {input_shape} and {other_shape}. ")
        dim1 = dim1[-2:]
        dim2 = dim2[-2:]
        size = 2 if len(dim1) == len(dim2) else 1
        input = input.move_dim(dim1, -1)
        other = other.move_dim(dim2, -1)
        obj = (input @ other).movedim(list(range_(-size, 0)), dim2[0]) # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "matmul"
        return obj
    
    @alias("matrix_power")
    def matpow(input: 'Tensor', k, *, dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
        return a matrix power of A^k.
    
        Automatically inheritted method from 'torch.Tensor.matpow'. The automatically generated codes are as follows:
            [ 1]: @alias("matrix_power")
            [ 2]: def matpow(input: 'Tensor', k, *, dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 3]:     '''
            [ 4]: return a matrix power of A^k.
            [ 5]:     '''
            [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 7]:
            [ 8]:     pivot = None
            [ 9]:     for t in [input]:
            [10]:         if isinstance(t, torch.Tensor): pivot = t; break
            [11]:     subclass = Tensor.get_tensor_subclass(pivot)
            [12]:
            [13]:     if input is None: ...
            [14]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [15]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [17]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [18]:
            [19]:     dim = linalg_dim[2](input, dim)
            [20]:
            [21]:     # Obtain available sized in arguments (which will be fed into size function).
            [22]:     input_shape=None if input is None else Size(input.shape)
            [23]:     # Use the given inner codes if they are provided.
            [24]:     torch_returned = False
            [25]:     L, V = eig(input, dim=dim)
            [26]:     L = L.move_dim(dim[0], -1)
            [27]:     V = V.move_dim(dim, -1)
            [28]:     L_k = where((L.real < 0) & (L.imag.abs() < 1e-6), -complex((-L.real) ** k, L.imag), L ** k)
            [29]:     R = V @ diag(L_k, dim=-1) @ V.inv()
            [30]:     if R.is_complex() and not input.is_complex():  R = R.real
            [31]:     obj = R.move_dim([-2, -1], dim).type(input.dtype) # suppress:  special_from
            [32]:
            [33]:     def update_special(obj, updates):
            [34]:         if isinstance(obj, tuple):
            [35]:             for x in obj: update_special(x, updates)
            [36]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [37]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [38]:
            [39]:     if getattr(obj, 'grad_fn', None) is not None:
            [40]:         obj.grad_fn_name = "matpow"
            [41]:     return obj
            [42]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = linalg_dim[2](input, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        L, V = eig(input, dim=dim)
        L = L.move_dim(dim[0], -1)
        V = V.move_dim(dim, -1)
        L_k = where((L.real < 0) & (L.imag.abs() < 1e-6), -complex((-L.real) ** k, L.imag), L ** k)
        R = V @ diag(L_k, dim=-1) @ V.inv()
        if R.is_complex() and not input.is_complex():  R = R.real
        obj = R.move_dim([-2, -1], dim).type(input.dtype) # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "matpow"
        return obj
    
    @alias("matrix_exp")
    def matexp(input: 'Tensor', *, dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
        return a matrix exponential of e^A.
    
        Automatically inheritted method from 'torch.Tensor.matexp'. The automatically generated codes are as follows:
            [ 1]: @alias("matrix_exp")
            [ 2]: def matexp(input: 'Tensor', *, dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 3]:     '''
            [ 4]: return a matrix exponential of e^A.
            [ 5]:     '''
            [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 7]:
            [ 8]:     pivot = None
            [ 9]:     for t in [input]:
            [10]:         if isinstance(t, torch.Tensor): pivot = t; break
            [11]:     subclass = Tensor.get_tensor_subclass(pivot)
            [12]:
            [13]:     if input is None: ...
            [14]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [15]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [17]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [18]:
            [19]:     dim = linalg_dim[2](input, dim)
            [20]:
            [21]:     # Obtain available sized in arguments (which will be fed into size function).
            [22]:     input_shape=None if input is None else Size(input.shape)
            [23]:     # Use the given inner codes if they are provided.
            [24]:     torch_returned = False
            [25]:     L, V = eig(input, dim=dim)
            [26]:     L = L.move_dim(dim[0], -1)
            [27]:     V = V.move_dim(dim, -1)
            [28]:     R = V @ diag(exp(L), dim=-1) @ V.inv()
            [29]:     if R.is_complex() and not input.is_complex():  R = R.real
            [30]:     obj = R.move_dim([-2, -1], dim).type(input.dtype) # suppress:  special_from
            [31]:
            [32]:     def update_special(obj, updates):
            [33]:         if isinstance(obj, tuple):
            [34]:             for x in obj: update_special(x, updates)
            [35]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [36]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [37]:
            [38]:     if getattr(obj, 'grad_fn', None) is not None:
            [39]:         obj.grad_fn_name = "matexp"
            [40]:     return obj
            [41]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = linalg_dim[2](input, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        L, V = eig(input, dim=dim)
        L = L.move_dim(dim[0], -1)
        V = V.move_dim(dim, -1)
        R = V @ diag(exp(L), dim=-1) @ V.inv()
        if R.is_complex() and not input.is_complex():  R = R.real
        obj = R.move_dim([-2, -1], dim).type(input.dtype) # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "matexp"
        return obj
    
    @alias("matrix_log")
    def matlog(input: 'Tensor', *, dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
        return a matrix exponential of e^A.
    
        Automatically inheritted method from 'torch.Tensor.matlog'. The automatically generated codes are as follows:
            [ 1]: @alias("matrix_log")
            [ 2]: def matlog(input: 'Tensor', *, dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 3]:     '''
            [ 4]: return a matrix exponential of e^A.
            [ 5]:     '''
            [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 7]:
            [ 8]:     pivot = None
            [ 9]:     for t in [input]:
            [10]:         if isinstance(t, torch.Tensor): pivot = t; break
            [11]:     subclass = Tensor.get_tensor_subclass(pivot)
            [12]:
            [13]:     if input is None: ...
            [14]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [15]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [17]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [18]:
            [19]:     dim = linalg_dim[2](input, dim)
            [20]:
            [21]:     # Obtain available sized in arguments (which will be fed into size function).
            [22]:     input_shape=None if input is None else Size(input.shape)
            [23]:     # Use the given inner codes if they are provided.
            [24]:     torch_returned = False
            [25]:     L, V = eig(input, dim=dim)
            [26]:     L = L.move_dim(dim[0], -1)
            [27]:     V = V.move_dim(dim, -1)
            [28]:     R = V @ diag(log(L), dim=-1) @ V.inv()
            [29]:     if R.is_complex() and not input.is_complex():  R = R.real
            [30]:     obj = R.move_dim([-2, -1], dim).type(input.dtype) # suppress:  special_from
            [31]:
            [32]:     def update_special(obj, updates):
            [33]:         if isinstance(obj, tuple):
            [34]:             for x in obj: update_special(x, updates)
            [35]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [36]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [37]:
            [38]:     if getattr(obj, 'grad_fn', None) is not None:
            [39]:         obj.grad_fn_name = "matlog"
            [40]:     return obj
            [41]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = linalg_dim[2](input, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        L, V = eig(input, dim=dim)
        L = L.move_dim(dim[0], -1)
        V = V.move_dim(dim, -1)
        R = V @ diag(log(L), dim=-1) @ V.inv()
        if R.is_complex() and not input.is_complex():  R = R.real
        obj = R.move_dim([-2, -1], dim).type(input.dtype) # suppress:  special_from
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "matlog"
        return obj
    
    @alias("matrix_rank")
    def rank(input: 'Tensor', *, atol=None, rtol=None, hermitian=False, dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.rank'. The automatically generated codes are as follows:
            [ 1]: @alias("matrix_rank")
            [ 2]: def rank(input: 'Tensor', *, atol=None, rtol=None, hermitian=False, dim: linalg_dim[2]=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 3]:     '''
            [ 4]:         ***
            [ 5]:     '''
            [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 7]:
            [ 8]:     pivot = None
            [ 9]:     for t in [input]:
            [10]:         if isinstance(t, torch.Tensor): pivot = t; break
            [11]:     subclass = Tensor.get_tensor_subclass(pivot)
            [12]:
            [13]:     if input is None: ...
            [14]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [15]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [17]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [18]:
            [19]:     dim = linalg_dim[2](input, dim)
            [20]:
            [21]:     # Obtain available sized in arguments (which will be fed into size function).
            [22]:     input_shape=None if input is None else Size(input.shape)
            [23]:     # Use the given inner codes if they are provided.
            [24]:     torch_returned = False
            [25]:     A = input.move_dim(dim, -1)
            [26]:     with torch._C.DisableTorchFunction():
            [27]:         obj = Tensor.inherit_from(torch.linalg.matrix_rank(A, atol=atol, rtol=rtol, hermitian=hermitian, **dict(out=out) if locals().get('out', None) is not None else {}), input, shape=A.shape[:-2])
            [28]:
            [29]:     def update_special(obj, updates):
            [30]:         if isinstance(obj, tuple):
            [31]:             for x in obj: update_special(x, updates)
            [32]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [33]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [34]:
            [35]:     if getattr(obj, 'grad_fn', None) is not None:
            [36]:         obj.grad_fn_name = "rank"
            [37]:     return obj
            [38]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = linalg_dim[2](input, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        A = input.move_dim(dim, -1)
        with torch._C.DisableTorchFunction():
            obj = Tensor.inherit_from(torch.linalg.matrix_rank(A, atol=atol, rtol=rtol, hermitian=hermitian, **dict(out=out) if locals().get('out', None) is not None else {}), input, shape=A.shape[:-2])
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "rank"
        return obj
    
    @alias("matrix_norm")
    def matnorm(input: 'Tensor', ord='fro', dim: linalg_dim[2]=None, keepdim=False, *, dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        """
    
        Automatically inheritted method from 'torch.Tensor.matnorm'. The automatically generated codes are as follows:
            [ 1]: @alias("matrix_norm")
            [ 2]: def matnorm(input: 'Tensor', ord='fro', dim: linalg_dim[2]=None, keepdim=False, *, dtype=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
            [ 3]:     '''
            [ 4]:         ***
            [ 5]:     '''
            [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
            [ 7]:
            [ 8]:     pivot = None
            [ 9]:     for t in [input]:
            [10]:         if isinstance(t, torch.Tensor): pivot = t; break
            [11]:     subclass = Tensor.get_tensor_subclass(pivot)
            [12]:
            [13]:     if input is None: ...
            [14]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
            [15]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
            [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            [17]:         if input.device.type == 'cpu': input = input.to(pivot.device)
            [18]:
            [19]:     dim = linalg_dim[2](input, dim)
            [20]:
            [21]:     # Obtain available sized in arguments (which will be fed into size function).
            [22]:     input_shape=None if input is None else Size(input.shape)
            [23]:     # Use the given inner codes if they are provided.
            [24]:     torch_returned = False
            [25]:     A = input.move_dim(dim, -1)
            [26]:     with torch._C.DisableTorchFunction():
            [27]:         obj = Tensor.inherit_from(torch.linalg.matrix_norm(A, ord=ord, dim=dim, keepdim=keepdim, dtype=dtype, **dict(out=out) if locals().get('out', None) is not None else {}), input, shape=A.shape if keepdim else A.shape[:-2])
            [28]:
            [29]:     def update_special(obj, updates):
            [30]:         if isinstance(obj, tuple):
            [31]:             for x in obj: update_special(x, updates)
            [32]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
            [33]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
            [34]:
            [35]:     if getattr(obj, 'grad_fn', None) is not None:
            [36]:         obj.grad_fn_name = "matnorm"
            [37]:     return obj
            [38]:
    
        """
        # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
    
        pivot = None
        for t in [input]:
            if isinstance(t, torch.Tensor): pivot = t; break
        subclass = Tensor.get_tensor_subclass(pivot)
    
        if input is None: ...
        elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
            if input.device.type == 'cpu': input = input.to(pivot.device)
    
        dim = linalg_dim[2](input, dim)
    
        # Obtain available sized in arguments (which will be fed into size function).
        input_shape=None if input is None else Size(input.shape)
        # Use the given inner codes if they are provided.
        torch_returned = False
        A = input.move_dim(dim, -1)
        with torch._C.DisableTorchFunction():
            obj = Tensor.inherit_from(torch.linalg.matrix_norm(A, ord=ord, dim=dim, keepdim=keepdim, dtype=dtype, **dict(out=out) if locals().get('out', None) is not None else {}), input, shape=A.shape if keepdim else A.shape[:-2])
    
        def update_special(obj, updates):
            if isinstance(obj, tuple):
                for x in obj: update_special(x, updates)
            elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
    
        if getattr(obj, 'grad_fn', None) is not None:
            obj.grad_fn_name = "matnorm"
        return obj
    
    
    @classmethod
    def __torch_function__(cls, func, types, args=(), kwargs=None):

        try:
            # if Tensor in types and cls != Tensor: return NotImplemented
            
            self = args[0] if len(args) > 0 else None
            # if func.__qualname__.startswith("_VariableFunctionsClass"): # basic functions 'torch.*'
            #     if func.__name__ in globals(): self = None

            # if func.__qualname__.startswith("_TensorBase"): # tensor functions 'torch.Tensor;*'
            #     if hasattr(Tensor, func.__name__, func.__name__): self = None

            types = tuple(cls if t.__name__ == "Parameter" else t for t in types)
            ret = super().__torch_function__(func, types, args, kwargs)
            if isinstance(ret, torch.Tensor):
                avouch(isinstance(ret, cls), RuntimeError(f"Error in having return value not of subclass '{cls.__name__}', this should be done by PyTorch >= 1.6. "))
                if hasattr(ret, 'special_dims'): return ret
                ret.init_special()
                if isinstance(self, Tensor) and ret.n_dim == self.n_dim: ret.special_from(self)
            
            return ret
            
        except Exception as e:
            raise e.__class__(f"[In function {func.__qualname__}]\t" + str(e))

# Inherit all the properties for special dimensions from 'bt.Size'. 
for dim in SpecialDimensions + [SpaceDim, SpecDim]:
    
    Tensor.get_dim_type = Size.get_dim_type
    Tensor.get_dim_start = Size.get_dim_start
    Tensor.get_dim_stop = Size.get_dim_stop
    Tensor.get_dim_range = Size.get_dim_range
    Tensor.get_dim_domain = Size.get_dim_domain
    Tensor.has_dim = Size.has_dim
    Tensor.is_of_dim = Size.is_of_dim
    Tensor.get_i_dim = Size.get_i_dim
    Tensor.set_i_dim = Size.set_i_dim
    Tensor.get_n_dim = Size.get_n_dim
    Tensor.set_n_dim = Size.set_n_dim
    def get_dim_size(self, dim, use_tuple=True):
        return Size.get_dim_size(self.shape, dim, use_tuple=use_tuple)
    Tensor.get_dim_size = get_dim_size
    
    # property methods
    for name in (
        dim.named_vars("is_{name_dim}") + 
        (dim.named_vars("with_{name_dim}") if dim not in (SpecDim, SpaceDim) else tuple()) +
        (dim.named_vars("with_i_{name_dim}") if dim not in (SpecDim, SpaceDim) and dim.unique else tuple()) +
        (dim.named_vars("with_prop_{name_dim}") if dim not in (SpecDim, SpaceDim) else tuple()) + 
        (dim.named_vars("with_n_{name_dim}") if dim not in (SpecDim, SpaceDim) and not dim.unique else tuple())
    ): setattr(Tensor, name, getattr(Size, name))
    
    # properties with get
    for name in (
        ("i_special_dims",) +
        dim.named_vars("has_{name}") + dim.named_vars("has_{name_dim}") + 
        (dim.named_vars("{name}_start") + dim.named_vars("{name}_stop") + dim.named_vars("{name}_range") + dim.named_vars("{name}_domain") + 
            dim.named_vars("n_{name}") + dim.named_vars("n{name}") + dim.named_vars("{name}_size") if dim not in (SpecDim,) else tuple()) + 
        (dim.named_vars("n_{name_dim}") if dim in (SpecDim, SpaceDim) else tuple()) + 
        (dim.named_vars("{name}") if dim not in (SpecDim,) and not dim.unique else tuple())
    ): setattr(Tensor, name, property(getattr(Size, name).fget))
    
    # properties with set&get
    for name in (
        (dim.named_vars("i_{name_dim}") if dim not in (SpecDim, SpaceDim) and dim.unique else tuple()) + 
        (dim.named_vars("{name_dim}") if dim not in (SpecDim, SpaceDim) and dim.unique else tuple()) + 
        (dim.named_vars("n_{name_dim}") if dim not in (SpecDim, SpaceDim) else tuple())
    ): setattr(Tensor, name, property(getattr(Size, name).fget).setter(getattr(Size, name).fset))

def expand(self: 'Tensor', *sizes: Size): return self.expand(*sizes)
def expand_as(self: 'Tensor', other: 'Tensor'): return self.expand_as(other)
def expand_to(self: 'Tensor', *target, assign_to_dims: exist_dim=None, dims_allow_mismatch: exist_dim=None): return self.expand_to(*target, assign_to_dims=assign_to_dims, dims_allow_mismatch=dims_allow_mismatch)

### START GLOBAL AUTO GENERATION
def complex(real: 'Tensor', imag: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.complex'. The automatically generated codes are as follows:
        [ 1]: def complex(real: 'Tensor', imag: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [real, imag]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if real is None: ...
        [13]:     elif not isinstance(real, torch.Tensor): real = torch.tensor(real)
        [14]:     if not isinstance(real, subclass): real = real.as_subclass(subclass).special_from(real.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if real.device.type == 'cpu': real = real.to(pivot.device)
        [17]:
        [18]:     if imag is None: ...
        [19]:     elif not isinstance(imag, torch.Tensor): imag = torch.tensor(imag)
        [20]:     if not isinstance(imag, subclass): imag = imag.as_subclass(subclass).special_from(imag.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if imag.device.type == 'cpu': imag = imag.to(pivot.device)
        [23]:
        [24]:     # Obtain available sized in arguments (which will be fed into size function).
        [25]:     real_shape=None if real is None else Size(real.shape)
        [26]:     imag_shape=None if imag is None else Size(imag.shape)
        [27]:     # Use the given inner codes if they are provided.
        [28]:     ref_shape, real_shape, imag_shape = size_mapping_op['complex'](real_shape, imag_shape)
        [29]:     real = real.view(real_shape)
        [30]:     imag = imag.view(imag_shape)
        [31]:     with torch._C.DisableTorchFunction():
        [32]:         auto_generated_args = tuple(x for x in [real,imag] if x is not None)
        [33]:         obj = torch.complex(*auto_generated_args, out=out)
        [34]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [35]:
        [36]:     def update_special(obj, updates):
        [37]:         if isinstance(obj, tuple):
        [38]:             for x in obj: update_special(x, updates)
        [39]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [40]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [41]:
        [42]:     if getattr(obj, 'grad_fn', None) is not None:
        [43]:         obj.grad_fn_name = "complex"
        [44]:     return obj
        [45]:
        The document of the original function is:

        complex(real, imag, *, out=None) -> Tensor

        Constructs a complex tensor with its real part equal to :attr:`real` and its
        imaginary part equal to :attr:`imag`.

        Args:
            real (Tensor): The real part of the complex tensor. Must be float or double.
            imag (Tensor): The imaginary part of the complex tensor. Must be same dtype
                as :attr:`real`.

        Keyword args:
            out (Tensor): If the inputs are ``torch.float32``, must be
                ``torch.complex64``. If the inputs are ``torch.float64``, must be
                ``torch.complex128``.

        Example::

            >>> real = torch.tensor([1, 2], dtype=torch.float32)
            >>> imag = torch.tensor([3, 4], dtype=torch.float32)
            >>> z = torch.complex(real, imag)
            >>> z
            tensor([(1.+3.j), (2.+4.j)])
            >>> z.dtype
            torch.complex64

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [real, imag]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if real is None: ...
    elif not isinstance(real, torch.Tensor): real = torch.tensor(real)
    if not isinstance(real, subclass): real = real.as_subclass(subclass).special_from(real.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if real.device.type == 'cpu': real = real.to(pivot.device)

    if imag is None: ...
    elif not isinstance(imag, torch.Tensor): imag = torch.tensor(imag)
    if not isinstance(imag, subclass): imag = imag.as_subclass(subclass).special_from(imag.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if imag.device.type == 'cpu': imag = imag.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    real_shape=None if real is None else Size(real.shape)
    imag_shape=None if imag is None else Size(imag.shape)
    # Use the given inner codes if they are provided.
    ref_shape, real_shape, imag_shape = size_mapping_op['complex'](real_shape, imag_shape)
    real = real.view(real_shape)
    imag = imag.view(imag_shape)
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [real,imag] if x is not None)
        obj = torch.complex(*auto_generated_args, out=out)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "complex"
    return obj

def tensor(data, *, dtype=None, device=_device.main_device, requires_grad=False, pin_memory=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.tensor'. The automatically generated codes are as follows:
        [ 1]: def tensor(data, *, dtype=None, device=_device.main_device, requires_grad=False, pin_memory=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in []:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     # Obtain available sized in arguments (which will be fed into size function).
        [13]:
        [14]:     # Use the given inner codes if they are provided.
        [15]:     torch_returned = False
        [16]:     if isinstance(data, torch.Tensor):
        [17]:         if isinstance(data, bt.Tensor): torch_returned = True; obj = data.clone()
        [18]:         else: torch_returned = True; obj = data
        [19]:     if not torch_returned:
        [20]:         with torch._C.DisableTorchFunction():
        [21]:             obj = torch.tensor(data, dtype=dtype,device=device,requires_grad=requires_grad,pin_memory=pin_memory)
        [22]:     ref_shape = size_mapping['tensor']()
        [23]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [24]:
        [25]:     def update_special(obj, updates):
        [26]:         if isinstance(obj, tuple):
        [27]:             for x in obj: update_special(x, updates)
        [28]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [29]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [30]:
        [31]:     if getattr(obj, 'grad_fn', None) is not None:
        [32]:         obj.grad_fn_name = "tensor"
        [33]:     return obj
        [34]:
        The document of the original function is:

        tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False) -> Tensor

        Constructs a tensor with no autograd history (also known as a "leaf tensor", see :doc:`/notes/autograd`) by copying :attr:`data`.

        .. warning::

            When working with tensors prefer using :func:`torch.Tensor.clone`,
            :func:`torch.Tensor.detach`, and :func:`torch.Tensor.requires_grad_` for
            readability. Letting `t` be a tensor, ``torch.tensor(t)`` is equivalent to
            ``t.clone().detach()``, and ``torch.tensor(t, requires_grad=True)``
            is equivalent to ``t.clone().detach().requires_grad_(True)``.

        .. seealso::

            :func:`torch.as_tensor` preserves autograd history and avoids copies where possible.
            :func:`torch.from_numpy` creates a tensor that shares storage with a NumPy array.

        Args:
            data (array_like): Initial data for the tensor. Can be a list, tuple,
                NumPy ``ndarray``, scalar, and other types.

        Keyword args:
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                Default: if ``None``, infers data type from :attr:`data`.
            device (:class:`torch.device`, optional): the device of the constructed tensor. If None and data is a tensor
                then the device of data is used. If None and data is not a tensor then
                the result tensor is constructed on the CPU.
            requires_grad (bool, optional): If autograd should record operations on the
                returned tensor. Default: ``False``.
            pin_memory (bool, optional): If set, returned tensor would be allocated in
                the pinned memory. Works only for CPU tensors. Default: ``False``.

        Example::

            >>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])
            tensor([[ 0.1000,  1.2000],
                    [ 2.2000,  3.1000],
                    [ 4.9000,  5.2000]])

            >>> torch.tensor([0, 1])  # Type inference on data
            tensor([ 0,  1])

            >>> torch.tensor([[0.11111, 0.222222, 0.3333333]],
            ...              dtype=torch.float64,
            ...              device=torch.device('cuda:0'))  # creates a double tensor on a CUDA device
            tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')

            >>> torch.tensor(3.14159)  # Create a zero-dimensional (scalar) tensor
            tensor(3.1416)

            >>> torch.tensor([])  # Create an empty tensor (of size (0,))
            tensor([])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    # Obtain available sized in arguments (which will be fed into size function).

    # Use the given inner codes if they are provided.
    torch_returned = False
    if isinstance(data, torch.Tensor):
        if isinstance(data, bt.Tensor): torch_returned = True; obj = data.clone()
        else: torch_returned = True; obj = data
    if not torch_returned:
        with torch._C.DisableTorchFunction():
            obj = torch.tensor(data, dtype=dtype,device=device,requires_grad=requires_grad,pin_memory=pin_memory)
    ref_shape = size_mapping['tensor']()
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "tensor"
    return obj

def as_tensor(data: 'Tensor', dtype=None, device=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.as_tensor'. The automatically generated codes are as follows:
        [ 1]: def as_tensor(data: 'Tensor', dtype=None, device=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [data]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if data is None: ...
        [13]:     elif not isinstance(data, torch.Tensor): data = torch.tensor(data)
        [14]:     if not isinstance(data, subclass): data = data.as_subclass(subclass).special_from(data.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if data.device.type == 'cpu': data = data.to(pivot.device)
        [17]:
        [18]:     # Obtain available sized in arguments (which will be fed into size function).
        [19]:     data_shape=None if data is None else Size(data.shape)
        [20]:     # Use the given inner codes if they are provided.
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         auto_generated_args = tuple(x for x in [data] if x is not None)
        [23]:         obj = torch.as_tensor(*auto_generated_args, dtype=dtype,device=device)
        [24]:     ref_shape = size_mapping['as_tensor'](data_shape)
        [25]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [26]:
        [27]:     def update_special(obj, updates):
        [28]:         if isinstance(obj, tuple):
        [29]:             for x in obj: update_special(x, updates)
        [30]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [31]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [32]:
        [33]:     if getattr(obj, 'grad_fn', None) is not None:
        [34]:         obj.grad_fn_name = "as_tensor"
        [35]:     return obj
        [36]:
        The document of the original function is:

        as_tensor(data, dtype=None, device=None) -> Tensor

        Converts :attr:`data` into a tensor, sharing data and preserving autograd
        history if possible.

        If :attr:`data` is already a tensor with the requested dtype and device
        then :attr:`data` itself is returned, but if :attr:`data` is a
        tensor with a different dtype or device then it's copied as if using
        `data.to(dtype=dtype, device=device)`.

        If :attr:`data` is a NumPy array (an ndarray) with the same dtype and device then a
        tensor is constructed using :func:`torch.from_numpy`.

        .. seealso::

            :func:`torch.tensor` never shares its data and creates a new "leaf tensor" (see :doc:`/notes/autograd`).

        Args:
            data (array_like): Initial data for the tensor. Can be a list, tuple,
                NumPy ``ndarray``, scalar, and other types.
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                Default: if ``None``, infers data type from :attr:`data`.
            device (:class:`torch.device`, optional): the device of the constructed tensor. If None and data is a tensor
                then the device of data is used. If None and data is not a tensor then
                the result tensor is constructed on the CPU.

        Example::

            >>> a = numpy.array([1, 2, 3])
            >>> t = torch.as_tensor(a)
            >>> t
            tensor([ 1,  2,  3])
            >>> t[0] = -1
            >>> a
            array([-1,  2,  3])

            >>> a = numpy.array([1, 2, 3])
            >>> t = torch.as_tensor(a, device=torch.device('cuda'))
            >>> t
            tensor([ 1,  2,  3])
            >>> t[0] = -1
            >>> a
            array([1,  2,  3])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [data]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if data is None: ...
    elif not isinstance(data, torch.Tensor): data = torch.tensor(data)
    if not isinstance(data, subclass): data = data.as_subclass(subclass).special_from(data.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if data.device.type == 'cpu': data = data.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    data_shape=None if data is None else Size(data.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [data] if x is not None)
        obj = torch.as_tensor(*auto_generated_args, dtype=dtype,device=device)
    ref_shape = size_mapping['as_tensor'](data_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "as_tensor"
    return obj

@collect_memory
def empty(*size: Size, out=None, dtype=None, layout=torch.strided, device=_device.main_device, requires_grad=False, pin_memory=False, memory_format=torch.contiguous_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.empty'. The automatically generated codes are as follows:
        [ 1]: @collect_memory
        [ 2]: def empty(*size: Size, out=None, dtype=None, layout=torch.strided, device=_device.main_device, requires_grad=False, pin_memory=False, memory_format=torch.contiguous_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:         ***
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in []:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     # Obtain available sized in arguments (which will be fed into size function).
        [14]:     size=Size(*size)
        [15]:     # Use the given inner codes if they are provided.
        [16]:     with torch._C.DisableTorchFunction():
        [17]:         auto_generated_args = tuple(x for x in [] if x is not None)
        [18]:         obj = torch.empty(*auto_generated_args, *size,out=out,dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,pin_memory=pin_memory,memory_format=memory_format)
        [19]:     ref_shape = size_mapping['empty'](size)
        [20]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [21]:
        [22]:     def update_special(obj, updates):
        [23]:         if isinstance(obj, tuple):
        [24]:             for x in obj: update_special(x, updates)
        [25]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [26]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [27]:
        [28]:     if getattr(obj, 'grad_fn', None) is not None:
        [29]:         obj.grad_fn_name = "empty"
        [30]:     return obj
        [31]:
        The document of the original function is:

        empty(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False, memory_format=torch.contiguous_format) -> Tensor

        Returns a tensor filled with uninitialized data. The shape of the tensor is
        defined by the variable argument :attr:`size`.

        Args:
            size (int...): a sequence of integers defining the shape of the output tensor.
                Can be a variable number of arguments or a collection like a list or tuple.

        Keyword args:
            out (Tensor, optional): the output tensor.
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).
            layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
                Default: ``torch.strided``.
            device (:class:`torch.device`, optional): the desired device of returned tensor.
                Default: if ``None``, uses the current device for the default tensor type
                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU
                for CPU tensor types and the current CUDA device for CUDA tensor types.
            requires_grad (bool, optional): If autograd should record operations on the
                returned tensor. Default: ``False``.
            pin_memory (bool, optional): If set, returned tensor would be allocated in
                the pinned memory. Works only for CPU tensors. Default: ``False``.
            memory_format (:class:`torch.memory_format`, optional): the desired memory format of
                returned Tensor. Default: ``torch.contiguous_format``.

        Example::

            >>> torch.empty((2,3), dtype=torch.int64)
            tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],
                    [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    # Obtain available sized in arguments (which will be fed into size function).
    size=Size(*size)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [] if x is not None)
        obj = torch.empty(*auto_generated_args, *size,out=out,dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,pin_memory=pin_memory,memory_format=memory_format)
    ref_shape = size_mapping['empty'](size)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "empty"
    return obj

@collect_memory
def full(*size, fill_value=None, out=None, dtype=None, layout=torch.strided, device=_device.main_device, requires_grad=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.full'. The automatically generated codes are as follows:
        [ 1]: @collect_memory
        [ 2]: def full(*size, fill_value=None, out=None, dtype=None, layout=torch.strided, device=_device.main_device, requires_grad=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:         ***
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in []:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     # Obtain available sized in arguments (which will be fed into size function).
        [14]:
        [15]:     # Use the given inner codes if they are provided.
        [16]:     torch_returned = False
        [17]:     if len(size) == 2 and isinstance(size[0], tuple):  size, fill_value = size[0]
        [18]:     elif len(size) == 1 and isinstance(size[0], tuple):  size = size[0]
        [19]:     if not isinstance(size, Size):  size = Size(size)
        [20]:     if fill_value is None:  fill_value = 0
        [21]:     with torch._C.DisableTorchFunction():
        [22]:         res = torch.full(size, fill_value=fill_value, out=out, dtype=dtype, layout=layout, device=device, requires_grad=requires_grad)
        [23]:     obj = res.as_subclass(Tensor).special_from(size)
        [24]:
        [25]:     def update_special(obj, updates):
        [26]:         if isinstance(obj, tuple):
        [27]:             for x in obj: update_special(x, updates)
        [28]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [29]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [30]:
        [31]:     if getattr(obj, 'grad_fn', None) is not None:
        [32]:         obj.grad_fn_name = "full"
        [33]:     return obj
        [34]:
        The document of the original function is:

        full(size, fill_value, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor

        Creates a tensor of size :attr:`size` filled with :attr:`fill_value`. The
        tensor's dtype is inferred from :attr:`fill_value`.

        Args:
            size (int...): a list, tuple, or :class:`torch.Size` of integers defining the
                shape of the output tensor.
            fill_value (Scalar): the value to fill the output tensor with.

        Keyword args:
            out (Tensor, optional): the output tensor.
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).
            layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
                Default: ``torch.strided``.
            device (:class:`torch.device`, optional): the desired device of returned tensor.
                Default: if ``None``, uses the current device for the default tensor type
                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU
                for CPU tensor types and the current CUDA device for CUDA tensor types.
            requires_grad (bool, optional): If autograd should record operations on the
                returned tensor. Default: ``False``.

        Example::

            >>> torch.full((2, 3), 3.141592)
            tensor([[ 3.1416,  3.1416,  3.1416],
                    [ 3.1416,  3.1416,  3.1416]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    # Obtain available sized in arguments (which will be fed into size function).

    # Use the given inner codes if they are provided.
    torch_returned = False
    if len(size) == 2 and isinstance(size[0], tuple):  size, fill_value = size[0]
    elif len(size) == 1 and isinstance(size[0], tuple):  size = size[0]
    if not isinstance(size, Size):  size = Size(size)
    if fill_value is None:  fill_value = 0
    with torch._C.DisableTorchFunction():
        res = torch.full(size, fill_value=fill_value, out=out, dtype=dtype, layout=layout, device=device, requires_grad=requires_grad)
    obj = res.as_subclass(Tensor).special_from(size)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "full"
    return obj

@collect_memory
def ones(*size: Size, out=None, dtype=None, layout=torch.strided, device=_device.main_device, requires_grad=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.ones'. The automatically generated codes are as follows:
        [ 1]: @collect_memory
        [ 2]: def ones(*size: Size, out=None, dtype=None, layout=torch.strided, device=_device.main_device, requires_grad=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:         ***
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in []:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     # Obtain available sized in arguments (which will be fed into size function).
        [14]:     size=Size(*size)
        [15]:     # Use the given inner codes if they are provided.
        [16]:     with torch._C.DisableTorchFunction():
        [17]:         auto_generated_args = tuple(x for x in [] if x is not None)
        [18]:         obj = torch.ones(*auto_generated_args, *size,out=out,dtype=dtype,layout=layout,device=device,requires_grad=requires_grad)
        [19]:     ref_shape = size_mapping['ones'](size)
        [20]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [21]:
        [22]:     def update_special(obj, updates):
        [23]:         if isinstance(obj, tuple):
        [24]:             for x in obj: update_special(x, updates)
        [25]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [26]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [27]:
        [28]:     if getattr(obj, 'grad_fn', None) is not None:
        [29]:         obj.grad_fn_name = "ones"
        [30]:     return obj
        [31]:
        The document of the original function is:

        ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor

        Returns a tensor filled with the scalar value `1`, with the shape defined
        by the variable argument :attr:`size`.

        Args:
            size (int...): a sequence of integers defining the shape of the output tensor.
                Can be a variable number of arguments or a collection like a list or tuple.

        Keyword arguments:
            out (Tensor, optional): the output tensor.
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).
            layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
                Default: ``torch.strided``.
            device (:class:`torch.device`, optional): the desired device of returned tensor.
                Default: if ``None``, uses the current device for the default tensor type
                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU
                for CPU tensor types and the current CUDA device for CUDA tensor types.
            requires_grad (bool, optional): If autograd should record operations on the
                returned tensor. Default: ``False``.

        Example::

            >>> torch.ones(2, 3)
            tensor([[ 1.,  1.,  1.],
                    [ 1.,  1.,  1.]])

            >>> torch.ones(5)
            tensor([ 1.,  1.,  1.,  1.,  1.])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    # Obtain available sized in arguments (which will be fed into size function).
    size=Size(*size)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [] if x is not None)
        obj = torch.ones(*auto_generated_args, *size,out=out,dtype=dtype,layout=layout,device=device,requires_grad=requires_grad)
    ref_shape = size_mapping['ones'](size)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "ones"
    return obj

@collect_memory
def zeros(*size: Size, out=None, dtype=None, layout=torch.strided, device=_device.main_device, requires_grad=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.zeros'. The automatically generated codes are as follows:
        [ 1]: @collect_memory
        [ 2]: def zeros(*size: Size, out=None, dtype=None, layout=torch.strided, device=_device.main_device, requires_grad=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:         ***
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in []:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     # Obtain available sized in arguments (which will be fed into size function).
        [14]:     size=Size(*size)
        [15]:     # Use the given inner codes if they are provided.
        [16]:     with torch._C.DisableTorchFunction():
        [17]:         auto_generated_args = tuple(x for x in [] if x is not None)
        [18]:         obj = torch.zeros(*auto_generated_args, *size,out=out,dtype=dtype,layout=layout,device=device,requires_grad=requires_grad)
        [19]:     ref_shape = size_mapping['zeros'](size)
        [20]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [21]:
        [22]:     def update_special(obj, updates):
        [23]:         if isinstance(obj, tuple):
        [24]:             for x in obj: update_special(x, updates)
        [25]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [26]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [27]:
        [28]:     if getattr(obj, 'grad_fn', None) is not None:
        [29]:         obj.grad_fn_name = "zeros"
        [30]:     return obj
        [31]:
        The document of the original function is:

        zeros(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor

        Returns a tensor filled with the scalar value `0`, with the shape defined
        by the variable argument :attr:`size`.

        Args:
            size (int...): a sequence of integers defining the shape of the output tensor.
                Can be a variable number of arguments or a collection like a list or tuple.

        Keyword args:
            out (Tensor, optional): the output tensor.
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).
            layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
                Default: ``torch.strided``.
            device (:class:`torch.device`, optional): the desired device of returned tensor.
                Default: if ``None``, uses the current device for the default tensor type
                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU
                for CPU tensor types and the current CUDA device for CUDA tensor types.
            requires_grad (bool, optional): If autograd should record operations on the
                returned tensor. Default: ``False``.

        Example::

            >>> torch.zeros(2, 3)
            tensor([[ 0.,  0.,  0.],
                    [ 0.,  0.,  0.]])

            >>> torch.zeros(5)
            tensor([ 0.,  0.,  0.,  0.,  0.])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    # Obtain available sized in arguments (which will be fed into size function).
    size=Size(*size)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [] if x is not None)
        obj = torch.zeros(*auto_generated_args, *size,out=out,dtype=dtype,layout=layout,device=device,requires_grad=requires_grad)
    ref_shape = size_mapping['zeros'](size)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "zeros"
    return obj

@collect_memory
def empty_like(input: 'Tensor', *, dtype=None, layout=None, device=_device.main_device, requires_grad=False, memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.empty_like'. The automatically generated codes are as follows:
        [ 1]: @collect_memory
        [ 2]: def empty_like(input: 'Tensor', *, dtype=None, layout=None, device=_device.main_device, requires_grad=False, memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:         ***
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in [input]:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     if input is None: ...
        [14]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [15]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [17]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [18]:
        [19]:     # Obtain available sized in arguments (which will be fed into size function).
        [20]:     input_shape=None if input is None else Size(input.shape)
        [21]:     # Use the given inner codes if they are provided.
        [22]:     with torch._C.DisableTorchFunction():
        [23]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [24]:         obj = torch.empty_like(*auto_generated_args, dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,memory_format=memory_format)
        [25]:     ref_shape = size_mapping['empty_like'](input_shape)
        [26]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [27]:
        [28]:     def update_special(obj, updates):
        [29]:         if isinstance(obj, tuple):
        [30]:             for x in obj: update_special(x, updates)
        [31]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [32]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [33]:
        [34]:     if getattr(obj, 'grad_fn', None) is not None:
        [35]:         obj.grad_fn_name = "empty_like"
        [36]:     return obj
        [37]:
        The document of the original function is:

        empty_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor

        Returns an uninitialized tensor with the same size as :attr:`input`.
        ``torch.empty_like(input)`` is equivalent to
        ``torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``.

        Args:
            input (Tensor): the size of :attr:`input` will determine size of the output tensor.

        Keyword args:
            dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor.
                Default: if ``None``, defaults to the dtype of :attr:`input`.
            layout (:class:`torch.layout`, optional): the desired layout of returned tensor.
                Default: if ``None``, defaults to the layout of :attr:`input`.
            device (:class:`torch.device`, optional): the desired device of returned tensor.
                Default: if ``None``, defaults to the device of :attr:`input`.
            requires_grad (bool, optional): If autograd should record operations on the
                returned tensor. Default: ``False``.
            memory_format (:class:`torch.memory_format`, optional): the desired memory format of
                returned Tensor. Default: ``torch.preserve_format``.

        Example::

            >>> a=torch.empty((2,3), dtype=torch.int32, device = 'cuda')
            >>> torch.empty_like(a)
            tensor([[0, 0, 0],
                    [0, 0, 0]], device='cuda:0', dtype=torch.int32)

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.empty_like(*auto_generated_args, dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,memory_format=memory_format)
    ref_shape = size_mapping['empty_like'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "empty_like"
    return obj

@collect_memory
def full_like(input: 'Tensor', fill_value=0, *, dtype=None, layout=torch.strided, device=_device.main_device, requires_grad=False, memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.full_like'. The automatically generated codes are as follows:
        [ 1]: @collect_memory
        [ 2]: def full_like(input: 'Tensor', fill_value=0, *, dtype=None, layout=torch.strided, device=_device.main_device, requires_grad=False, memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:         ***
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in [input]:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     if input is None: ...
        [14]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [15]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [17]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [18]:
        [19]:     # Obtain available sized in arguments (which will be fed into size function).
        [20]:     input_shape=None if input is None else Size(input.shape)
        [21]:     # Use the given inner codes if they are provided.
        [22]:     with torch._C.DisableTorchFunction():
        [23]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [24]:         obj = torch.full_like(*auto_generated_args, fill_value=fill_value,dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,memory_format=memory_format)
        [25]:     ref_shape = size_mapping['full_like'](input_shape)
        [26]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [27]:
        [28]:     def update_special(obj, updates):
        [29]:         if isinstance(obj, tuple):
        [30]:             for x in obj: update_special(x, updates)
        [31]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [32]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [33]:
        [34]:     if getattr(obj, 'grad_fn', None) is not None:
        [35]:         obj.grad_fn_name = "full_like"
        [36]:     return obj
        [37]:
        The document of the original function is:

        full_like(input, fill_value, \*, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor

        Returns a tensor with the same size as :attr:`input` filled with :attr:`fill_value`.
        ``torch.full_like(input, fill_value)`` is equivalent to
        ``torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device)``.

        Args:
            input (Tensor): the size of :attr:`input` will determine size of the output tensor.
            fill_value: the number to fill the output tensor with.

        Keyword args:
            dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor.
                Default: if ``None``, defaults to the dtype of :attr:`input`.
            layout (:class:`torch.layout`, optional): the desired layout of returned tensor.
                Default: if ``None``, defaults to the layout of :attr:`input`.
            device (:class:`torch.device`, optional): the desired device of returned tensor.
                Default: if ``None``, defaults to the device of :attr:`input`.
            requires_grad (bool, optional): If autograd should record operations on the
                returned tensor. Default: ``False``.
            memory_format (:class:`torch.memory_format`, optional): the desired memory format of
                returned Tensor. Default: ``torch.preserve_format``.

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.full_like(*auto_generated_args, fill_value=fill_value,dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,memory_format=memory_format)
    ref_shape = size_mapping['full_like'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "full_like"
    return obj

@collect_memory
def ones_like(input: 'Tensor', *, dtype=None, layout=None, device=_device.main_device, requires_grad=False, memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.ones_like'. The automatically generated codes are as follows:
        [ 1]: @collect_memory
        [ 2]: def ones_like(input: 'Tensor', *, dtype=None, layout=None, device=_device.main_device, requires_grad=False, memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:         ***
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in [input]:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     if input is None: ...
        [14]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [15]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [17]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [18]:
        [19]:     # Obtain available sized in arguments (which will be fed into size function).
        [20]:     input_shape=None if input is None else Size(input.shape)
        [21]:     # Use the given inner codes if they are provided.
        [22]:     with torch._C.DisableTorchFunction():
        [23]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [24]:         obj = torch.ones_like(*auto_generated_args, dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,memory_format=memory_format)
        [25]:     ref_shape = size_mapping['ones_like'](input_shape)
        [26]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [27]:
        [28]:     def update_special(obj, updates):
        [29]:         if isinstance(obj, tuple):
        [30]:             for x in obj: update_special(x, updates)
        [31]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [32]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [33]:
        [34]:     if getattr(obj, 'grad_fn', None) is not None:
        [35]:         obj.grad_fn_name = "ones_like"
        [36]:     return obj
        [37]:
        The document of the original function is:

        ones_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor

        Returns a tensor filled with the scalar value `1`, with the same size as
        :attr:`input`. ``torch.ones_like(input)`` is equivalent to
        ``torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``.

        .. warning::
            As of 0.4, this function does not support an :attr:`out` keyword. As an alternative,
            the old ``torch.ones_like(input, out=output)`` is equivalent to
            ``torch.ones(input.size(), out=output)``.

        Args:
            input (Tensor): the size of :attr:`input` will determine size of the output tensor.

        Keyword arguments:
            dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor.
                Default: if ``None``, defaults to the dtype of :attr:`input`.
            layout (:class:`torch.layout`, optional): the desired layout of returned tensor.
                Default: if ``None``, defaults to the layout of :attr:`input`.
            device (:class:`torch.device`, optional): the desired device of returned tensor.
                Default: if ``None``, defaults to the device of :attr:`input`.
            requires_grad (bool, optional): If autograd should record operations on the
                returned tensor. Default: ``False``.
            memory_format (:class:`torch.memory_format`, optional): the desired memory format of
                returned Tensor. Default: ``torch.preserve_format``.

        Example::

            >>> input = torch.empty(2, 3)
            >>> torch.ones_like(input)
            tensor([[ 1.,  1.,  1.],
                    [ 1.,  1.,  1.]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.ones_like(*auto_generated_args, dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,memory_format=memory_format)
    ref_shape = size_mapping['ones_like'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "ones_like"
    return obj

@collect_memory
def zeros_like(input: 'Tensor', *, dtype=None, layout=None, device=_device.main_device, requires_grad=False, memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.zeros_like'. The automatically generated codes are as follows:
        [ 1]: @collect_memory
        [ 2]: def zeros_like(input: 'Tensor', *, dtype=None, layout=None, device=_device.main_device, requires_grad=False, memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:         ***
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in [input]:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     if input is None: ...
        [14]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [15]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [17]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [18]:
        [19]:     # Obtain available sized in arguments (which will be fed into size function).
        [20]:     input_shape=None if input is None else Size(input.shape)
        [21]:     # Use the given inner codes if they are provided.
        [22]:     with torch._C.DisableTorchFunction():
        [23]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [24]:         obj = torch.zeros_like(*auto_generated_args, dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,memory_format=memory_format)
        [25]:     ref_shape = size_mapping['zeros_like'](input_shape)
        [26]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [27]:
        [28]:     def update_special(obj, updates):
        [29]:         if isinstance(obj, tuple):
        [30]:             for x in obj: update_special(x, updates)
        [31]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [32]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [33]:
        [34]:     if getattr(obj, 'grad_fn', None) is not None:
        [35]:         obj.grad_fn_name = "zeros_like"
        [36]:     return obj
        [37]:
        The document of the original function is:

        zeros_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor

        Returns a tensor filled with the scalar value `0`, with the same size as
        :attr:`input`. ``torch.zeros_like(input)`` is equivalent to
        ``torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``.

        .. warning::
            As of 0.4, this function does not support an :attr:`out` keyword. As an alternative,
            the old ``torch.zeros_like(input, out=output)`` is equivalent to
            ``torch.zeros(input.size(), out=output)``.

        Args:
            input (Tensor): the size of :attr:`input` will determine size of the output tensor.

        Keyword args:
            dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor.
                Default: if ``None``, defaults to the dtype of :attr:`input`.
            layout (:class:`torch.layout`, optional): the desired layout of returned tensor.
                Default: if ``None``, defaults to the layout of :attr:`input`.
            device (:class:`torch.device`, optional): the desired device of returned tensor.
                Default: if ``None``, defaults to the device of :attr:`input`.
            requires_grad (bool, optional): If autograd should record operations on the
                returned tensor. Default: ``False``.
            memory_format (:class:`torch.memory_format`, optional): the desired memory format of
                returned Tensor. Default: ``torch.preserve_format``.

        Example::

            >>> input = torch.empty(2, 3)
            >>> torch.zeros_like(input)
            tensor([[ 0.,  0.,  0.],
                    [ 0.,  0.,  0.]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.zeros_like(*auto_generated_args, dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,memory_format=memory_format)
    ref_shape = size_mapping['zeros_like'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "zeros_like"
    return obj

@collect_memory
def rand(*size: Size, generator=None, out=None, dtype=None, layout=torch.strided, device=_device.main_device, requires_grad=False, pin_memory=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.rand'. The automatically generated codes are as follows:
        [ 1]: @collect_memory
        [ 2]: def rand(*size: Size, generator=None, out=None, dtype=None, layout=torch.strided, device=_device.main_device, requires_grad=False, pin_memory=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:         ***
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in []:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     # Obtain available sized in arguments (which will be fed into size function).
        [14]:     size=Size(*size)
        [15]:     # Use the given inner codes if they are provided.
        [16]:     with torch._C.DisableTorchFunction():
        [17]:         auto_generated_args = tuple(x for x in [] if x is not None)
        [18]:         obj = torch.rand(*auto_generated_args, *size,generator=generator,out=out,dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,pin_memory=pin_memory)
        [19]:     ref_shape = size_mapping['rand'](size)
        [20]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [21]:
        [22]:     def update_special(obj, updates):
        [23]:         if isinstance(obj, tuple):
        [24]:             for x in obj: update_special(x, updates)
        [25]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [26]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [27]:
        [28]:     if getattr(obj, 'grad_fn', None) is not None:
        [29]:         obj.grad_fn_name = "rand"
        [30]:     return obj
        [31]:
        The document of the original function is:

        rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -> Tensor

        Returns a tensor filled with random numbers from a uniform distribution
        on the interval :math:`[0, 1)`

        The shape of the tensor is defined by the variable argument :attr:`size`.

        Args:
            size (int...): a sequence of integers defining the shape of the output tensor.
                Can be a variable number of arguments or a collection like a list or tuple.

        Keyword args:
            generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling
            out (Tensor, optional): the output tensor.
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).
            layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
                Default: ``torch.strided``.
            device (:class:`torch.device`, optional): the desired device of returned tensor.
                Default: if ``None``, uses the current device for the default tensor type
                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU
                for CPU tensor types and the current CUDA device for CUDA tensor types.
            requires_grad (bool, optional): If autograd should record operations on the
                returned tensor. Default: ``False``.
            pin_memory (bool, optional): If set, returned tensor would be allocated in
                the pinned memory. Works only for CPU tensors. Default: ``False``.

        Example::

            >>> torch.rand(4)
            tensor([ 0.5204,  0.2503,  0.3525,  0.5673])
            >>> torch.rand(2, 3)
            tensor([[ 0.8237,  0.5781,  0.6879],
                    [ 0.3816,  0.7249,  0.0998]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    # Obtain available sized in arguments (which will be fed into size function).
    size=Size(*size)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [] if x is not None)
        obj = torch.rand(*auto_generated_args, *size,generator=generator,out=out,dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,pin_memory=pin_memory)
    ref_shape = size_mapping['rand'](size)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "rand"
    return obj

@collect_memory
def randn(*size: Size, out=None, dtype=None, layout=torch.strided, device=_device.main_device, requires_grad=False, pin_memory=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.randn'. The automatically generated codes are as follows:
        [ 1]: @collect_memory
        [ 2]: def randn(*size: Size, out=None, dtype=None, layout=torch.strided, device=_device.main_device, requires_grad=False, pin_memory=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:         ***
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in []:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     # Obtain available sized in arguments (which will be fed into size function).
        [14]:     size=Size(*size)
        [15]:     # Use the given inner codes if they are provided.
        [16]:     with torch._C.DisableTorchFunction():
        [17]:         auto_generated_args = tuple(x for x in [] if x is not None)
        [18]:         obj = torch.randn(*auto_generated_args, *size,out=out,dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,pin_memory=pin_memory)
        [19]:     ref_shape = size_mapping['randn'](size)
        [20]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [21]:
        [22]:     def update_special(obj, updates):
        [23]:         if isinstance(obj, tuple):
        [24]:             for x in obj: update_special(x, updates)
        [25]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [26]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [27]:
        [28]:     if getattr(obj, 'grad_fn', None) is not None:
        [29]:         obj.grad_fn_name = "randn"
        [30]:     return obj
        [31]:
        The document of the original function is:

        randn(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -> Tensor

        Returns a tensor filled with random numbers from a normal distribution
        with mean `0` and variance `1` (also called the standard normal
        distribution).

        .. math::
            \text{out}_{i} \sim \mathcal{N}(0, 1)

        The shape of the tensor is defined by the variable argument :attr:`size`.

        Args:
            size (int...): a sequence of integers defining the shape of the output tensor.
                Can be a variable number of arguments or a collection like a list or tuple.

        Keyword args:
            generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling
            out (Tensor, optional): the output tensor.
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).
            layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
                Default: ``torch.strided``.
            device (:class:`torch.device`, optional): the desired device of returned tensor.
                Default: if ``None``, uses the current device for the default tensor type
                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU
                for CPU tensor types and the current CUDA device for CUDA tensor types.
            requires_grad (bool, optional): If autograd should record operations on the
                returned tensor. Default: ``False``.
            pin_memory (bool, optional): If set, returned tensor would be allocated in
                the pinned memory. Works only for CPU tensors. Default: ``False``.

        Example::

            >>> torch.randn(4)
            tensor([-2.1436,  0.9966,  2.3426, -0.6366])
            >>> torch.randn(2, 3)
            tensor([[ 1.5954,  2.8929, -1.0923],
                    [ 1.1719, -0.4709, -0.1996]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    # Obtain available sized in arguments (which will be fed into size function).
    size=Size(*size)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [] if x is not None)
        obj = torch.randn(*auto_generated_args, *size,out=out,dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,pin_memory=pin_memory)
    ref_shape = size_mapping['randn'](size)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "randn"
    return obj

@collect_memory
def rand_like(input: 'Tensor', *, dtype=None, layout=None, device=_device.main_device, requires_grad=False, memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.rand_like'. The automatically generated codes are as follows:
        [ 1]: @collect_memory
        [ 2]: def rand_like(input: 'Tensor', *, dtype=None, layout=None, device=_device.main_device, requires_grad=False, memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:         ***
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in [input]:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     if input is None: ...
        [14]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [15]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [17]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [18]:
        [19]:     # Obtain available sized in arguments (which will be fed into size function).
        [20]:     input_shape=None if input is None else Size(input.shape)
        [21]:     # Use the given inner codes if they are provided.
        [22]:     with torch._C.DisableTorchFunction():
        [23]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [24]:         obj = torch.rand_like(*auto_generated_args, dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,memory_format=memory_format)
        [25]:     ref_shape = size_mapping['rand_like'](input_shape)
        [26]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [27]:
        [28]:     def update_special(obj, updates):
        [29]:         if isinstance(obj, tuple):
        [30]:             for x in obj: update_special(x, updates)
        [31]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [32]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [33]:
        [34]:     if getattr(obj, 'grad_fn', None) is not None:
        [35]:         obj.grad_fn_name = "rand_like"
        [36]:     return obj
        [37]:
        The document of the original function is:

        rand_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor

        Returns a tensor with the same size as :attr:`input` that is filled with
        random numbers from a uniform distribution on the interval :math:`[0, 1)`.
        ``torch.rand_like(input)`` is equivalent to
        ``torch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``.

        Args:
            input (Tensor): the size of :attr:`input` will determine size of the output tensor.

        Keyword args:
            dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor.
                Default: if ``None``, defaults to the dtype of :attr:`input`.
            layout (:class:`torch.layout`, optional): the desired layout of returned tensor.
                Default: if ``None``, defaults to the layout of :attr:`input`.
            device (:class:`torch.device`, optional): the desired device of returned tensor.
                Default: if ``None``, defaults to the device of :attr:`input`.
            requires_grad (bool, optional): If autograd should record operations on the
                returned tensor. Default: ``False``.
            memory_format (:class:`torch.memory_format`, optional): the desired memory format of
                returned Tensor. Default: ``torch.preserve_format``.

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.rand_like(*auto_generated_args, dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,memory_format=memory_format)
    ref_shape = size_mapping['rand_like'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "rand_like"
    return obj

@collect_memory
def randn_like(input: 'Tensor', *, dtype=None, layout=None, device=_device.main_device, requires_grad=False, memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.randn_like'. The automatically generated codes are as follows:
        [ 1]: @collect_memory
        [ 2]: def randn_like(input: 'Tensor', *, dtype=None, layout=None, device=_device.main_device, requires_grad=False, memory_format=torch.preserve_format, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:         ***
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in [input]:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     if input is None: ...
        [14]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [15]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [16]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [17]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [18]:
        [19]:     # Obtain available sized in arguments (which will be fed into size function).
        [20]:     input_shape=None if input is None else Size(input.shape)
        [21]:     # Use the given inner codes if they are provided.
        [22]:     with torch._C.DisableTorchFunction():
        [23]:         auto_generated_args = tuple(x for x in [input] if x is not None)
        [24]:         obj = torch.randn_like(*auto_generated_args, dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,memory_format=memory_format)
        [25]:     ref_shape = size_mapping['randn_like'](input_shape)
        [26]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [27]:
        [28]:     def update_special(obj, updates):
        [29]:         if isinstance(obj, tuple):
        [30]:             for x in obj: update_special(x, updates)
        [31]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [32]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [33]:
        [34]:     if getattr(obj, 'grad_fn', None) is not None:
        [35]:         obj.grad_fn_name = "randn_like"
        [36]:     return obj
        [37]:
        The document of the original function is:

        randn_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) -> Tensor

        Returns a tensor with the same size as :attr:`input` that is filled with
        random numbers from a normal distribution with mean 0 and variance 1.
        ``torch.randn_like(input)`` is equivalent to
        ``torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)``.

        Args:
            input (Tensor): the size of :attr:`input` will determine size of the output tensor.

        Keyword args:
            dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor.
                Default: if ``None``, defaults to the dtype of :attr:`input`.
            layout (:class:`torch.layout`, optional): the desired layout of returned tensor.
                Default: if ``None``, defaults to the layout of :attr:`input`.
            device (:class:`torch.device`, optional): the desired device of returned tensor.
                Default: if ``None``, defaults to the device of :attr:`input`.
            requires_grad (bool, optional): If autograd should record operations on the
                returned tensor. Default: ``False``.
            memory_format (:class:`torch.memory_format`, optional): the desired memory format of
                returned Tensor. Default: ``torch.preserve_format``.

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [input] if x is not None)
        obj = torch.randn_like(*auto_generated_args, dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,memory_format=memory_format)
    ref_shape = size_mapping['randn_like'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "randn_like"
    return obj

@collect_memory
def randperm(*n: Size, generator=None, out=None, dtype=torch.int64,layout=torch.strided, device=_device.main_device, requires_grad=False, pin_memory=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.randperm'. The automatically generated codes are as follows:
        [ 1]: @collect_memory
        [ 2]: def randperm(*n: Size, generator=None, out=None, dtype=torch.int64,layout=torch.strided, device=_device.main_device, requires_grad=False, pin_memory=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:         ***
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in []:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     # Obtain available sized in arguments (which will be fed into size function).
        [14]:     n=Size(*n)
        [15]:     # Use the given inner codes if they are provided.
        [16]:     torch_returned = False
        [17]:     avouch(n.n_space_dim == 1, TypeError("'torch.randperm' only accepts 1 space dimension for permutation. "))
        [18]:     n_batch = (n.n_batch if n.has_batch else 1) * (n.n_feature if n.has_feature else 1) * (n.n_sequence if n.has_sequence else 1)
        [19]:     with torch._C.DisableTorchFunction():
        [20]:         result = stack([torch.randperm(*n.space,generator=generator,out=out,dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,pin_memory=pin_memory).as_subclass(Tensor).init_special() for _ in range_(n_batch)], {})
        [21]:     if n_batch == 0:  result = zeros({0}, n.space[0]).long()
        [22]:     obj = result.split_dim({}, n.with_space()).move_dim(-1, n.space_start)
        [23]:     ref_shape = size_mapping['randperm'](n)
        [24]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [25]:
        [26]:     def update_special(obj, updates):
        [27]:         if isinstance(obj, tuple):
        [28]:             for x in obj: update_special(x, updates)
        [29]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [30]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [31]:
        [32]:     if getattr(obj, 'grad_fn', None) is not None:
        [33]:         obj.grad_fn_name = "randperm"
        [34]:     return obj
        [35]:
        The document of the original function is:

        randperm(n, *, generator=None, out=None, dtype=torch.int64,layout=torch.strided, device=None, requires_grad=False, pin_memory=False) -> Tensor

        Returns a random permutation of integers from ``0`` to ``n - 1``.

        Args:
            n (int): the upper bound (exclusive)

        Keyword args:
            generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling
            out (Tensor, optional): the output tensor.
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                Default: ``torch.int64``.
            layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
                Default: ``torch.strided``.
            device (:class:`torch.device`, optional): the desired device of returned tensor.
                Default: if ``None``, uses the current device for the default tensor type
                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU
                for CPU tensor types and the current CUDA device for CUDA tensor types.
            requires_grad (bool, optional): If autograd should record operations on the
                returned tensor. Default: ``False``.
            pin_memory (bool, optional): If set, returned tensor would be allocated in
                the pinned memory. Works only for CPU tensors. Default: ``False``.

        Example::

            >>> torch.randperm(4)
            tensor([2, 1, 0, 3])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    # Obtain available sized in arguments (which will be fed into size function).
    n=Size(*n)
    # Use the given inner codes if they are provided.
    torch_returned = False
    avouch(n.n_space_dim == 1, TypeError("'torch.randperm' only accepts 1 space dimension for permutation. "))
    n_batch = (n.n_batch if n.has_batch else 1) * (n.n_feature if n.has_feature else 1) * (n.n_sequence if n.has_sequence else 1)
    with torch._C.DisableTorchFunction():
        result = stack([torch.randperm(*n.space,generator=generator,out=out,dtype=dtype,layout=layout,device=device,requires_grad=requires_grad,pin_memory=pin_memory).as_subclass(Tensor).init_special() for _ in range_(n_batch)], {})
    if n_batch == 0:  result = zeros({0}, n.space[0]).long()
    obj = result.split_dim({}, n.with_space()).move_dim(-1, n.space_start)
    ref_shape = size_mapping['randperm'](n)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "randperm"
    return obj

@collect_memory
def arange(*start_stop_step, out=None, dtype=None, layout=torch.strided, device=_device.main_device, requires_grad=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.arange'. The automatically generated codes are as follows:
        [ 1]: @collect_memory
        [ 2]: def arange(*start_stop_step, out=None, dtype=None, layout=torch.strided, device=_device.main_device, requires_grad=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:         ***
        [ 5]:     '''
        [ 6]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 7]:
        [ 8]:     pivot = None
        [ 9]:     for t in []:
        [10]:         if isinstance(t, torch.Tensor): pivot = t; break
        [11]:     subclass = Tensor.get_tensor_subclass(pivot)
        [12]:
        [13]:     # Obtain available sized in arguments (which will be fed into size function).
        [14]:
        [15]:     # Use the given inner codes if they are provided.
        [16]:     torch_returned = False
        [17]:     start_stop_step = Size(*start_stop_step)
        [18]:     with torch._C.DisableTorchFunction():
        [19]:         obj = torch.arange(*start_stop_step,out=out,dtype=dtype,layout=layout,device=device,requires_grad=requires_grad)
        [20]:     obj = obj.as_subclass(Tensor).special_from(start_stop_step[:1]) # suppress:  special_from
        [21]:
        [22]:     def update_special(obj, updates):
        [23]:         if isinstance(obj, tuple):
        [24]:             for x in obj: update_special(x, updates)
        [25]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [26]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [27]:
        [28]:     if getattr(obj, 'grad_fn', None) is not None:
        [29]:         obj.grad_fn_name = "arange"
        [30]:     return obj
        [31]:
        The document of the original function is:

        arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor

        Returns a 1-D tensor of size :math:`\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil`
        with values from the interval ``[start, end)`` taken with common difference
        :attr:`step` beginning from `start`.

        Note that non-integer :attr:`step` is subject to floating point rounding errors when
        comparing against :attr:`end`; to avoid inconsistency, we advise adding a small epsilon to :attr:`end`
        in such cases.

        .. math::
            \text{out}_{{i+1}} = \text{out}_{i} + \text{step}

        Args:
            start (Number): the starting value for the set of points. Default: ``0``.
            end (Number): the ending value for the set of points
            step (Number): the gap between each pair of adjacent points. Default: ``1``.

        Keyword args:
            out (Tensor, optional): the output tensor.
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`). If `dtype` is not given, infer the data type from the other input
                arguments. If any of `start`, `end`, or `stop` are floating-point, the
                `dtype` is inferred to be the default dtype, see
                :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to
                be `torch.int64`.
            layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
                Default: ``torch.strided``.
            device (:class:`torch.device`, optional): the desired device of returned tensor.
                Default: if ``None``, uses the current device for the default tensor type
                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU
                for CPU tensor types and the current CUDA device for CUDA tensor types.
            requires_grad (bool, optional): If autograd should record operations on the
                returned tensor. Default: ``False``.

        Example::

            >>> torch.arange(5)
            tensor([ 0,  1,  2,  3,  4])
            >>> torch.arange(1, 4)
            tensor([ 1,  2,  3])
            >>> torch.arange(1, 2.5, 0.5)
            tensor([ 1.0000,  1.5000,  2.0000])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    # Obtain available sized in arguments (which will be fed into size function).

    # Use the given inner codes if they are provided.
    torch_returned = False
    start_stop_step = Size(*start_stop_step)
    with torch._C.DisableTorchFunction():
        obj = torch.arange(*start_stop_step,out=out,dtype=dtype,layout=layout,device=device,requires_grad=requires_grad)
    obj = obj.as_subclass(Tensor).special_from(start_stop_step[:1]) # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "arange"
    return obj

def where(condition: 'Tensor', input: 'Tensor', other: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.where'. The automatically generated codes are as follows:
        [ 1]: def where(condition: 'Tensor', input: 'Tensor', other: 'Tensor', *, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in [condition, input, other]:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     if condition is None: ...
        [13]:     elif not isinstance(condition, torch.Tensor): condition = torch.tensor(condition)
        [14]:     if not isinstance(condition, subclass): condition = condition.as_subclass(subclass).special_from(condition.shape)
        [15]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [16]:         if condition.device.type == 'cpu': condition = condition.to(pivot.device)
        [17]:
        [18]:     if input is None: ...
        [19]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [20]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [21]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [22]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [23]:
        [24]:     if other is None: ...
        [25]:     elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
        [26]:     if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
        [27]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [28]:         if other.device.type == 'cpu': other = other.to(pivot.device)
        [29]:
        [30]:     # Obtain available sized in arguments (which will be fed into size function).
        [31]:     condition_shape=None if condition is None else Size(condition.shape)
        [32]:     input_shape=None if input is None else Size(input.shape)
        [33]:     other_shape=None if other is None else Size(other.shape)
        [34]:     # Use the given inner codes if they are provided.
        [35]:     ref_shape, condition_shape, input_shape, other_shape = size_mapping_op['where'](condition_shape, input_shape, other_shape)
        [36]:     condition = condition.view(condition_shape)
        [37]:     input = input.view(input_shape)
        [38]:     other = other.view(other_shape)
        [39]:     with torch._C.DisableTorchFunction():
        [40]:         auto_generated_args = tuple(x for x in [condition,input,other] if x is not None)
        [41]:         obj = torch.where(*auto_generated_args, out=out)
        [42]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [43]:
        [44]:     def update_special(obj, updates):
        [45]:         if isinstance(obj, tuple):
        [46]:             for x in obj: update_special(x, updates)
        [47]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [48]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [49]:
        [50]:     if getattr(obj, 'grad_fn', None) is not None:
        [51]:         obj.grad_fn_name = "where"
        [52]:     return obj
        [53]:
        The document of the original function is:

        where(condition, input, other, *, out=None) -> Tensor

        Return a tensor of elements selected from either :attr:`input` or :attr:`other`, depending on :attr:`condition`.

        The operation is defined as:

        .. math::
            \text{out}_i = \begin{cases}
                \text{input}_i & \text{if } \text{condition}_i \\
                \text{other}_i & \text{otherwise} \\
            \end{cases}

        .. note::
            The tensors :attr:`condition`, :attr:`input`, :attr:`other` must be :ref:`broadcastable <broadcasting-semantics>`.

        Arguments:
            condition (BoolTensor): When True (nonzero), yield input, otherwise yield other
            input (Tensor or Scalar): value (if :attr:`input` is a scalar) or values selected at indices
                                  where :attr:`condition` is ``True``
            other (Tensor or Scalar): value (if :attr:`other` is a scalar) or values selected at indices
                                  where :attr:`condition` is ``False``

        Keyword args:
            out (Tensor, optional): the output tensor.

        Returns:
            Tensor: A tensor of shape equal to the broadcasted shape of :attr:`condition`, :attr:`input`, :attr:`other`

        Example::

            >>> x = torch.randn(3, 2)
            >>> y = torch.ones(3, 2)
            >>> x
            tensor([[-0.4620,  0.3139],
                    [ 0.3898, -0.7197],
                    [ 0.0478, -0.1657]])
            >>> torch.where(x > 0, 1.0, 0.0)
            tensor([[0., 1.],
                    [1., 0.],
                    [1., 0.]])
            >>> torch.where(x > 0, x, y)
            tensor([[ 1.0000,  0.3139],
                    [ 0.3898,  1.0000],
                    [ 0.0478,  1.0000]])
            >>> x = torch.randn(2, 2, dtype=torch.double)
            >>> x
            tensor([[ 1.0779,  0.0383],
                    [-0.8785, -1.1089]], dtype=torch.float64)
            >>> torch.where(x > 0, x, 0.)
            tensor([[1.0779, 0.0383],
                    [0.0000, 0.0000]], dtype=torch.float64)

        .. function:: where(condition) -> tuple of LongTensor
           :noindex:

        ``torch.where(condition)`` is identical to
        ``torch.nonzero(condition, as_tuple=True)``.

        .. note::
            See also :func:`torch.nonzero`.

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [condition, input, other]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if condition is None: ...
    elif not isinstance(condition, torch.Tensor): condition = torch.tensor(condition)
    if not isinstance(condition, subclass): condition = condition.as_subclass(subclass).special_from(condition.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if condition.device.type == 'cpu': condition = condition.to(pivot.device)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    if other is None: ...
    elif not isinstance(other, torch.Tensor): other = torch.tensor(other)
    if not isinstance(other, subclass): other = other.as_subclass(subclass).special_from(other.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if other.device.type == 'cpu': other = other.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    condition_shape=None if condition is None else Size(condition.shape)
    input_shape=None if input is None else Size(input.shape)
    other_shape=None if other is None else Size(other.shape)
    # Use the given inner codes if they are provided.
    ref_shape, condition_shape, input_shape, other_shape = size_mapping_op['where'](condition_shape, input_shape, other_shape)
    condition = condition.view(condition_shape)
    input = input.view(input_shape)
    other = other.view(other_shape)
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [condition,input,other] if x is not None)
        obj = torch.where(*auto_generated_args, out=out)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "where"
    return obj

def reshape(self, *size: Size, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """

    Automatically inheritted method from 'torch.reshape'. The automatically generated codes are as follows:
        [ 1]: def reshape(self, *size: Size, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:         ***
        [ 4]:     '''
        [ 5]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [ 6]:
        [ 7]:     pivot = None
        [ 8]:     for t in []:
        [ 9]:         if isinstance(t, torch.Tensor): pivot = t; break
        [10]:     subclass = Tensor.get_tensor_subclass(pivot)
        [11]:
        [12]:     # Obtain available sized in arguments (which will be fed into size function).
        [13]:     size=Size(*size)
        [14]:     # Use the given inner codes if they are provided.
        [15]:     with torch._C.DisableTorchFunction():
        [16]:         auto_generated_args = tuple(x for x in [self] if x is not None)
        [17]:         obj = torch.reshape(*auto_generated_args, *size)
        [18]:     ref_shape = size_mapping['reshape'](size)
        [19]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [20]:
        [21]:     def update_special(obj, updates):
        [22]:         if isinstance(obj, tuple):
        [23]:             for x in obj: update_special(x, updates)
        [24]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [25]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [26]:
        [27]:     if getattr(obj, 'grad_fn', None) is not None:
        [28]:         obj.grad_fn_name = "reshape"
        [29]:     return obj
        [30]:
        The document of the original function is:

        reshape(input, shape) -> Tensor

        Returns a tensor with the same data and number of elements as :attr:`input`,
        but with the specified shape. When possible, the returned tensor will be a view
        of :attr:`input`. Otherwise, it will be a copy. Contiguous inputs and inputs
        with compatible strides can be reshaped without copying, but you should not
        depend on the copying vs. viewing behavior.

        See :meth:`torch.Tensor.view` on when it is possible to return a view.

        A single dimension may be -1, in which case it's inferred from the remaining
        dimensions and the number of elements in :attr:`input`.

        Args:
            input (Tensor): the tensor to be reshaped
            shape (tuple of int): the new shape

        Example::

            >>> a = torch.arange(4.)
            >>> torch.reshape(a, (2, 2))
            tensor([[ 0.,  1.],
                    [ 2.,  3.]])
            >>> b = torch.tensor([[0, 1], [2, 3]])
            >>> torch.reshape(b, (-1,))
            tensor([ 0,  1,  2,  3])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    # Obtain available sized in arguments (which will be fed into size function).
    size=Size(*size)
    # Use the given inner codes if they are provided.
    with torch._C.DisableTorchFunction():
        auto_generated_args = tuple(x for x in [self] if x is not None)
        obj = torch.reshape(*auto_generated_args, *size)
    ref_shape = size_mapping['reshape'](size)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "reshape"
    return obj

@alias('concatenate')
def cat(*tensors, dim=None, crop=False, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    Concatenate tensors along dimension `dim`.

    Args:
        dim (int/exist_dim, optional): The dimension for concatenation.
            Defaults to auto concatenation at
            (1) the first feature dimension if tensors have feature;
            (2) the batch dimension if tensors have batch;
            (3) the first sequence dimension if tensors have sequence;
            (4) the first spacial dimension otherwise.
        crop (bool): Whether to crop the sizes automatically when necessary.

    Automatically inheritted method from 'torch.cat'. The automatically generated codes are as follows:
        [ 1]: @alias('concatenate')
        [ 2]: def cat(*tensors, dim=None, crop=False, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:     Concatenate tensors along dimension `dim`.
        [ 5]:
        [ 6]:     Args:
        [ 7]:         dim (int/exist_dim, optional): The dimension for concatenation.
        [ 8]:             Defaults to auto concatenation at
        [ 9]:             (1) the first feature dimension if tensors have feature;
        [10]:             (2) the batch dimension if tensors have batch;
        [11]:             (3) the first sequence dimension if tensors have sequence;
        [12]:             (4) the first spacial dimension otherwise.
        [13]:         crop (bool): Whether to crop the sizes automatically when necessary.
        [14]:     '''
        [15]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [16]:
        [17]:     pivot = None
        [18]:     for t in []:
        [19]:         if isinstance(t, torch.Tensor): pivot = t; break
        [20]:     subclass = Tensor.get_tensor_subclass(pivot)
        [21]:
        [22]:     # Obtain available sized in arguments (which will be fed into size function).
        [23]:
        [24]:     # Use the given inner codes if they are provided.
        [25]:     torch_returned = False
        [26]:     from .tensorfunc import crop_as
        [27]:     if len(tensors) == 0: torch_returned = True; obj = tensor([])
        [28]:     elif len(tensors) == 1 and isinstance(tensors[0], (list, tuple)):  tensors = tuple(tensors[0])
        [29]:     elif len(tensors) == 2 and isinstance(tensors[0], (list, tuple)) and isinstance(tensors[1], exist_dim):
        [30]:         avouch(dim is None, TypeError(f"Cannot concatenate tensors by multiple dimensions."))
        [31]:         dim = tensors[1]
        [32]:         tensors = tuple(tensors[0])
        [33]:     elif len(tensors) > 2 and isinstance(tensors[-1], exist_dim):
        [34]:         avouch(dim is None, TypeError(f"Cannot concatenate tensors by multiple dimensions."))
        [35]:         dim = tensors[-1]
        [36]:         tensors = tuple(tensors[:-1])
        [37]:     if not torch_returned:
        [38]:         avouch(all_(isinstance(x, torch.Tensor) for x in tensors), TypeError(f"'bt.cat' can only concatenate torch.Tensor objects. "))
        [39]:         if len(tensors) == 0: torch_returned = True; obj = tensor([])
        [40]:         if not torch_returned:
        [41]:             pivot = tensors[argmax_([x.n_dim for x in tensors])[0]]
        [42]:             if dim is None:
        [43]:                 if not isinstance(pivot, Tensor):  dim = (0,)
        [44]:                 elif pivot.has_feature:  dim = exist_dim(pivot, [0])
        [45]:                 elif pivot.has_batch:  dim = exist_dim(pivot, {})
        [46]:                 elif pivot.has_sequence:  dim = exist_dim(pivot, '0')
        [47]:                 else:  dim = (0,)
        [48]:             else:  dim = exist_dim(pivot, dim)
        [49]:             avouch(len(dim) == 1, TypeError(f"Cannot concat tensors in dimensions {dim}, please flatten them first or use '[0]' and "'0'" to specify the first feature/sequence dimension."))
        [50]:             dim = dim[0]
        [51]:
        [52]:             if crop:  dims_allow_mismatch = (dim,) + tuple(range_(pivot.space_start, pivot.space_stop))
        [53]:             else:  dims_allow_mismatch = dim
        [54]:             try:  tensors = [x.expand_to(pivot, dims_allow_mismatch=dims_allow_mismatch) for x in tensors if x.n_ele > 0]
        [55]:             except TypeError as e:
        [56]:                 if "Cannot expand tensor" in str(e) or "Size mismatch in 'expand_to'" in str(e):
        [57]:                     raise TypeError("Tensors can only be concatenated when all of them have a same shape except for one dimension. " + f"Currently: {[x.shape for x in tensors]}")
        [58]:                 else:  raise e
        [59]:             if crop:  tensors = [x if x.shape[:dim] == pivot.shape[:dim] and x.shape[dim+1:] == pivot.shape[dim+1:] else crop_as(x, pivot.space) for x in tensors]
        [60]:
        [61]:             bt_tensors = [t for t in tensors if isinstance(t, Tensor)]
        [62]:             with torch._C.DisableTorchFunction():
        [63]:                 if len(bt_tensors) == 0: torch_returned = True; obj = torch.cat(tensors, dim, out=out).as_subclass(Tensor)
        [64]:                 else: torch_returned = True; obj = Tensor.inherit_from(torch.cat(tensors, dim, out=out), bt_tensors[0])
        [65]:
        [66]:     def update_special(obj, updates):
        [67]:         if isinstance(obj, tuple):
        [68]:             for x in obj: update_special(x, updates)
        [69]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [70]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [71]:
        [72]:     if getattr(obj, 'grad_fn', None) is not None:
        [73]:         obj.grad_fn_name = "cat"
        [74]:     return obj
        [75]:
        The document of the original function is:

        cat(tensors, dim=0, *, out=None) -> Tensor

        Concatenates the given sequence of :attr:`seq` tensors in the given dimension.
        All tensors must either have the same shape (except in the concatenating
        dimension) or be empty.

        :func:`torch.cat` can be seen as an inverse operation for :func:`torch.split`
        and :func:`torch.chunk`.

        :func:`torch.cat` can be best understood via examples.

        Args:
            tensors (sequence of Tensors): any python sequence of tensors of the same type.
                Non-empty tensors provided must have the same shape, except in the
                cat dimension.
            dim (int, optional): the dimension over which the tensors are concatenated

        Keyword args:
            out (Tensor, optional): the output tensor.

        Example::

            >>> x = torch.randn(2, 3)
            >>> x
            tensor([[ 0.6580, -1.0969, -0.4614],
                    [-0.1034, -0.5790,  0.1497]])
            >>> torch.cat((x, x, x), 0)
            tensor([[ 0.6580, -1.0969, -0.4614],
                    [-0.1034, -0.5790,  0.1497],
                    [ 0.6580, -1.0969, -0.4614],
                    [-0.1034, -0.5790,  0.1497],
                    [ 0.6580, -1.0969, -0.4614],
                    [-0.1034, -0.5790,  0.1497]])
            >>> torch.cat((x, x, x), 1)
            tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,
                     -1.0969, -0.4614],
                    [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,
                     -0.5790,  0.1497]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    # Obtain available sized in arguments (which will be fed into size function).

    # Use the given inner codes if they are provided.
    torch_returned = False
    from .tensorfunc import crop_as
    if len(tensors) == 0: torch_returned = True; obj = tensor([])
    elif len(tensors) == 1 and isinstance(tensors[0], (list, tuple)):  tensors = tuple(tensors[0])
    elif len(tensors) == 2 and isinstance(tensors[0], (list, tuple)) and isinstance(tensors[1], exist_dim):
        avouch(dim is None, TypeError(f"Cannot concatenate tensors by multiple dimensions."))
        dim = tensors[1]
        tensors = tuple(tensors[0])
    elif len(tensors) > 2 and isinstance(tensors[-1], exist_dim):
        avouch(dim is None, TypeError(f"Cannot concatenate tensors by multiple dimensions."))
        dim = tensors[-1]
        tensors = tuple(tensors[:-1])
    if not torch_returned:
        avouch(all_(isinstance(x, torch.Tensor) for x in tensors), TypeError(f"'bt.cat' can only concatenate torch.Tensor objects. "))
        if len(tensors) == 0: torch_returned = True; obj = tensor([])
        if not torch_returned:
            pivot = tensors[argmax_([x.n_dim for x in tensors])[0]]
            if dim is None:
                if not isinstance(pivot, Tensor):  dim = (0,)
                elif pivot.has_feature:  dim = exist_dim(pivot, [0])
                elif pivot.has_batch:  dim = exist_dim(pivot, {})
                elif pivot.has_sequence:  dim = exist_dim(pivot, '0')
                else:  dim = (0,)
            else:  dim = exist_dim(pivot, dim)
            avouch(len(dim) == 1, TypeError(f"Cannot concat tensors in dimensions {dim}, please flatten them first or use '[0]' and "'0'" to specify the first feature/sequence dimension."))
            dim = dim[0]

            if crop:  dims_allow_mismatch = (dim,) + tuple(range_(pivot.space_start, pivot.space_stop))
            else:  dims_allow_mismatch = dim
            try:  tensors = [x.expand_to(pivot, dims_allow_mismatch=dims_allow_mismatch) for x in tensors if x.n_ele > 0]
            except TypeError as e:
                if "Cannot expand tensor" in str(e) or "Size mismatch in 'expand_to'" in str(e):
                    raise TypeError("Tensors can only be concatenated when all of them have a same shape except for one dimension. " + f"Currently: {[x.shape for x in tensors]}")
                else:  raise e
            if crop:  tensors = [x if x.shape[:dim] == pivot.shape[:dim] and x.shape[dim+1:] == pivot.shape[dim+1:] else crop_as(x, pivot.space) for x in tensors]

            bt_tensors = [t for t in tensors if isinstance(t, Tensor)]
            with torch._C.DisableTorchFunction():
                if len(bt_tensors) == 0: torch_returned = True; obj = torch.cat(tensors, dim, out=out).as_subclass(Tensor)
                else: torch_returned = True; obj = Tensor.inherit_from(torch.cat(tensors, dim, out=out), bt_tensors[0])

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "cat"
    return obj

def stack(*tensors, dim=None, crop=False, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    Stack tensors along a new dimension `dim`.

    Args:
        dim (int/new_dim, optional): The dimension for stacking.
            Defaults to auto stack at
            (1) a new batch dimension if tensors have no batch;
            (2) a new feature dimension if tensors have batch dimension;
        crop (bool): Whether to crop the sizes automatically when necessary.

    Automatically inheritted method from 'torch.stack'. The automatically generated codes are as follows:
        [ 1]: def stack(*tensors, dim=None, crop=False, out=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:     Stack tensors along a new dimension `dim`.
        [ 4]:
        [ 5]:     Args:
        [ 6]:         dim (int/new_dim, optional): The dimension for stacking.
        [ 7]:             Defaults to auto stack at
        [ 8]:             (1) a new batch dimension if tensors have no batch;
        [ 9]:             (2) a new feature dimension if tensors have batch dimension;
        [10]:         crop (bool): Whether to crop the sizes automatically when necessary.
        [11]:     '''
        [12]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [13]:
        [14]:     pivot = None
        [15]:     for t in []:
        [16]:         if isinstance(t, torch.Tensor): pivot = t; break
        [17]:     subclass = Tensor.get_tensor_subclass(pivot)
        [18]:
        [19]:     # Obtain available sized in arguments (which will be fed into size function).
        [20]:
        [21]:     # Use the given inner codes if they are provided.
        [22]:     torch_returned = False
        [23]:     from .tensorfunc import crop_as
        [24]:     if len(tensors) == 0: torch_returned = True; obj = tensor([])
        [25]:     elif len(tensors) == 1 and isinstance(tensors[0], (list, tuple)):  tensors = tuple(tensors[0])
        [26]:     elif len(tensors) == 2 and isinstance(tensors[0], (list, tuple)) and isinstance(tensors[1], new_dim):
        [27]:         avouch(dim is None, TypeError(f"Cannot stack tensors by multiple dimensions."))
        [28]:         dim = tensors[1]
        [29]:         tensors = tuple(tensors[0])
        [30]:     elif len(tensors) > 2 and isinstance(tensors[-1], new_dim):
        [31]:         avouch(dim is None, TypeError(f"Cannot stack tensors by multiple dimensions."))
        [32]:         dim = tensors[-1]
        [33]:         tensors = tuple(tensors[:-1])
        [34]:     if not torch_returned:
        [35]:         avouch(all_(isinstance(x, torch.Tensor) for x in tensors), TypeError(f"'bt.stack' can only stack torch.Tensor objects. "))
        [36]:         if len(tensors) == 0: torch_returned = True; obj = tensor([])
        [37]:         if not torch_returned:
        [38]:             pivot = tensors[argmax_([x.n_dim for x in tensors])[0]]
        [39]:             if dim is None:
        [40]:                 if not isinstance(pivot, Tensor):  dim = new_dim(pivot, {})
        [41]:                 elif not pivot.has_batch:  dim = new_dim(pivot, {})
        [42]:                 else:  dim = new_dim(pivot, [pivot.non_bat_start])
        [43]:             else:  dim = new_dim(pivot, dim)
        [44]:             avouch(len(dim) == 1, TypeError(f"Cannot concat tensors in dimensions {dim}, please flatten them first or use '[0]' and "'0'" to specify the first feature/sequence dimension."))
        [45]:
        [46]:             if crop:  dims_allow_mismatch = tuple(range_(pivot.space_start, pivot.space_stop))
        [47]:             else:  dims_allow_mismatch = None
        [48]:             try:  tensors = [x.expand_to(pivot, dims_allow_mismatch=dims_allow_mismatch) for x in tensors if x.n_ele > 0]
        [49]:             except TypeError as e:
        [50]:                 if "Cannot expand tensor" in str(e) or "Size mismatch in 'expand_to'" in str(e):
        [51]:                     raise TypeError("Tensors can only be stacked when all of them have a same shape. " + f"Currently: {[x.shape for x in tensors]}")
        [52]:                 else:  raise e
        [53]:             if crop:  tensors = [x if x.shape == pivot.shape else crop_as(x, pivot.space) for x in tensors]
        [54]:
        [55]:             bt_tensors = [t for t in tensors if isinstance(t, Tensor)]
        [56]:             with torch._C.DisableTorchFunction():
        [57]:                 if len(bt_tensors) == 0: torch_returned = True; obj = torch.stack(tensors, dim[0], out=out).as_subclass(Tensor)
        [58]:                 else: torch_returned = True; obj = Tensor.inherit_from(torch.stack(tensors, dim[0], out=out), bt_tensors[0], shape=dim)
        [59]:
        [60]:     def update_special(obj, updates):
        [61]:         if isinstance(obj, tuple):
        [62]:             for x in obj: update_special(x, updates)
        [63]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [64]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [65]:
        [66]:     if getattr(obj, 'grad_fn', None) is not None:
        [67]:         obj.grad_fn_name = "stack"
        [68]:     return obj
        [69]:
        The document of the original function is:

        stack(tensors, dim=0, *, out=None) -> Tensor

        Concatenates a sequence of tensors along a new dimension.

        All tensors need to be of the same size.

        Arguments:
            tensors (sequence of Tensors): sequence of tensors to concatenate
            dim (int): dimension to insert. Has to be between 0 and the number
                of dimensions of concatenated tensors (inclusive)

        Keyword args:
            out (Tensor, optional): the output tensor.

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    # Obtain available sized in arguments (which will be fed into size function).

    # Use the given inner codes if they are provided.
    torch_returned = False
    from .tensorfunc import crop_as
    if len(tensors) == 0: torch_returned = True; obj = tensor([])
    elif len(tensors) == 1 and isinstance(tensors[0], (list, tuple)):  tensors = tuple(tensors[0])
    elif len(tensors) == 2 and isinstance(tensors[0], (list, tuple)) and isinstance(tensors[1], new_dim):
        avouch(dim is None, TypeError(f"Cannot stack tensors by multiple dimensions."))
        dim = tensors[1]
        tensors = tuple(tensors[0])
    elif len(tensors) > 2 and isinstance(tensors[-1], new_dim):
        avouch(dim is None, TypeError(f"Cannot stack tensors by multiple dimensions."))
        dim = tensors[-1]
        tensors = tuple(tensors[:-1])
    if not torch_returned:
        avouch(all_(isinstance(x, torch.Tensor) for x in tensors), TypeError(f"'bt.stack' can only stack torch.Tensor objects. "))
        if len(tensors) == 0: torch_returned = True; obj = tensor([])
        if not torch_returned:
            pivot = tensors[argmax_([x.n_dim for x in tensors])[0]]
            if dim is None:
                if not isinstance(pivot, Tensor):  dim = new_dim(pivot, {})
                elif not pivot.has_batch:  dim = new_dim(pivot, {})
                else:  dim = new_dim(pivot, [pivot.non_bat_start])
            else:  dim = new_dim(pivot, dim)
            avouch(len(dim) == 1, TypeError(f"Cannot concat tensors in dimensions {dim}, please flatten them first or use '[0]' and "'0'" to specify the first feature/sequence dimension."))

            if crop:  dims_allow_mismatch = tuple(range_(pivot.space_start, pivot.space_stop))
            else:  dims_allow_mismatch = None
            try:  tensors = [x.expand_to(pivot, dims_allow_mismatch=dims_allow_mismatch) for x in tensors if x.n_ele > 0]
            except TypeError as e:
                if "Cannot expand tensor" in str(e) or "Size mismatch in 'expand_to'" in str(e):
                    raise TypeError("Tensors can only be stacked when all of them have a same shape. " + f"Currently: {[x.shape for x in tensors]}")
                else:  raise e
            if crop:  tensors = [x if x.shape == pivot.shape else crop_as(x, pivot.space) for x in tensors]

            bt_tensors = [t for t in tensors if isinstance(t, Tensor)]
            with torch._C.DisableTorchFunction():
                if len(bt_tensors) == 0: torch_returned = True; obj = torch.stack(tensors, dim[0], out=out).as_subclass(Tensor)
                else: torch_returned = True; obj = Tensor.inherit_from(torch.stack(tensors, dim[0], out=out), bt_tensors[0], shape=dim)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "stack"
    return obj

def meshgrid(*tensors, indexing: str = None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    Create the mesh grid using 1D tensors.

    Args:
        tensors (tuple of Tensors): The tensors used for mesh grid.
            output[j][i_0, ..., i_{k-1}] = tensors[j][i_{j}],
            e.g. output_0, output_1 = meshgrid(arange(2), arange(3), indexing='ij') =>
                output_0 = Tensor([[0, 0, 0],
                                   [1, 1, 1]])
                output_1 = Tensor([[0, 1, 2],
                                   [0, 1, 2]])
        indexing (str, optional): The indexing criteria.
            indexing = 'ij' means the index for an element goes as (i_row, j_column).
            indexing = 'xy' means the index for an element goes as (x+_coordinate (in column), y-_coordinate (in row)),
                note that y_coordinate=0 for the first row and increases for lower rows.
            Altering indexing from 'ij' and 'xy' will result in a transpose of results.
            Defaults to 'ij' in PyTorch < 1.10 and 'xy' in future versions (following PyTorch).

    Automatically inheritted method from 'torch.meshgrid'. The automatically generated codes are as follows:
        [ 1]: def meshgrid(*tensors, indexing: str = None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 2]:     '''
        [ 3]:     Create the mesh grid using 1D tensors.
        [ 4]:
        [ 5]:     Args:
        [ 6]:         tensors (tuple of Tensors): The tensors used for mesh grid.
        [ 7]:             output[j][i_0, ..., i_{k-1}] = tensors[j][i_{j}],
        [ 8]:             e.g. output_0, output_1 = meshgrid(arange(2), arange(3), indexing='ij') =>
        [ 9]:                 output_0 = Tensor([[0, 0, 0],
        [10]:                                    [1, 1, 1]])
        [11]:                 output_1 = Tensor([[0, 1, 2],
        [12]:                                    [0, 1, 2]])
        [13]:         indexing (str, optional): The indexing criteria.
        [14]:             indexing = 'ij' means the index for an element goes as (i_row, j_column).
        [15]:             indexing = 'xy' means the index for an element goes as (x+_coordinate (in column), y-_coordinate (in row)),
        [16]:                 note that y_coordinate=0 for the first row and increases for lower rows.
        [17]:             Altering indexing from 'ij' and 'xy' will result in a transpose of results.
        [18]:             Defaults to 'ij' in PyTorch < 1.10 and 'xy' in future versions (following PyTorch).
        [19]:     '''
        [20]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [21]:
        [22]:     pivot = None
        [23]:     for t in []:
        [24]:         if isinstance(t, torch.Tensor): pivot = t; break
        [25]:     subclass = Tensor.get_tensor_subclass(pivot)
        [26]:
        [27]:     # Obtain available sized in arguments (which will be fed into size function).
        [28]:
        [29]:     # Use the given inner codes if they are provided.
        [30]:     torch_returned = False
        [31]:     avouch(all_(isinstance(x, torch.Tensor) and x.ndim == 1 for x in tensors), TypeError(f"'bt.meshgrid' can only span torch.Tensor objects. "))
        [32]:     with torch._C.DisableTorchFunction():
        [33]:         ret = tuple(Tensor.inherit_from(t, tensors[0], shape=[]) for t in torch.meshgrid(*tensors, indexing=indexing))
        [34]:     for i, t in enumerate(tensors):
        [35]:         for r in ret:  r.add_special_dim(i, t.shape)
        [36]:     obj = ret # suppress:  special_from
        [37]:
        [38]:     def update_special(obj, updates):
        [39]:         if isinstance(obj, tuple):
        [40]:             for x in obj: update_special(x, updates)
        [41]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [42]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [43]:
        [44]:     if getattr(obj, 'grad_fn', None) is not None:
        [45]:         obj.grad_fn_name = "meshgrid"
        [46]:     return obj
        [47]:
        The document of the original function is:
        Creates grids of coordinates specified by the 1D inputs in `attr`:tensors.

                This is helpful when you want to visualize data over some
                range of inputs. See below for a plotting example.

                Given :math:`N` 1D tensors :math:`T_0 \ldots T_{N-1}` as
                inputs with corresponding sizes :math:`S_0 \ldots S_{N-1}`,
                this creates :math:`N` N-dimensional tensors :math:`G_0 \ldots
                G_{N-1}`, each with shape :math:`(S_0, ..., S_{N-1})` where
                the output :math:`G_i` is constructed by expanding :math:`T_i`
                to the result shape.

                .. note::
                    0D inputs are treated equivalently to 1D inputs of a
                    single element.

                .. warning::
                    `torch.meshgrid(*tensors)` currently has the same behavior
                    as calling `numpy.meshgrid(*arrays, indexing='ij')`.

                    In the future `torch.meshgrid` will transition to
                    `indexing='xy'` as the default.

                    https://github.com/pytorch/pytorch/issues/50276 tracks
                    this issue with the goal of migrating to NumPy's behavior.

                .. seealso::

                    :func:`torch.cartesian_prod` has the same effect but it
                    collects the data in a tensor of vectors.

                Args:
                    tensors (list of Tensor): list of scalars or 1 dimensional tensors. Scalars will be
                        treated as tensors of size :math:`(1,)` automatically

                    indexing: (str, optional): the indexing mode, either "xy"
                        or "ij", defaults to "ij". See warning for future changes.

                        If "xy" is selected, the first dimension corresponds
                        to the cardinality of the second input and the second
                        dimension corresponds to the cardinality of the first
                        input.

                        If "ij" is selected, the dimensions are in the same
                        order as the cardinality of the inputs.

                Returns:
                    seq (sequence of Tensors): If the input has :math:`N`
                    tensors of size :math:`S_0 \ldots S_{N-1}``, then the
                    output will also have :math:`N` tensors, where each tensor
                    is of shape :math:`(S_0, ..., S_{N-1})`.

                Example::

                    >>> x = torch.tensor([1, 2, 3])
                    >>> y = torch.tensor([4, 5, 6])

                    Observe the element-wise pairings across the grid, (1, 4),
                    (1, 5), ..., (3, 6). This is the same thing as the
                    cartesian product.
                    >>> grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')
                    >>> grid_x
                    tensor([[1, 1, 1],
                            [2, 2, 2],
                            [3, 3, 3]])
                    >>> grid_y
                    tensor([[4, 5, 6],
                            [4, 5, 6],
                            [4, 5, 6]])

                    This correspondence can be seen when these grids are
                    stacked properly.
                    >>> torch.equal(torch.cat(tuple(torch.dstack([grid_x, grid_y]))),
                    ...             torch.cartesian_prod(x, y))
                    True

                    `torch.meshgrid` is commonly used to produce a grid for
                    plotting.
                    >>> # xdoctest: +REQUIRES(module:matplotlib)
                    >>> import matplotlib.pyplot as plt
                    >>> xs = torch.linspace(-5, 5, steps=100)
                    >>> ys = torch.linspace(-5, 5, steps=100)
                    >>> x, y = torch.meshgrid(xs, ys, indexing='xy')
                    >>> z = torch.sin(torch.sqrt(x * x + y * y))
                    >>> ax = plt.axes(projection='3d')
                    >>> ax.plot_surface(x.numpy(), y.numpy(), z.numpy())
                    >>> plt.show()

                .. image:: ../_static/img/meshgrid.png
                    :width: 512

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    # Obtain available sized in arguments (which will be fed into size function).

    # Use the given inner codes if they are provided.
    torch_returned = False
    avouch(all_(isinstance(x, torch.Tensor) and x.ndim == 1 for x in tensors), TypeError(f"'bt.meshgrid' can only span torch.Tensor objects. "))
    with torch._C.DisableTorchFunction():
        ret = tuple(Tensor.inherit_from(t, tensors[0], shape=[]) for t in torch.meshgrid(*tensors, indexing=indexing))
    for i, t in enumerate(tensors):
        for r in ret:  r.add_special_dim(i, t.shape)
    obj = ret # suppress:  special_from

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "meshgrid"
    return obj

@collect_memory
def eye(*size: Size, dim=None, out=None, dtype=None, layout=torch.strided, device=_device.main_device, requires_grad=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    create identity matrix in (the first available condition):
    (1) feature dimensions if size has at least one;
    (2) space dimensions if size has at least one;
    (3) sequence dimensions if size has at least one.

    Automatically inheritted method from 'torch.eye'. The automatically generated codes are as follows:
        [ 1]: @collect_memory
        [ 2]: def eye(*size: Size, dim=None, out=None, dtype=None, layout=torch.strided, device=_device.main_device, requires_grad=False, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:     create identity matrix in (the first available condition):
        [ 5]:     (1) feature dimensions if size has at least one;
        [ 6]:     (2) space dimensions if size has at least one;
        [ 7]:     (3) sequence dimensions if size has at least one.
        [ 8]:     '''
        [ 9]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [10]:
        [11]:     pivot = None
        [12]:     for t in []:
        [13]:         if isinstance(t, torch.Tensor): pivot = t; break
        [14]:     subclass = Tensor.get_tensor_subclass(pivot)
        [15]:
        [16]:     # Obtain available sized in arguments (which will be fed into size function).
        [17]:     size=Size(*size)
        [18]:     # Use the given inner codes if they are provided.
        [19]:     torch_returned = False
        [20]:     avouch(len(size) > 0, TypeError("bt.eye receives at least one size input. "))
        [21]:     if dim is None:
        [22]:         if size.has_feature:
        [23]:             dim = exist_dim(size, [])
        [24]:             if len(dim) > 2:  dim = dim[-2:]
        [25]:         elif size.has_space:  dim = exist_dim(size, ...)
        [26]:         elif size.has_sequence:  dim = exist_dim(size, '')
        [27]:         else:  raise TypeError(f"Invalid size {size} for bt.eye: at least one non-batch dimension needed. ")
        [28]:     if len(dim) == 1:
        [29]:         size = size[:dim[0]] + size[dim[0]:dim[0]+1] + size[dim[0]:]
        [30]:         dim = (dim[0], dim[0]+1)
        [31]:     avouch(len(dim) == 2, TypeError("bt.eye can only be created in two-dimensional space, please make sure the shape has 2 space dimensions or use keyword argument 'dim' to identify them. "))
        [32]:
        [33]:     kwargs = dict(out=out, dtype=dtype, layout=layout, device=device, requires_grad=requires_grad)
        [34]:     if dim[0] > dim[1]:  dim = dim[::-1]
        [35]:     new_size = size[:dim[1]].with_dim_size(dim[0], min_(size[dim[0]], size[dim[1]])) + size[dim[1]+1:]
        [36]:     eye_output = ones(new_size, **kwargs).diag(dim=(dim[0],)).move_dim(dim[0]+1, dim[1]).special_from(size)
        [37]:     if size[dim[1]] > size[dim[0]]:
        [38]:         obj = cat(eye_output, zeros(eye_output.shape.with_dim_size(dim[1], size[dim[1]]-size[dim[0]]), **kwargs), dim[1]).special_from(size)
        [39]:     elif size[dim[1]] < size[dim[0]]:
        [40]:         obj = eye_output[(slice(None),) * dim[1] + (slice(0, size[dim[1]]),)].special_from(size)
        [41]:     else: torch_returned = True; obj = eye_output.special_from(size)
        [42]:
        [43]:     def update_special(obj, updates):
        [44]:         if isinstance(obj, tuple):
        [45]:             for x in obj: update_special(x, updates)
        [46]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [47]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [48]:
        [49]:     if getattr(obj, 'grad_fn', None) is not None:
        [50]:         obj.grad_fn_name = "eye"
        [51]:     return obj
        [52]:
        The document of the original function is:

        eye(n, m=None, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor

        Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.

        Args:
            n (int): the number of rows
            m (int, optional): the number of columns with default being :attr:`n`

        Keyword arguments:
            out (Tensor, optional): the output tensor.
            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
                Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).
            layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
                Default: ``torch.strided``.
            device (:class:`torch.device`, optional): the desired device of returned tensor.
                Default: if ``None``, uses the current device for the default tensor type
                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU
                for CPU tensor types and the current CUDA device for CUDA tensor types.
            requires_grad (bool, optional): If autograd should record operations on the
                returned tensor. Default: ``False``.

        Returns:
            Tensor: A 2-D tensor with ones on the diagonal and zeros elsewhere

        Example::

            >>> torch.eye(3)
            tensor([[ 1.,  0.,  0.],
                    [ 0.,  1.,  0.],
                    [ 0.,  0.,  1.]])

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in []:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    # Obtain available sized in arguments (which will be fed into size function).
    size=Size(*size)
    # Use the given inner codes if they are provided.
    torch_returned = False
    avouch(len(size) > 0, TypeError("bt.eye receives at least one size input. "))
    if dim is None:
        if size.has_feature:
            dim = exist_dim(size, [])
            if len(dim) > 2:  dim = dim[-2:]
        elif size.has_space:  dim = exist_dim(size, ...)
        elif size.has_sequence:  dim = exist_dim(size, '')
        else:  raise TypeError(f"Invalid size {size} for bt.eye: at least one non-batch dimension needed. ")
    if len(dim) == 1:
        size = size[:dim[0]] + size[dim[0]:dim[0]+1] + size[dim[0]:]
        dim = (dim[0], dim[0]+1)
    avouch(len(dim) == 2, TypeError("bt.eye can only be created in two-dimensional space, please make sure the shape has 2 space dimensions or use keyword argument 'dim' to identify them. "))

    kwargs = dict(out=out, dtype=dtype, layout=layout, device=device, requires_grad=requires_grad)
    if dim[0] > dim[1]:  dim = dim[::-1]
    new_size = size[:dim[1]].with_dim_size(dim[0], min_(size[dim[0]], size[dim[1]])) + size[dim[1]+1:]
    eye_output = ones(new_size, **kwargs).diag(dim=(dim[0],)).move_dim(dim[0]+1, dim[1]).special_from(size)
    if size[dim[1]] > size[dim[0]]:
        obj = cat(eye_output, zeros(eye_output.shape.with_dim_size(dim[1], size[dim[1]]-size[dim[0]]), **kwargs), dim[1]).special_from(size)
    elif size[dim[1]] < size[dim[0]]:
        obj = eye_output[(slice(None),) * dim[1] + (slice(0, size[dim[1]]),)].special_from(size)
    else: torch_returned = True; obj = eye_output.special_from(size)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "eye"
    return obj

@collect_memory
def eye_like(input: 'Tensor', dim=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
    """
    create identity matrix from shape of `input` in (the first available condition):
    (1) feature dimensions if size has at least one;
    (2) space dimensions if size has at least one;
    (3) sequence dimensions if size has at least one.

    Automatically inheritted method from 'torch.eye_like'. The automatically generated codes are as follows:
        [ 1]: @collect_memory
        [ 2]: def eye_like(input: 'Tensor', dim=None, function_dim=None, batch_dim=None, sequence_dim=None, feature_dim=None):
        [ 3]:     '''
        [ 4]:     create identity matrix from shape of `input` in (the first available condition):
        [ 5]:     (1) feature dimensions if size has at least one;
        [ 6]:     (2) space dimensions if size has at least one;
        [ 7]:     (3) sequence dimensions if size has at least one.
        [ 8]:     '''
        [ 9]:     # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.
        [10]:
        [11]:     pivot = None
        [12]:     for t in [input]:
        [13]:         if isinstance(t, torch.Tensor): pivot = t; break
        [14]:     subclass = Tensor.get_tensor_subclass(pivot)
        [15]:
        [16]:     if input is None: ...
        [17]:     elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
        [18]:     if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
        [19]:     if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        [20]:         if input.device.type == 'cpu': input = input.to(pivot.device)
        [21]:
        [22]:     # Obtain available sized in arguments (which will be fed into size function).
        [23]:     input_shape=None if input is None else Size(input.shape)
        [24]:     # Use the given inner codes if they are provided.
        [25]:     torch_returned = False
        [26]:     obj = eye(input_shape, dim=dim, dtype=input.dtype, device=input.device, layout=input.layout)
        [27]:     ref_shape = size_mapping['eye_like'](input_shape)
        [28]:     obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)
        [29]:
        [30]:     def update_special(obj, updates):
        [31]:         if isinstance(obj, tuple):
        [32]:             for x in obj: update_special(x, updates)
        [33]:         elif isinstance(obj, Tensor): obj.special_dims.update(updates)
        [34]:     update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})
        [35]:
        [36]:     if getattr(obj, 'grad_fn', None) is not None:
        [37]:         obj.grad_fn_name = "eye_like"
        [38]:     return obj
        [39]:

    """
    # Cast all arguments to the wanted 'Tensor' or 'Size' type, and convert the dim arguments.

    pivot = None
    for t in [input]:
        if isinstance(t, torch.Tensor): pivot = t; break
    subclass = Tensor.get_tensor_subclass(pivot)

    if input is None: ...
    elif not isinstance(input, torch.Tensor): input = torch.tensor(input)
    if not isinstance(input, subclass): input = input.as_subclass(subclass).special_from(input.shape)
    if pivot is not None and hasattr(pivot, 'device') and pivot.device.type != 'cpu':
        if input.device.type == 'cpu': input = input.to(pivot.device)

    # Obtain available sized in arguments (which will be fed into size function).
    input_shape=None if input is None else Size(input.shape)
    # Use the given inner codes if they are provided.
    torch_returned = False
    obj = eye(input_shape, dim=dim, dtype=input.dtype, device=input.device, layout=input.layout)
    ref_shape = size_mapping['eye_like'](input_shape)
    obj = Tensor.inherit_from(obj, pivot, shape=...).special_from(ref_shape, allow_view=True)

    def update_special(obj, updates):
        if isinstance(obj, tuple):
            for x in obj: update_special(x, updates)
        elif isinstance(obj, Tensor): obj.special_dims.update(updates)
    update_special(obj, {k: v for k, v in dict(function_dim=function_dim, batch_dim=batch_dim, sequence_dim=sequence_dim, feature_dim=feature_dim).items() if v != None})

    if getattr(obj, 'grad_fn', None) is not None:
        obj.grad_fn_name = "eye_like"
    return obj

### STOP GLOBAL AUTO GENERATION

@collect_memory
def tensor_like(input, target: 'Tensor', *, dtype=None, device=None, requires_grad=None):
    """
    bt.tensor_like(input, target) creates a tensor with the same `dtype`, `device` and `requires_grad` as `target` (namely in the same characteristics). 
    Note that an expanding reshape will also be performed just like `ones/zeros_like`. Use `tensor_to` to create a tensor with only the same `dtype` and `device`. 
    """
    target = to_bttensor(target)
    if dtype is None: dtype = target.dtype
    if device is None: device = target.device
    if requires_grad is None: requires_grad = target.requires_grad
    if not isinstance(input, torch.Tensor): input = torch.tensor(input, dtype=dtype, device=device, requires_grad=requires_grad)
    if not isinstance(input, Tensor): input = as_tensor(Tensor.inherit_from(input, target, shape=[]), dtype=dtype, device=device)
    else: input = as_tensor(input, dtype=dtype, device=device)
    if requires_grad and not input.requires_grad: input.requires_grad = True
    elif not requires_grad and input.requires_grad: input = input.detach()
    if input.n_dim == target.n_dim: input = input.special_from(target)
    else: input = input.expand_to(target)
    return input

@collect_memory
def tensor_to(input, target: 'Tensor', *, dtype=None, device=None, requires_grad=None):
    """
    bt.tensor_to(input, target) creates a tensor with the same `dtype` and `device` as `target` (namely in the same scope). 
    Note that no shape changes will be performed (similar to method `to`) but special dimensions would be inherited from `target` when they have the same number of dimensions. 
    """
    target = to_bttensor(target)
    if dtype is None: dtype = target.dtype
    if device is None: device = target.device
    if requires_grad is not None:
        if not isinstance(input, torch.Tensor): input = torch.tensor(input, dtype=dtype, device=device, requires_grad=requires_grad)
        if not isinstance(input, Tensor): input = as_tensor(Tensor.inherit_from(input, target, shape=[]), dtype=dtype, device=device)
        else: input = as_tensor(input, dtype=dtype, device=device)
        if requires_grad and not input.requires_grad: input.requires_grad = True
        elif not requires_grad and input.requires_grad: input = input.detach()
    else:
        if not isinstance(input, torch.Tensor): input = torch.tensor(input, dtype=dtype, device=device)
        if not isinstance(input, Tensor): input = as_tensor(Tensor.inherit_from(input, target, shape=[]), dtype=dtype, device=device)
        else: input = as_tensor(input, dtype=dtype, device=device)
    if input.n_dim == target.n_dim: input = input.special_from(target)
    return input

def to_bttensor(data, *, dtype=None, device=_device.main_device, requires_grad=False, pin_memory=False):
    if data is None: return
    elif not isinstance(data, torch.Tensor):
        return tensor(data, dtype=dtype, device=device, requires_grad=requires_grad, pin_memory=pin_memory)
    elif not isinstance(data, Tensor):
        return data.as_subclass(Tensor).init_special()
    else: return data

@collect_memory
def batch_arange(*start_stop_step, out=None, dtype=None, layout=torch.strided, device=_device.device, requires_grad=False):
    return arange(*start_stop_step, out=out, dtype=dtype, layout=layout, device=device, requires_grad=requires_grad).with_batch_dim(1)

@collect_memory
@alias('channel_arange')
def feature_arange(*start_stop_step, out=None, dtype=None, layout=torch.strided, device=_device.device, requires_grad=False):
    return arange(*start_stop_step, out=out, dtype=dtype, layout=layout, device=device, requires_grad=requires_grad).with_feature_dim(1)

@collect_memory
def sequence_arange(*start_stop_step, out=None, dtype=None, layout=torch.strided, device=_device.device, requires_grad=False):
    return arange(*start_stop_step, out=out, dtype=dtype, layout=layout, device=device, requires_grad=requires_grad).with_sequence_dim(-1)

def batch_tensor(data, *, dtype=None, device=_device.device, requires_grad=False, pin_memory=False):
    self = tensor(data, dtype=dtype, device=device, requires_grad=requires_grad, pin_memory=pin_memory)
    avouch(self.n_dim == 1, TypeError(f"Cannot create 'batch_tensor' from {data}: dimension is not 1. "))
    return self.with_batch_dim(1)

@alias("channel_tensor", one_dim_only=True)
def feature_tensor(data, *, dtype=None, device=_device.device, requires_grad=False, pin_memory=False, one_dim_only=False):
    self = tensor(data, dtype=dtype, device=device, requires_grad=requires_grad, pin_memory=pin_memory)
    if one_dim_only: avouch(self.n_dim == 1, TypeError(f"Cannot create 'channel/feature_tensor' from {data}: dimension is not 1. "))
    return self.with_feature_dim(self.n_dim)

@alias("time_tensor", "series_tensor")
def sequence_tensor(data, *, dtype=None, device=_device.device, requires_grad=False, pin_memory=False):
    self = tensor(data, dtype=dtype, device=device, requires_grad=requires_grad, pin_memory=pin_memory)
    return self.with_sequence_dim(self.n_dim)

class _Randint:

    def __init__(self):
        """Please use randint[lower, upper] to specify the range with upper end excluded. """
        self.range = (0, 2)

    def __getitem__(self, *t):
        if len(t) == 1 and isinstance(t[0], slice):
            t = t[0]
            if t.step is not None: raise TypeError(f"Please use randint_like[lower:upper] to specify the range with upper end excluded. ")
            t = (t.start if t.start is None else 0, t.stop if t.stop is not None else 2)
        elif len(t) == 1 and isinstance(t[0], tuple): t = t[0]
        if len(t) == 0: t = (0, 2)
        elif len(t) == 1: t = (0, t[0])
        if len(t) > 2 or t[0] >= t[1]: raise TypeError(f"Please use randint_like[lower, upper] to specify the range with upper end excluded. ")
        self.range = t
        return self

    def __call__(self, *size, generator=None, dtype=None, device=None, requires_grad=False):
        if len(size) <= 3 and isinstance(size[-1], tuple):
            *t, size = size
            if len(t) == 0: t = (0, 2)
            elif len(t) == 1: t = (0, t[0])
            elif len(t) > 2: raise TypeError(f"Please use randint[lower, upper] to specify the range with upper end excluded. ")
            self.range = t
        size = Size(*size)
        with torch._C.DisableTorchFunction():
            return torch.randint(self.range[0], self.range[1], size, generator=generator, dtype=dtype, device=device, requires_grad=requires_grad).as_subclass(Tensor).special_from(size)

class _Randint_like:

    def __init__(self):
        """Please use randint_like[lower, upper] to specify the range with upper end excluded. """
        self.range = (0, 2)

    def __getitem__(self, *t):
        if len(t) == 1 and isinstance(t[0], slice):
            t = t[0]
            if t.step is not None: raise TypeError(f"Please use randint_like[lower:upper] to specify the range with upper end excluded. ")
            t = (t.start if t.start is None else 0, t.stop if t.stop is not None else 2)
        elif len(t) == 1 and isinstance(t[0], tuple): t = t[0]
        if len(t) == 0: t = (0, 2)
        elif len(t) == 1: t = (0, t[0])
        if len(t) > 2 or t[0] >= t[1]: raise TypeError(f"Please use randint_like[lower, upper] to specify the range with upper end excluded. ")
        self.range = t
        return self

    def __call__(self, data, *t, memory_format=None, dtype=None, layout=None, device=None, pin_memory=False, requires_grad=False):
        if 0 < len(t) <= 2:
            if len(t) == 1: t = (0, t[0])
            elif len(t) > 2: raise TypeError(f"Please use randint[lower, upper] to specify the range with upper end excluded. ")
            self.range = t
        with torch._C.DisableTorchFunction():
            if layout is None:
                return torch.randint_like(data, self.range[0], self.range[1], memory_format=memory_format, dtype=dtype, device=device, pin_memory=pin_memory, requires_grad=requires_grad).as_subclass(Tensor).special_from(data.shape)
            else:
                return torch.randint_like(data, self.range[0], self.range[1], memory_format=memory_format, dtype=dtype, layout=layout, device=device, pin_memory=pin_memory, requires_grad=requires_grad).as_subclass(Tensor).special_from(data.shape)

randint = _Randint()
randint_like = _Randint_like()

##########################
##  Direct inheritance  ##
##########################

dtype = torch.dtype;              device = torch.device

# D-Types
bfloat16 = torch.bfloat16;        bool = torch.bool
cdouble = torch.cdouble;          cfloat = torch.cfloat;             chalf = torch.chalf
complex128 = torch.complex128;    complex64 = torch.complex64;       complex32 = torch.complex32
double = torch.double;            half = torch.half
float = torch.float;              float16 = torch.float16;           float32 = torch.float32;           float64 = torch.float64
int = torch.int;                  int16 = torch.int16;               int32 = torch.int32;               int64 = torch.int64;               int8 = torch.int8
qint32 = torch.qint32;            qint8 = torch.qint8;               quint2x4 = torch.quint2x4;         quint4x2 = torch.quint4x2;         quint8 = torch.quint8
long = torch.long;                short = torch.short;               uint8 = torch.uint8

# functions
def manual_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic=True
    torch.backends.cudnn.benchmark = True

